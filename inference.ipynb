{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":99552,"databundleVersionId":13190393,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":256223143,"sourceType":"kernelVersion"},{"sourceId":3732,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":2659,"modelId":312}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/03-aneurysmnet-inference-intracranial-nb153?scriptVersionId=256350722\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# ====================================================\n# CELL 1: IMPORTS & CONFIG\n# ====================================================\n\nimport os\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport timm\nimport cv2\nimport pydicom\nimport nibabel as nib\nfrom scipy import ndimage\nfrom scipy.ndimage import label, center_of_mass\nfrom PIL import Image\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport kaggle_evaluation.rsna_inference_server\nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:42:11.70019Z","iopub.execute_input":"2025-08-16T18:42:11.700456Z","iopub.status.idle":"2025-08-16T18:42:29.396061Z","shell.execute_reply.started":"2025-08-16T18:42:11.700435Z","shell.execute_reply":"2025-08-16T18:42:29.3955Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Competition Configuration\nclass Config:\n    # Paths\n    TRAIN_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv'\n    SERIES_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/'\n    SEGMENTATION_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/segmentations/'\n    STAGE1_MODEL_PATH = '/kaggle/input/pytorch-aneurysmnet-intracranial-nb153/pytorch/custom3dunet-v1/1/stage1_segmentation_best.pth'\n    \n    # Stage 2 Configuration\n    ROI_SIZE = (224, 224)\n    ROIS_PER_SERIES = 3\n    BATCH_SIZE = 32\n    EPOCHS = 1\n    LEARNING_RATE = 1e-4\n    N_FOLDS = 2\n    \n    # Competition constants\n    ID_COL = 'SeriesInstanceUID'\n    LABEL_COLS = [\n        'Left Infraclinoid Internal Carotid Artery', 'Right Infraclinoid Internal Carotid Artery',\n        'Left Supraclinoid Internal Carotid Artery', 'Right Supraclinoid Internal Carotid Artery',\n        'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery', 'Anterior Communicating Artery',\n        'Left Anterior Cerebral Artery', 'Right Anterior Cerebral Artery',\n        'Left Posterior Communicating Artery', 'Right Posterior Communicating Artery',\n        'Basilar Tip', 'Other Posterior Circulation', 'Aneurysm Present',\n    ]\n    \n    # Device and training\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MIXED_PRECISION = True\n    STAGE2_CACHE_DIR = '/kaggle/working/stage2_cache'\n    \n    # Debug\n    DEBUG_MODE = False\n    DEBUG_SAMPLES = 0\n\nprint(f\"✅ Configuration loaded - Device: {Config.DEVICE}\")\n\n# ====================================================\n# CELL 1.5: CUSTOM 3D UNET (REPLACES MONAI BASICUNET)\n# ====================================================\n\nclass Custom3DUNet(nn.Module):\n    \"\"\"Pure PyTorch 3D UNet implementation to replace MONAI BasicUNet\"\"\"\n    \n    def __init__(self, spatial_dims=3, in_channels=1, out_channels=32, \n                 features=(32, 64, 128, 256, 512, 32), dropout=0.1):\n        super().__init__()\n        \n        self.features = features\n        self.dropout = dropout\n        \n        # Encoder (downsampling path)\n        self.encoder_blocks = nn.ModuleList()\n        prev_channels = in_channels\n        \n        for i, feature_count in enumerate(features[:-1]):  # Exclude last feature (decoder output)\n            # Each encoder block: Conv3D -> BatchNorm -> ReLU -> Conv3D -> BatchNorm -> ReLU\n            block = nn.Sequential(\n                nn.Conv3d(prev_channels, feature_count, kernel_size=3, padding=1),\n                nn.BatchNorm3d(feature_count),\n                nn.ReLU(inplace=True),\n                nn.Conv3d(feature_count, feature_count, kernel_size=3, padding=1),\n                nn.BatchNorm3d(feature_count),\n                nn.ReLU(inplace=True),\n                nn.Dropout3d(dropout) if dropout > 0 else nn.Identity()\n            )\n            self.encoder_blocks.append(block)\n            prev_channels = feature_count\n        \n        # Downsampling layers (MaxPool)\n        self.downsample_layers = nn.ModuleList([\n            nn.MaxPool3d(kernel_size=2, stride=2) \n            for _ in range(len(features) - 2)  # No downsampling after last encoder block\n        ])\n        \n        # Decoder (upsampling path)\n        self.decoder_blocks = nn.ModuleList()\n        self.upsample_layers = nn.ModuleList()\n        \n        # Reverse the features for decoder (skip the input feature count)\n        decoder_features = list(reversed(features[:-1]))  # [512, 256, 128, 64, 32]\n        \n        for i in range(len(decoder_features) - 1):\n            current_features = decoder_features[i]\n            next_features = decoder_features[i + 1]\n            \n            # Upsampling layer\n            upsample = nn.ConvTranspose3d(\n                current_features, next_features, \n                kernel_size=2, stride=2\n            )\n            self.upsample_layers.append(upsample)\n            \n            # Decoder block (concatenation + convolutions)\n            # Input: upsampled features + skip connection = next_features * 2\n            decoder_block = nn.Sequential(\n                nn.Conv3d(next_features * 2, next_features, kernel_size=3, padding=1),\n                nn.BatchNorm3d(next_features),\n                nn.ReLU(inplace=True),\n                nn.Conv3d(next_features, next_features, kernel_size=3, padding=1),\n                nn.BatchNorm3d(next_features),\n                nn.ReLU(inplace=True),\n                nn.Dropout3d(dropout) if dropout > 0 else nn.Identity()\n            )\n            self.decoder_blocks.append(decoder_block)\n        \n        # Final output convolution\n        self.final_conv = nn.Conv3d(features[0], out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        # Store skip connections\n        skip_connections = []\n        \n        # Encoder path\n        for i, encoder_block in enumerate(self.encoder_blocks):\n            x = encoder_block(x)\n            skip_connections.append(x)\n            \n            # Downsample (except for the last encoder block)\n            if i < len(self.downsample_layers):\n                x = self.downsample_layers[i](x)\n        \n        # Decoder path\n        skip_connections = skip_connections[:-1]  # Remove the deepest layer (no skip for bottleneck)\n        skip_connections.reverse()  # Reverse to match decoder order\n        \n        for i, (upsample_layer, decoder_block) in enumerate(zip(self.upsample_layers, self.decoder_blocks)):\n            # Upsample\n            x = upsample_layer(x)\n            \n            # Get corresponding skip connection\n            skip = skip_connections[i]\n            \n            # Ensure spatial dimensions match (handle odd-sized inputs)\n            if x.shape[2:] != skip.shape[2:]:\n                x = nn.functional.interpolate(x, size=skip.shape[2:], mode='trilinear', align_corners=False)\n            \n            # Concatenate skip connection\n            x = torch.cat([x, skip], dim=1)\n            \n            # Apply decoder block\n            x = decoder_block(x)\n        \n        # Final output\n        x = self.final_conv(x)\n        \n        return x\n\nclass CustomTransforms:\n    \"\"\"Pure PyTorch transforms to replace MONAI transforms\"\"\"\n    \n    def __init__(self, keys=['volume']):\n        self.keys = keys\n        \n    def __call__(self, data_dict):\n        \"\"\"Apply transforms to data dictionary\"\"\"\n        result = {}\n        \n        for key in data_dict:\n            if key in self.keys:\n                # Convert numpy array to tensor if needed\n                if isinstance(data_dict[key], np.ndarray):\n                    result[key] = torch.from_numpy(data_dict[key]).float()\n                else:\n                    result[key] = data_dict[key]\n            else:\n                result[key] = data_dict[key]\n        \n        return result\n\nprint(\"✅ Custom 3D UNet and transforms loaded (MONAI-free!)\")\n\n\n\n# ====================================================\n# CELL 2: DATA LOADING & ROI EXTRACTION\n# ====================================================\n\nclass Simple3DSegmentationNet(nn.Module):\n    \"\"\"Stage 1 architecture for loading pre-trained model (config-aligned)\"\"\"\n    def __init__(self, in_channels=1, out_channels=1, features=(24, 48, 96, 192, 384, 24)):\n        super().__init__()\n        \n        self.backbone = Custom3DUNet(\n            spatial_dims=3,\n            in_channels=in_channels,\n            out_channels=features[-1],\n            features=features,\n            dropout=0.1\n        )\n        \n        self.seg_head = nn.Conv3d(features[-1], out_channels, kernel_size=1)\n        self.global_pool = nn.AdaptiveAvgPool3d(1)\n        self.classifier = nn.Sequential(\n            nn.Linear(features[-1], 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 1)\n        )\n        \n    def forward(self, x):\n        features = self.backbone(x)\n        seg_logits = self.seg_head(features)\n        pooled_features = self.global_pool(features).flatten(1)\n        cls_logits = self.classifier(pooled_features)\n        return seg_logits, cls_logits\n\nclass SimpleDICOMProcessor:\n    \"\"\"Simplified DICOM processor aligned with Stage 1\"\"\"\n    def __init__(self, target_size=(48, 112, 112)):\n        self.target_size = target_size\n        \n    def load_dicom_series(self, series_path):\n        \"\"\"Load DICOM series for Stage 1 model\"\"\"\n        try:\n            dicom_files = [f for f in os.listdir(series_path) if f.endswith('.dcm')]\n            if not dicom_files:\n                return np.zeros(self.target_size, dtype=np.float32)\n            \n            pixel_arrays = []\n            for f in dicom_files[:50]:\n                try:\n                    ds = pydicom.dcmread(os.path.join(series_path, f), force=True)\n                    if hasattr(ds, 'pixel_array'):\n                        arr = ds.pixel_array\n                        if arr.ndim == 2:\n                            pixel_arrays.append(arr)\n                        elif arr.ndim == 3:\n                            mid_start = arr.shape[0] // 4\n                            mid_end = 3 * arr.shape[0] // 4\n                            for slice_idx in range(mid_start, mid_end, 2):\n                                pixel_arrays.append(arr[slice_idx])\n                except:\n                    continue\n            \n            if not pixel_arrays:\n                return np.zeros(self.target_size, dtype=np.float32)\n            \n            # Resize all slices to same shape before stacking\n            target_slice_shape = (256, 256)\n            resized_arrays = []\n            for arr in pixel_arrays:\n                if arr.shape != target_slice_shape:\n                    resized_arr = cv2.resize(arr.astype(np.float32), (target_slice_shape[1], target_slice_shape[0]))\n                    resized_arrays.append(resized_arr)\n                else:\n                    resized_arrays.append(arr)\n            \n            volume = np.stack(resized_arrays, axis=0).astype(np.float32)\n            volume = self.preprocess_volume(volume)\n            return volume\n            \n        except Exception as e:\n            print(f\"Failed to load {series_path}: {e}\")\n            return np.zeros(self.target_size, dtype=np.float32)\n    \n    def preprocess_volume(self, volume):\n        \"\"\"Simple preprocessing (match Stage 1)\"\"\"\n        p1, p99 = np.percentile(volume, [1, 99])\n        volume = np.clip(volume, p1, p99)\n        denom = (p99 - p1) if (p99 - p1) > 1e-6 else 1e-6\n        volume = (volume - p1) / denom\n        \n        if volume.shape != self.target_size:\n            target_d, target_h, target_w = self.target_size\n            D, H, W = volume.shape\n            if D != target_d:\n                idx = np.linspace(0, max(D - 1, 0), num=target_d).astype(int) if D > 0 else np.zeros(target_d, dtype=int)\n                volume = volume[idx]\n            if (H, W) != (target_h, target_w):\n                resized = np.empty((target_d, target_h, target_w), dtype=np.float32)\n                for i in range(target_d):\n                    resized[i] = cv2.resize(volume[i].astype(np.float32), (target_w, target_h))\n                volume = resized\n        \n        return volume.astype(np.float32)\n\nclass Stage1Predictor:\n    \"\"\"Load and use Stage 1 model for ROI extraction\"\"\"\n    def __init__(self, model_path):\n        self.device = Config.DEVICE\n        self.processor = SimpleDICOMProcessor()\n        \n        # Load Stage 1 model\n        print(\"Loading Stage 1 model...\")\n        self.model = Simple3DSegmentationNet(features=(24, 48, 96, 192, 384, 24)).to(self.device)\n        \n        try:\n            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)\n            if 'model_state_dict' in checkpoint:\n                state_dict = checkpoint['model_state_dict']\n            else:\n                state_dict = checkpoint\n            \n            # Handle DataParallel wrapper\n            if any(key.startswith('module.') for key in state_dict.keys()):\n                state_dict = {key.replace('module.', ''): value for key, value in state_dict.items()}\n            \n            self.model.load_state_dict(state_dict, strict=False)\n            self.model.eval()\n            print(\"✅ Stage 1 model loaded successfully\")\n        except Exception as e:\n            print(f\"❌ Error loading Stage 1 model: {e}\")\n            self.model = None\n    \n    def predict_segmentation(self, series_path):\n        \"\"\"Get segmentation mask from Stage 1 model\"\"\"\n        if self.model is None:\n            return np.zeros((48, 112, 112), dtype=np.float32)\n        \n        try:\n            # Load volume\n            volume = self.processor.load_dicom_series(series_path)\n            \n            # Predict\n            with torch.no_grad():\n                volume_tensor = torch.from_numpy(volume).unsqueeze(0).unsqueeze(0).to(self.device)\n                seg_logits, _ = self.model(volume_tensor)\n                seg_mask = torch.sigmoid(seg_logits).cpu().numpy()[0, 0]\n            \n            return seg_mask\n        except Exception as e:\n            print(f\"Error predicting segmentation for {series_path}: {e}\")\n            return np.zeros((64, 128, 128), dtype=np.float32)\n\nclass ROIExtractor:\n    \"\"\"Research-backed ROI extraction with adaptive count and quality filtering\"\"\"\n    def __init__(self, stage1_predictor, roi_size=(224, 224)):\n        self.stage1_predictor = stage1_predictor\n        self.roi_size = roi_size\n        self.processor = SimpleDICOMProcessor()\n\n        # Research-backed thresholds\n        # Relaxed thresholds to avoid over-pruning when Stage 1 is weak\n        self.min_confidence_threshold = 0.25\n        self.high_confidence_threshold = 0.5\n        self.max_rois_per_series = 3\n    \n    def extract_top3_rois(self, series_path):\n        \"\"\"Extract 0-5 ROIs based on segmentation quality (research-backed)\"\"\"\n        # Cache ROI results per series to avoid recomputation\n        try:\n            os.makedirs(Config.STAGE2_CACHE_DIR, exist_ok=True)\n            sid = os.path.basename(series_path)\n            cache_path = os.path.join(Config.STAGE2_CACHE_DIR, f\"{sid}_rois.npy\")\n            if os.path.exists(cache_path):\n                arr = np.load(cache_path, allow_pickle=True)\n                return list(arr)\n        except Exception:\n            cache_path = None\n        rois = self.extract_adaptive_rois(series_path)\n        try:\n            if cache_path is not None:\n                np.save(cache_path, np.array(rois, dtype=object), allow_pickle=True)\n        except Exception:\n            pass\n        return rois\n\n\n    def extract_adaptive_rois(self, series_path):\n        \"\"\"Extract 0-5 ROIs based on segmentation quality (research-backed)\"\"\"\n        try:\n            print(f\"🔍 DEBUG: Quality-based ROI extraction for {os.path.basename(series_path)}\")\n            \n            # Get Stage 1 segmentation mask\n            seg_mask = self.stage1_predictor.predict_segmentation(series_path)\n            print(f\"🔍 DEBUG: Segmentation mask shape: {seg_mask.shape}\")\n            \n            # STEP 1: Assess overall segmentation quality (Key research finding)\n            seg_quality = self._assess_segmentation_quality(seg_mask)\n            print(f\"🔍 DEBUG: Segmentation quality score: {seg_quality:.3f}\")\n            \n            # STEP 2: Early termination if segmentation is too poor (Critical optimization)\n            if seg_quality < self.min_confidence_threshold:\n                print(f\"🔍 DEBUG: Poor segmentation quality ({seg_quality:.3f} < {self.min_confidence_threshold}), using fallback\")\n                return self._get_quality_fallback_rois(series_path, seg_mask)\n            \n            # STEP 3: Load original volume for ROI extraction\n            original_volume = self._load_efficient_volume(series_path)\n            print(f\"🔍 DEBUG: Efficient volume shape: {original_volume.shape}\")\n            \n            # STEP 4: Extract ROIs with confidence-based filtering\n            roi_candidates = self._find_quality_based_rois(seg_mask, original_volume)\n            \n            # STEP 5: Adaptive ROI count based on quality (research-backed approach)\n            selected_rois = self._select_adaptive_rois(roi_candidates, seg_quality, original_volume)\n            \n            print(f\"🔍 DEBUG: Selected {len(selected_rois)} ROIs based on quality assessment\")\n            return selected_rois\n            \n        except Exception as e:\n            print(f\"❌ Error in quality-based ROI extraction: {e}\")\n            return self._get_emergency_fallback_rois()\n    \n    def _assess_segmentation_quality(self, seg_mask):\n        \"\"\"Assess overall segmentation quality using research-backed metrics\"\"\"\n        try:\n            # Multiple quality indicators (based on Medical Segmentation Decathlon)\n            max_confidence = np.max(seg_mask)                    # Peak confidence\n            mean_high_conf = np.mean(seg_mask[seg_mask > 0.3]) if np.any(seg_mask > 0.3) else 0   # Mean of confident regions\n            confident_ratio = np.sum(seg_mask > 0.5) / seg_mask.size  # Ratio of confident pixels\n            \n            # Weighted quality score (research-backed weights)\n            quality_score = (0.4 * max_confidence + \n                           0.3 * mean_high_conf + \n                           0.3 * confident_ratio)\n            \n            return quality_score\n            \n        except:\n            return 0.1  # Very low quality if assessment fails\n    \n    def _find_quality_based_rois(self, seg_mask, original_volume):\n        \"\"\"Find ROI candidates with confidence scores (no hardcoded count)\"\"\"\n        print(\"🔍 DEBUG: Finding quality-based ROI candidates...\")\n        \n        # Resize segmentation mask to match original volume\n        if seg_mask.shape != original_volume.shape:\n            print(\"🔍 DEBUG: Resizing segmentation mask with cv2...\")\n            seg_mask_resized = np.zeros(original_volume.shape, dtype=np.float32)\n            for i in range(min(seg_mask.shape[0], original_volume.shape[0])):\n                if i < seg_mask.shape[0]:\n                    resized_slice = cv2.resize(\n                        seg_mask[i], \n                        (original_volume.shape[2], original_volume.shape[1])\n                    )\n                    seg_mask_resized[i] = resized_slice\n        else:\n            seg_mask_resized = seg_mask\n        \n        # Find ALL potential ROI candidates with confidence scores\n        roi_candidates = []\n        \n        for slice_idx in range(seg_mask_resized.shape[0]):\n            slice_mask = seg_mask_resized[slice_idx]\n            \n            # Find regions above minimum threshold\n            high_conf_regions = slice_mask > self.min_confidence_threshold\n            \n            if np.any(high_conf_regions):\n                # Find connected components of high-confidence regions\n                labeled_regions, num_regions = label(high_conf_regions)\n                \n                for region_id in range(1, num_regions + 1):\n                    region_mask = (labeled_regions == region_id)\n                    if np.sum(region_mask) > 50:  # Minimum region size\n                        \n                        # Get region center of mass\n                        com = center_of_mass(region_mask)\n                        y, x = int(com[0]), int(com[1])\n                        \n                        # Calculate region confidence\n                        region_confidence = np.mean(slice_mask[region_mask])\n                        \n                        roi_candidates.append({\n                            'slice_idx': slice_idx,\n                            'y': y,\n                            'x': x,\n                            'confidence': region_confidence,\n                            'region_size': np.sum(region_mask)\n                        })\n        \n        # Sort by confidence (descending)\n        roi_candidates.sort(key=lambda x: x['confidence'], reverse=True)\n        \n        print(f\"🔍 DEBUG: Found {len(roi_candidates)} ROI candidates\")\n        return roi_candidates\n    \n    def _select_adaptive_rois(self, roi_candidates, seg_quality, original_volume):\n        \"\"\"Adaptively select ROIs based on segmentation quality (research-backed)\"\"\"\n        if not roi_candidates:\n            print(\"🔍 DEBUG: No candidates found, using fallback\")\n            return self._get_quality_fallback_rois_from_volume(original_volume)\n        \n        # Adaptive selection based on segmentation quality\n        if seg_quality >= self.high_confidence_threshold:\n            max_rois = self.max_rois_per_series\n            min_confidence = 0.5\n        elif seg_quality >= self.min_confidence_threshold + 0.2:\n            max_rois = self.max_rois_per_series\n            min_confidence = 0.4\n        else:\n            max_rois = self.max_rois_per_series\n            min_confidence = 0.3\n        \n        # Filter and select ROIs\n        filtered = [c for c in roi_candidates if c['confidence'] >= min_confidence]\n        selected_candidates = filtered[:max_rois]\n        # If not enough, top-off with next best candidates\n        if len(selected_candidates) < max_rois:\n            for c in roi_candidates:\n                if c in selected_candidates:\n                    continue\n                selected_candidates.append(c)\n                if len(selected_candidates) >= max_rois:\n                    break\n        \n        # Convert to ROI format\n        rois = []\n        for i, candidate in enumerate(selected_candidates):\n            roi_patch = self._extract_roi_patch(\n                original_volume,\n                candidate['slice_idx'], \n                candidate['y'], \n                candidate['x']\n            )\n            \n            rois.append({\n                'roi_image': roi_patch,\n                'slice_idx': candidate['slice_idx'],\n                'coordinates': (candidate['y'], candidate['x']),\n                'confidence': candidate['confidence'],\n                'roi_id': i\n            })\n        \n        # Ensure at least max_rois via center-based fallback if still short\n        if len(rois) < self.max_rois_per_series:\n            needed = self.max_rois_per_series - len(rois)\n            center_fallbacks = self._get_quality_fallback_rois_from_volume(original_volume, needed)\n            rois.extend(center_fallbacks)\n        print(f\"🔍 DEBUG: Adaptively selected {len(rois)} ROIs (quality: {seg_quality:.3f})\")\n        return rois[: self.max_rois_per_series]\n    \n    def _get_quality_fallback_rois(self, series_path, seg_mask):\n        \"\"\"Fallback for poor segmentation quality: generate multiple center-based ROIs\"\"\"\n        print(\"🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\")\n        original_volume = self._load_efficient_volume(series_path)\n        return self._get_quality_fallback_rois_from_volume(original_volume, self.max_rois_per_series)\n\n    def _get_quality_fallback_rois_from_volume(self, original_volume, count: int = 3):\n        D, H, W = original_volume.shape\n        # Choose slice indices: center and quartiles\n        slices = sorted(set([D // 2, max(0, D // 4), min(D - 1, 3 * D // 4)]))\n        # Ensure desired count\n        while len(slices) < count:\n            # Add random slices if needed\n            slices.append(np.random.randint(0, D))\n            slices = list(dict.fromkeys(slices))\n        rois = []\n        cy, cx = H // 2, W // 2\n        for i, s in enumerate(slices[:count]):\n            roi_patch = self._extract_roi_patch(original_volume, s, cy, cx)\n            rois.append({\n                'roi_image': roi_patch,\n                'slice_idx': s,\n                'coordinates': (cy, cx),\n                'confidence': 0.2,\n                'roi_id': i\n            })\n        return rois\n        \n    def _get_simple_fallback_rois(self):\n        \"\"\"Simple fallback when no quality ROIs found\"\"\"\n        print(\"🔍 DEBUG: Using simple fallback (single center ROI)\")\n        dummy_roi = np.random.random((*Config.ROI_SIZE, 3)).astype(np.float32)\n        return [{\n            'roi_image': dummy_roi,\n            'slice_idx': 25,\n            'coordinates': (128, 128),\n            'confidence': 0.1,\n            'roi_id': 0\n        }]\n    \n    def _get_emergency_fallback_rois(self):\n        \"\"\"Emergency fallback when everything fails\"\"\"\n        print(\"🔍 DEBUG: Using emergency fallback ROI\")\n        dummy_roi = np.random.random((*Config.ROI_SIZE, 3)).astype(np.float32)\n        return [{\n            'roi_image': dummy_roi,\n            'slice_idx': 0,\n            'coordinates': (128, 128),\n            'confidence': 0.1,\n            'roi_id': 0\n        }]\n\n    \n    def _load_efficient_volume(self, series_path):\n        \"\"\"Load volume with smart distributed sampling to cover entire brain\"\"\"\n        try:\n            # Cache original volume slices to reduce repeated I/O\n            os.makedirs(Config.STAGE2_CACHE_DIR, exist_ok=True)\n            sid = os.path.basename(series_path)\n            vcache = os.path.join(Config.STAGE2_CACHE_DIR, f\"{sid}_vol.npy\")\n            if os.path.exists(vcache):\n                return np.load(vcache, allow_pickle=False)\n            dicom_files = [f for f in os.listdir(series_path) if f.endswith('.dcm')]\n            pixel_arrays = []\n            \n            # SMART SAMPLING: Distribute 50 slices across entire volume\n            total_files = len(dicom_files)\n            if total_files > 50:\n                # Calculate step size to distribute slices evenly\n                step = total_files / 50\n                selected_indices = [int(i * step) for i in range(50)]\n                selected_files = [dicom_files[i] for i in selected_indices]\n                print(f\"🔍 DEBUG: Smart sampling - selected {len(selected_files)} files from {total_files} total (every {step:.1f})\")\n            else:\n                selected_files = dicom_files\n                print(f\"🔍 DEBUG: Using all {len(selected_files)} files (less than 50)\")\n            \n            for f in selected_files:\n                try:\n                    ds = pydicom.dcmread(os.path.join(series_path, f), force=True)\n                    if hasattr(ds, 'pixel_array'):\n                        arr = ds.pixel_array\n                        if arr.ndim == 2:\n                            pixel_arrays.append(arr)\n                except:\n                    continue\n            \n            if pixel_arrays:\n                # SMALLER target shape to reduce memory usage\n                target_shape = (256, 256)  # Reduced from (512, 512)\n                \n                resized_arrays = []\n                for arr in pixel_arrays:\n                    # Use cv2.resize instead of ndimage.zoom (more reliable)\n                    if arr.shape != target_shape:\n                        resized_arr = cv2.resize(arr.astype(np.float32), target_shape)\n                        resized_arrays.append(resized_arr)\n                    else:\n                        resized_arrays.append(arr.astype(np.float32))\n                \n                volume = np.stack(resized_arrays, axis=0)\n                \n                # Simple normalization\n                p1, p99 = np.percentile(volume, [1, 99])\n                volume = np.clip(volume, p1, p99)\n                volume = (volume - p1) / (p99 - p1 + 1e-8)\n                \n                try:\n                    np.save(vcache, volume.astype(np.float32), allow_pickle=False)\n                except Exception:\n                    pass\n                return volume\n            \n        except Exception as e:\n            print(f\"Error loading efficient volume: {e}\")\n        \n        # Fallback volume (matches our smart sampling approach)\n        return np.random.random((50, 256, 256)).astype(np.float32)\n\n    \n    def _extract_roi_patch(self, volume, slice_idx, center_y, center_x):\n        \"\"\"Extract 224x224 ROI patch from volume\"\"\"\n        slice_data = volume[slice_idx]\n        h, w = slice_data.shape\n        \n        # Calculate patch boundaries\n        half_size = Config.ROI_SIZE[0] // 2\n        y1 = max(0, center_y - half_size)\n        y2 = min(h, center_y + half_size)\n        x1 = max(0, center_x - half_size)\n        x2 = min(w, center_x + half_size)\n        \n        # Extract patch\n        patch = slice_data[y1:y2, x1:x2]\n        \n        # Resize to exact ROI size\n        patch_resized = cv2.resize(patch, Config.ROI_SIZE)\n        \n        # Convert to 3-channel for EfficientNet\n        patch_3ch = np.stack([patch_resized] * 3, axis=2)\n        \n        return patch_3ch\n    \n\ndef create_training_data(df, stage1_predictor):\n    \"\"\"Create training data with 3 ROIs per series\"\"\"\n    print(\"🔄 Extracting ROIs for training data...\")\n    \n    roi_extractor = ROIExtractor(stage1_predictor)\n    training_data = []\n    \n    os.makedirs('rois', exist_ok=True)\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting ROIs\"):\n        series_id = row[Config.ID_COL]\n        series_path = os.path.join(Config.SERIES_DIR, series_id)\n        \n        if not os.path.exists(series_path):\n            continue\n        \n        # Extract ROIs\n        rois = roi_extractor.extract_top3_rois(series_path)\n        \n        # Create training samples\n        for roi_data in rois:\n            roi_filename = f\"rois/{series_id}_roi_{roi_data['roi_id']}.png\"\n            \n            # Save ROI image\n            roi_image = (roi_data['roi_image'] * 255).astype(np.uint8)\n            Image.fromarray(roi_image).save(roi_filename)\n            \n            # Create training record\n            sample = {\n                'roi_id': f\"{series_id}_roi_{roi_data['roi_id']}\",\n                'roi_path': roi_filename,\n                'series_id': series_id,\n                'roi_confidence': roi_data['confidence'],\n                'slice_idx': roi_data['slice_idx']\n            }\n            \n            # Add all label columns\n            for col in Config.LABEL_COLS:\n                sample[col] = row[col]\n            \n            training_data.append(sample)\n    \n    training_df = pd.DataFrame(training_data)\n    print(f\"✅ Created {len(training_df)} training samples from {len(df)} series\")\n    \n    return training_df\n\nprint(\"✅ Data loading and ROI extraction functions loaded\")\n\n# ====================================================\n# CELL 3: MODEL DEFINITION\n# ====================================================\n\nclass AneurysmClassificationDataset(Dataset):\n    \"\"\"Dataset for ROI-based classification\"\"\"\n    def __init__(self, df, mode='train'):\n        self.df = df\n        self.mode = mode\n        \n        # Data augmentation for training\n        if mode == 'train':\n            self.transform = transforms.Compose([\n                transforms.RandomHorizontalFlip(0.5),\n                transforms.RandomVerticalFlip(0.5),\n                transforms.RandomRotation(15),\n                transforms.ColorJitter(brightness=0.2, contrast=0.2),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n        else:\n            self.transform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # Load ROI image\n        roi_path = row['roi_path']\n        try:\n            image = Image.open(roi_path).convert('RGB')\n        except:\n            # Fallback to dummy image\n            image = Image.fromarray(np.random.randint(0, 255, (*Config.ROI_SIZE, 3), dtype=np.uint8))\n        \n        # Apply transforms\n        image = self.transform(image)\n        \n        # Get labels\n        labels = torch.tensor([row[col] for col in Config.LABEL_COLS], dtype=torch.float32)\n        \n        return {\n            'image': image,\n            'labels': labels,\n            'roi_id': row['roi_id'],\n            'confidence': torch.tensor(row['roi_confidence'], dtype=torch.float32)\n        }\n\nclass AneurysmEfficientNet(nn.Module):\n    \"\"\"EfficientNet-B3 for aneurysm classification with offline weights\"\"\"\n    def __init__(self, num_classes=len(Config.LABEL_COLS)):\n        super().__init__()\n        \n        # Load EfficientNet-B3 with offline pre-trained weights\n        import timm\n        \n        # Path to the pre-trained weights you added\n        weights_path = '/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b3/1/tf_efficientnet_b3_aa-84b4657e.pth'\n        \n        try:\n            # Create model without pre-trained weights first\n            self.backbone = timm.create_model('efficientnet_b3', pretrained=False, num_classes=0)\n            \n            # Load the offline weights\n            if os.path.exists(weights_path):\n                print(f\"🔄 Loading offline EfficientNet-B3 weights from: {weights_path}\")\n                state_dict = torch.load(weights_path, map_location='cpu', weights_only=False)\n                \n                # Load weights into the model (ignore classifier since we're using num_classes=0)\n                self.backbone.load_state_dict(state_dict, strict=False)\n                print(\"✅ Successfully loaded offline EfficientNet-B3 weights!\")\n            else:\n                print(f\"⚠️ Weights file not found at {weights_path}, using random initialization\")\n                \n        except Exception as e:\n            print(f\"❌ Error loading offline weights: {e}\")\n            print(\"🔄 Falling back to timm without pre-training...\")\n            self.backbone = timm.create_model('efficientnet_b3', pretrained=False, num_classes=0)\n        \n        # Get feature dimension\n        feature_dim = self.backbone.num_features\n        \n        # Classification head with dropout\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(feature_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n        \n    def forward(self, x):\n        features = self.backbone(x)\n        logits = self.classifier(features)\n        return logits # Initialize model\n\n# Using original EfficientNet approach\n\ndef calculate_class_weights(df):\n    \"\"\"Calculate class weights with 13x multiplier for Aneurysm Present\"\"\"\n    pos_counts = df[Config.LABEL_COLS].sum()\n    neg_counts = len(df) - pos_counts\n    \n    # Standard frequency-based weights\n    class_weights = neg_counts / (pos_counts + 1e-8)\n    class_weights = np.minimum(class_weights, 100.0)  # Cap at 100\n    \n    # Apply 13x multiplier to \"Aneurysm Present\" (matches competition metric)\n    class_weights.iloc[-1] = class_weights.iloc[-1] * 13.0\n    \n    return torch.tensor(class_weights.values, dtype=torch.float32)\n\nprint(\"✅ Model definition loaded\")\n\n# ====================================================\n# CELL 4: TRAINING PIPELINE\n# ====================================================\n\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    num_batches = 0\n    \n    for batch in tqdm(loader, desc=\"Training\"):\n        images = batch['image'].to(device, non_blocking=True)\n        labels = batch['labels'].to(device, non_blocking=True)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        with torch.cuda.amp.autocast(enabled=Config.MIXED_PRECISION):\n            logits = model(images)\n            loss = criterion(logits, labels)\n        \n        # Backward pass\n        if Config.MIXED_PRECISION:\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            loss.backward()\n            optimizer.step()\n        \n        total_loss += loss.item()\n        num_batches += 1\n    \n    return total_loss / num_batches\n\ndef validate_epoch(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    num_batches = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Validating\"):\n            images = batch['image'].to(device, non_blocking=True)\n            labels = batch['labels'].to(device, non_blocking=True)\n            \n            with torch.cuda.amp.autocast(enabled=Config.MIXED_PRECISION):\n                logits = model(images)\n                loss = criterion(logits, labels)\n            \n            total_loss += loss.item()\n            num_batches += 1\n            \n            # Collect predictions for AUC\n            probs = torch.sigmoid(logits).cpu().numpy()\n            all_preds.append(probs)\n            all_labels.append(labels.cpu().numpy())\n    \n    # Calculate AUC\n    if len(all_preds) > 0:\n        all_preds = np.vstack(all_preds)\n        all_labels = np.vstack(all_labels)\n        \n        try:\n            auc_scores = []\n            for i in range(len(Config.LABEL_COLS)):\n                if len(np.unique(all_labels[:, i])) > 1:\n                    auc = roc_auc_score(all_labels[:, i], all_preds[:, i])\n                    auc_scores.append(auc)\n                else:\n                    auc_scores.append(0.5)\n            \n            # Weighted AUC (13x weight for Aneurysm Present)\n            weights = [1.0] * (len(Config.LABEL_COLS) - 1) + [13.0]\n            weighted_auc = np.average(auc_scores, weights=weights)\n        except:\n            weighted_auc = 0.5\n    else:\n        weighted_auc = 0.5\n    \n    return total_loss / num_batches, weighted_auc\n\ndef main_training():\n    print(\"🚀 STAGE 2: ANEURYSM CLASSIFICATION WITH EFFICIENTNET-B3\")\n    \n    # Load data\n    train_df = pd.read_csv(Config.TRAIN_CSV_PATH)\n    \n    if Config.DEBUG_MODE:\n        train_df = train_df.head(Config.DEBUG_SAMPLES)\n    \n    print(f\"Training samples: {len(train_df)}\")\n    print(f\"Aneurysm cases: {train_df['Aneurysm Present'].sum()}\")\n    \n    # Initialize Stage 1 predictor\n    stage1_predictor = Stage1Predictor(Config.STAGE1_MODEL_PATH)\n    \n    # Create training data with ROIs\n    training_df = create_training_data(train_df, stage1_predictor)\n    \n    # Calculate class weights\n    class_weights = calculate_class_weights(training_df)\n    print(f\"Class weights: {class_weights}\")\n    \n    # Create criterion with class weights\n    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights).to(Config.DEVICE)\n    \n    # Mixed precision scaler\n    global scaler\n    scaler = torch.cuda.amp.GradScaler(enabled=Config.MIXED_PRECISION)\n    \n    # 5-fold cross-validation\n    skf = StratifiedKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=42)\n    \n    # Use Aneurysm Present for stratification\n    fold_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(training_df, training_df['Aneurysm Present'])):\n        print(f\"\\n{'='*50}\")\n        print(f\"FOLD {fold + 1}/{Config.N_FOLDS}\")\n        print(f\"{'='*50}\")\n        \n        # Split data\n        train_fold_df = training_df.iloc[train_idx].reset_index(drop=True)\n        val_fold_df = training_df.iloc[val_idx].reset_index(drop=True)\n        \n        print(f\"Train ROIs: {len(train_fold_df)}, Val ROIs: {len(val_fold_df)}\")\n        \n        # Create datasets\n        train_dataset = AneurysmClassificationDataset(train_fold_df, mode='train')\n        val_dataset = AneurysmClassificationDataset(val_fold_df, mode='val')\n        \n        # Create loaders (tuned for throughput)\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=Config.BATCH_SIZE,\n            shuffle=True,\n            num_workers=8,\n            pin_memory=True,\n            persistent_workers=True,\n            prefetch_factor=8,\n        )\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=Config.BATCH_SIZE,\n            shuffle=False,\n            num_workers=8,\n            pin_memory=True,\n            persistent_workers=True,\n            prefetch_factor=8,\n        )\n        \n        # Initialize model\n        model = AneurysmEfficientNet().to(Config.DEVICE)\n        \n        # Optimizer with different learning rates\n        optimizer = optim.AdamW([\n            {'params': model.backbone.parameters(), 'lr': Config.LEARNING_RATE * 0.1},  # Lower LR for backbone\n            {'params': model.classifier.parameters(), 'lr': Config.LEARNING_RATE}\n        ], weight_decay=1e-4)\n\n        # Multi-GPU if available\n        if torch.cuda.device_count() > 1:\n            model = nn.DataParallel(model)\n        \n        # Scheduler\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.EPOCHS)\n        \n        # Training loop\n        best_auc = 0\n        \n        for epoch in range(Config.EPOCHS):\n            print(f\"\\nEpoch {epoch+1}/{Config.EPOCHS}\")\n            \n            # Train\n            train_loss = train_epoch(model, train_loader, optimizer, criterion, Config.DEVICE)\n            \n            # Validate\n            val_loss, val_auc = validate_epoch(model, val_loader, criterion, Config.DEVICE)\n            \n            # Step scheduler\n            scheduler.step()\n            \n            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f}\")\n            \n            # Save best model\n            if val_auc > best_auc:\n                best_auc = val_auc\n                torch.save({\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'val_auc': val_auc,\n                    'epoch': epoch,\n                    'fold': fold\n                }, f'stage2_fold_{fold}_best.pth')\n                print(f\"💾 Saved best model (AUC: {val_auc:.4f})\")\n        \n        fold_scores.append(best_auc)\n        print(f\"Fold {fold + 1} best AUC: {best_auc:.4f}\")\n    \n    # Final results\n    mean_cv_score = np.mean(fold_scores)\n    print(f\"\\n✅ Cross-validation complete!\")\n    print(f\"Mean CV AUC: {mean_cv_score:.4f} ± {np.std(fold_scores):.4f}\")\n    print(f\"Individual fold scores: {fold_scores}\")\n\nprint(\"✅ Training pipeline loaded\")\n\n# ====================================================\n# CELL 5: INFERENCE & SUBMISSION\n# ====================================================\n\nclass InferenceConfig:\n    \"\"\"Configuration for inference server\"\"\"\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    ID_COL = 'SeriesInstanceUID'\n    LABEL_COLS = [\n        'Left Infraclinoid Internal Carotid Artery', 'Right Infraclinoid Internal Carotid Artery',\n        'Left Supraclinoid Internal Carotid Artery', 'Right Supraclinoid Internal Carotid Artery',\n        'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery', 'Anterior Communicating Artery',\n        'Left Anterior Cerebral Artery', 'Right Anterior Cerebral Artery',\n        'Left Posterior Communicating Artery', 'Right Posterior Communicating Artery',\n        'Basilar Tip', 'Other Posterior Circulation', 'Aneurysm Present',\n    ]\n\nclass ModelEnsemble:\n    \"\"\"Ensemble of Stage 2 models for inference\"\"\"\n    def __init__(self, model_paths, device):\n        self.device = device\n        self.models = []\n        \n        for path in model_paths:\n            try:\n                model = AneurysmEfficientNet().to(device)\n                checkpoint = torch.load(path, map_location=device, weights_only=False)\n                \n                if 'model_state_dict' in checkpoint:\n                    state_dict = checkpoint['model_state_dict']\n                else:\n                    state_dict = checkpoint\n                \n                # Handle DataParallel wrapper\n                if any(key.startswith('module.') for key in state_dict.keys()):\n                    state_dict = {key.replace('module.', ''): value for key, value in state_dict.items()}\n                \n                model.load_state_dict(state_dict)\n                model.eval()\n                self.models.append(model)\n                print(f\"Loaded model: {path}\")\n            except Exception as e:\n                print(f\"Error loading {path}: {e}\")\n        \n        print(f\"Loaded {len(self.models)} models for ensemble\")\n    \n    def predict_single(self, series_path):\n        \"\"\"Predict for a single series\"\"\"\n        # Initialize predictors once and reuse\n        global _shared_stage1_predictor\n        if '_shared_stage1_predictor' not in globals() or _shared_stage1_predictor is None:\n            _shared_stage1_predictor = Stage1Predictor(Config.STAGE1_MODEL_PATH)\n        roi_extractor = ROIExtractor(_shared_stage1_predictor)\n        \n        # Extract ROIs\n        rois = roi_extractor.extract_top3_rois(series_path)\n        \n        # Prepare images\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        roi_images = []\n        for roi_data in rois:\n            roi_image = roi_data['roi_image']\n            roi_pil = Image.fromarray((roi_image * 255).astype(np.uint8))\n            roi_tensor = transform(roi_pil).unsqueeze(0).to(self.device)\n            roi_images.append(roi_tensor)\n        \n        # Ensemble predictions\n        all_predictions = []\n        \n        with torch.no_grad():\n            for model in self.models:\n                model_preds = []\n                for roi_tensor in roi_images:\n                    logits = model(roi_tensor)\n                    probs = torch.sigmoid(logits).cpu().numpy()[0]\n                    model_preds.append(probs)\n                \n                # Average predictions across ROIs\n                avg_pred = np.mean(model_preds, axis=0)\n                all_predictions.append(avg_pred)\n        \n        # Average ensemble predictions\n        ensemble_pred = np.mean(all_predictions, axis=0)\n        \n        return ensemble_pred\n\nclass InferenceDICOMProcessor:\n    \"\"\"DICOM processor for inference\"\"\"\n    def __init__(self):\n        pass\n\n# Global variables for model ensemble\nmodel_ensemble = None\nprocessor = None\n\ndef initialize_models():\n    \"\"\"Initialize models - called once at startup\"\"\"\n    global model_ensemble, processor\n    \n    print(\"Initializing models...\")\n    \n    # Model paths - adjust these to match your uploaded dataset structure\n    model_paths = [\n        'stage2_fold_0_best.pth',\n        'stage2_fold_1_best.pth',\n        'stage2_fold_2_best.pth',\n        'stage2_fold_3_best.pth',\n        'stage2_fold_4_best.pth',\n    ]\n    \n    # Check if models exist, use available ones\n    available_models = [path for path in model_paths if os.path.exists(path)]\n    \n    if not available_models:\n        print(\"Warning: No trained models found! Using dummy predictions.\")\n        model_ensemble = None\n    else:\n        try:\n            model_ensemble = ModelEnsemble(available_models, InferenceConfig.DEVICE)\n            print(\"Models initialized successfully!\")\n        except Exception as e:\n            print(f\"Error initializing models: {e}\")\n            model_ensemble = None\n    \n    processor = InferenceDICOMProcessor()\n\ndef predict(series_path: str) -> pl.DataFrame:\n    \"\"\"Make predictions for the competition API\"\"\"\n    global model_ensemble, processor\n    \n    # Initialize models on first call (lazy loading)\n    if model_ensemble is None and processor is None:\n        initialize_models()\n    \n    series_id = os.path.basename(series_path)\n    \n    try:\n        if model_ensemble is not None:\n            # Use trained ensemble\n            predictions = model_ensemble.predict_single(series_path)\n        else:\n            # Fallback: extract metadata and make informed dummy predictions\n            print(f\"Using fallback prediction for {series_id}\")\n            \n            # Load DICOM metadata\n            all_filepaths = []\n            for root, _, files in os.walk(series_path):\n                for file in files:\n                    if file.endswith('.dcm'):\n                        all_filepaths.append(os.path.join(root, file))\n            \n            if all_filepaths:\n                ds = pydicom.dcmread(all_filepaths[0], force=True)\n                modality = getattr(ds, 'Modality', 'UNKNOWN')\n                \n                # Slightly better informed predictions based on modality\n                if modality in ['CTA', 'MRA']:\n                    # Vascular imaging - slightly higher probability\n                    base_prob = 0.1\n                else:\n                    # Other modalities - lower baseline\n                    base_prob = 0.05\n                \n                # Add some noise to make predictions more realistic\n                predictions = np.random.normal(base_prob, 0.02, len(InferenceConfig.LABEL_COLS))\n                predictions = np.clip(predictions, 0.001, 0.999)\n            else:\n                # No DICOM files found\n                predictions = np.full(len(InferenceConfig.LABEL_COLS), 0.5)\n\n        # Ensure predictions is numpy array and convert to list safely\n        if not isinstance(predictions, np.ndarray):\n            predictions = np.array(predictions)\n        \n        # Create prediction DataFrame\n        prediction_df = pl.DataFrame(\n            data=[[series_id] + predictions.tolist()],\n            schema=[InferenceConfig.ID_COL, *InferenceConfig.LABEL_COLS],\n            orient='row',\n        )\n        \n    except Exception as e:\n        print(f\"Error processing {series_id}: {e}\")\n        # Return safe default predictions\n        prediction_df = pl.DataFrame(\n            data=[[series_id] + [0.5] * len(InferenceConfig.LABEL_COLS)],\n            schema=[InferenceConfig.ID_COL, *InferenceConfig.LABEL_COLS],\n            orient='row',\n        )\n    \n    # IMPORTANT: Remove SeriesInstanceUID before returning (API requirement)\n    prediction_df = prediction_df.drop(InferenceConfig.ID_COL)\n    \n    # IMPORTANT: Disk cleanup to prevent \"out of disk space\" errors\n    shutil.rmtree('/kaggle/shared', ignore_errors=True)\n    \n    return prediction_df","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:42:32.95332Z","iopub.execute_input":"2025-08-16T18:42:32.953816Z","iopub.status.idle":"2025-08-16T18:42:33.136017Z","shell.execute_reply.started":"2025-08-16T18:42:32.953795Z","shell.execute_reply":"2025-08-16T18:42:33.135392Z"}},"outputs":[{"name":"stdout","text":"✅ Configuration loaded - Device: cuda\n✅ Custom 3D UNet and transforms loaded (MONAI-free!)\n✅ Data loading and ROI extraction functions loaded\n✅ Model definition loaded\n✅ Training pipeline loaded\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ====================================================\n# SERVER EXECUTION\n# ====================================================\n\n# Initialize the inference server\ninference_server = kaggle_evaluation.rsna_inference_server.RSNAInferenceServer(predict)\n\nprint(\"✅ Inference and submission pipeline loaded\")\n\n# ====================================================\n# CELL 6: MAIN EXECUTION\n# ====================================================\n\nif __name__ == \"__main__\":\n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        # Production mode - serve the API\n        print(\"Starting inference server...\")\n        inference_server.serve()\n    else:\n        # Training mode\n        print(\"Ready for Stage 2 training!\")\n        print(\"Uncomment the line below to start training:\")\n        print(\"# main_training()\")\n        \n        # Uncomment to start training\n        main_training()\n        \n        # Or run local testing\n        print(\"Running local gateway for testing...\")\n        inference_server.run_local_gateway()\n        \n        # Display results if available\n        results_path = '/kaggle/working/submission.parquet'\n        if os.path.exists(results_path):\n            results_df = pl.read_parquet(results_path)\n            print(\"Submission preview:\")\n            print(results_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:42:41.820069Z","iopub.execute_input":"2025-08-16T18:42:41.820384Z","execution_failed":"2025-08-16T18:49:28.279Z"}},"outputs":[{"name":"stdout","text":"✅ Inference and submission pipeline loaded\nReady for Stage 2 training!\nUncomment the line below to start training:\n# main_training()\n🚀 STAGE 2: ANEURYSM CLASSIFICATION WITH EFFICIENTNET-B3\nTraining samples: 4405\nAneurysm cases: 1893\nLoading Stage 1 model...\n✅ Stage 1 model loaded successfully\n🔄 Extracting ROIs for training data...\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 0/4405 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10004044428023505108375152878107656647\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.061\n🔍 DEBUG: Poor segmentation quality (0.061 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 188 total (every 3.8)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 1/4405 [00:03<3:55:55,  3.21s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10004684224894397679901841656954650085\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.064\n🔍 DEBUG: Poor segmentation quality (0.064 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 147 total (every 2.9)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 2/4405 [00:04<2:41:46,  2.20s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10005158603912009425635473100344077317\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.070\n🔍 DEBUG: Poor segmentation quality (0.070 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 276 total (every 5.5)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 3/4405 [00:06<2:35:26,  2.12s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10009383108068795488741533244914370182\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.065\n🔍 DEBUG: Poor segmentation quality (0.065 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 224 total (every 4.5)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 4/4405 [00:10<3:18:44,  2.71s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10012790035410518400400834395242853657\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.070\n🔍 DEBUG: Poor segmentation quality (0.070 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Using all 1 files (less than 50)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 5/4405 [00:14<3:53:59,  3.19s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10014757658335054766479957992112625961\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.066\n🔍 DEBUG: Poor segmentation quality (0.066 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 856 total (every 17.1)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 6/4405 [00:16<3:26:32,  2.82s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10021411248005513321236647460239137906\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.071\n🔍 DEBUG: Poor segmentation quality (0.071 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 239 total (every 4.8)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 7/4405 [00:18<3:06:44,  2.55s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10022688097731894079510930966432818105\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.064\n🔍 DEBUG: Poor segmentation quality (0.064 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 178 total (every 3.6)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 8/4405 [00:22<3:33:08,  2.91s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10022796280698534221758473208024838831\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.065\n🔍 DEBUG: Poor segmentation quality (0.065 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 671 total (every 13.4)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 9/4405 [00:24<3:14:56,  2.66s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10023411164590664678534044036963716636\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.068\n🔍 DEBUG: Poor segmentation quality (0.068 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 205 total (every 4.1)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 10/4405 [00:26<3:00:15,  2.46s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10030095840917973694487307992374923817\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.075\n🔍 DEBUG: Poor segmentation quality (0.075 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 451 total (every 9.0)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 11/4405 [00:28<2:51:53,  2.35s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10030804647049037739144303822498146901\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.072\n🔍 DEBUG: Poor segmentation quality (0.072 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Using all 1 files (less than 50)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 12/4405 [00:29<2:17:24,  1.88s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10034081836061566510187499603024895557\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.074\n🔍 DEBUG: Poor segmentation quality (0.074 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 83 total (every 1.7)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 13/4405 [00:30<2:11:49,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10035643165968342618460849823699311381\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.075\n🔍 DEBUG: Poor segmentation quality (0.075 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 228 total (every 4.6)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 14/4405 [00:32<2:16:02,  1.86s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10035782880104673269567641444954004745\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.073\n🔍 DEBUG: Poor segmentation quality (0.073 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Using all 1 files (less than 50)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 15/4405 [00:35<2:39:30,  2.18s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10037266473301611864455091971206084528\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.062\n🔍 DEBUG: Poor segmentation quality (0.062 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 201 total (every 4.0)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 16/4405 [00:38<2:53:19,  2.37s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10040419508532196461125208817600495772\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.070\n🔍 DEBUG: Poor segmentation quality (0.070 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 188 total (every 3.8)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 17/4405 [00:40<2:34:40,  2.12s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10042423585566957032411171949972906248\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.059\n🔍 DEBUG: Poor segmentation quality (0.059 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 116 total (every 2.3)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 18/4405 [00:41<2:23:08,  1.96s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10042474696169267476037627878420766468\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.073\n🔍 DEBUG: Poor segmentation quality (0.073 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 587 total (every 11.7)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 19/4405 [00:43<2:22:34,  1.95s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10046318991957083423208748012349179640\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.068\n🔍 DEBUG: Poor segmentation quality (0.068 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 136 total (every 2.7)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 20/4405 [00:45<2:20:47,  1.93s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10048925006598672000564912882060003872\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.077\n🔍 DEBUG: Poor segmentation quality (0.077 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 594 total (every 11.9)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 21/4405 [00:47<2:19:34,  1.91s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10052893794239333131781802642788307307\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.069\n🔍 DEBUG: Poor segmentation quality (0.069 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 74 total (every 1.5)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   0%|          | 22/4405 [00:49<2:18:35,  1.90s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10057981374227560278263065500472865434\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.069\n🔍 DEBUG: Poor segmentation quality (0.069 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 723 total (every 14.5)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   1%|          | 23/4405 [00:51<2:25:02,  1.99s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10058383541003792190302541266378919328\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.063\n🔍 DEBUG: Poor segmentation quality (0.063 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Smart sampling - selected 50 files from 88 total (every 1.8)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   1%|          | 24/4405 [00:52<2:11:55,  1.81s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10058588444796585220635465116646088095\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.064\n🔍 DEBUG: Poor segmentation quality (0.064 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Using all 30 files (less than 50)\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs:   1%|          | 25/4405 [00:53<1:46:13,  1.46s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 DEBUG: Quality-based ROI extraction for 1.2.826.0.1.3680043.8.498.10063454172499468887877935052136698373\n🔍 DEBUG: Segmentation mask shape: (48, 112, 112)\n🔍 DEBUG: Segmentation quality score: 0.084\n🔍 DEBUG: Poor segmentation quality (0.084 < 0.25), using fallback\n🔍 DEBUG: Using quality-aware fallback (multi-center ROIs)\n🔍 DEBUG: Using all 2 files (less than 50)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}