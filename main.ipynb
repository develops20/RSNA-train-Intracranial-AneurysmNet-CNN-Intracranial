{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":99552,"databundleVersionId":13441085,"sourceType":"competition"},{"sourceId":12837944,"sourceType":"datasetVersion","datasetId":8119423}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/stage1-aneurysmnet-intracranial-training-nb153?scriptVersionId=258046122\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# ====================================================\n# RSNA INTRACRANIAL ANEURYSM - STAGE 1 TRAINING (v2)\n# Uses Stage-0 prebuilt v2 cache (volumes, masks, pseudo_masks, brainmasks, manifest)\n# Two-phase training with per-sample segmentation weights and rich progress logs:\n#   Phase 1: real masks weighted (real_seg_weight), synthetic seg weight = 0.0\n#   Phase 2: real same, synthetic seg weight = small (default 0.075)\n# Saves: stage1_phase1_best.pth, stage1_phase2_best.pth, stage1_segmentation_best.pth\n# ====================================================\n\nimport os\nimport math\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================\n# Config\n# ====================================================\nclass Config:\n    # --- Paths ---\n    PREBUILT_ROOT = \"/kaggle/input/rsna2025-v2-intracranial-aneurysm-detection-nb153/stage1_AneurysmNet_prebuilt_v2\"\n    MANIFEST_PATH = os.path.join(PREBUILT_ROOT, \"meta/manifest.csv\")\n    VOLUMES_DIR   = os.path.join(PREBUILT_ROOT, \"volumes\")\n    MASKS_DIR     = os.path.join(PREBUILT_ROOT, \"masks\")          # real\n    PSEUDO_DIR    = os.path.join(PREBUILT_ROOT, \"pseudo_masks\")   # synthetic\n    BRAINMASKS_DIR= os.path.join(PREBUILT_ROOT, \"brainmasks\")\n\n    # --- Data ---\n    TARGET_SIZE = (48, 112, 112)  # (D,H,W)\n    USE_BRAINMASKS = True\n    BRAINMASK_KEY = 'm'\n    BRAINMASK_MIN_FRAC = 0.02  # if below, skip masking\n\n    # --- Training ---\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MIXED_PRECISION = True\n    STAGE1_BATCH_SIZE = 8  # bump to 12/16 if GPU mem allows\n    NUM_WORKERS = 2        # bump to 4/8 to reduce CPU bottlenecks\n    PREFETCH_FACTOR = 2\n    PERSISTENT_WORKERS = True\n    STAGE1_LR = 2e-4\n    WEIGHT_DECAY = 1e-4\n    EPOCHS_PHASE1 = 15\n    EPOCHS_PHASE2 = 15\n    EARLY_STOP_PATIENCE = 5\n\n    # --- Segmentation weights ---\n    REAL_SEG_DEFAULT_W = 0.7      # used if manifest lacks real_seg_weight\n    PHASE1_SYNTH_SEG_W = 0.0\n    PHASE2_SYNTH_SEG_W = 0.075\n    FOCAL_LOSS_WEIGHT = 0.2\n\n    # --- Splits ---\n    FOLDS = 1   # set >1 later if you want CV here\n    SEED = 42\n\n# ====================================================\n# Utils\n# ====================================================\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True  # speeds up fixed-size convs\n\n\ndef load_manifest_df() -> pd.DataFrame:\n    df = pd.read_csv(Config.MANIFEST_PATH)\n    # Required columns: series_id, label, vol_relpath\n    for col in [\"series_id\", \"label\", \"vol_relpath\"]:\n        if col not in df.columns:\n            raise RuntimeError(f\"Manifest missing required column: {col}\")\n    return df\n\n\ndef gpu_mem_str():\n    if not torch.cuda.is_available():\n        return \"cpu\"\n    try:\n        a = torch.cuda.memory_allocated() / (1024**3)\n        r = torch.cuda.memory_reserved() / (1024**3)\n        return f\"{a:.2f}G/{r:.2f}G\"\n    except Exception:\n        return \"gpu\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\nclass PrebuiltDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, phase_synth_w: float):\n        self.df = df.reset_index(drop=True)\n        self.phase_synth_w = float(phase_synth_w)\n\n    def __len__(self):\n        return len(self.df)\n\n    def _load_volume(self, sid: str) -> np.ndarray:\n        path = os.path.join(Config.VOLUMES_DIR, f\"{sid}.npy\")\n        v = np.load(path).astype(np.float32)  # (D,H,W), 0..1\n        v = np.nan_to_num(v, nan=0.0, posinf=1.0, neginf=0.0)\n        return v\n\n    def _load_brainmask(self, sid: str, frac: Optional[float], shp: Tuple[int,int,int]):\n        if not Config.USE_BRAINMASKS:\n            return None\n        if frac is not None and float(frac) < Config.BRAINMASK_MIN_FRAC:\n            return None\n        p = os.path.join(Config.BRAINMASKS_DIR, f\"{sid}_brainmask.npz\")\n        if not os.path.exists(p):\n            return None\n        try:\n            bm = np.load(p)[Config.BRAINMASK_KEY].astype(np.float32)\n            bm = np.nan_to_num(bm, nan=0.0, posinf=1.0, neginf=0.0)\n            if bm.shape != shp or bm.sum() <= 0:\n                return None\n            return bm\n        except Exception:\n            return None\n\n    def _load_mask(self, sid: str, mask_rel: str, is_synth: int, label: int) -> Tuple[np.ndarray, bool]:\n        # Returns (mask[D,H,W] float32 in {0,1}, is_synthetic: bool)\n        if isinstance(mask_rel, str) and len(mask_rel) > 0:\n            if mask_rel.startswith('masks/'):\n                p = os.path.join(Config.PREBUILT_ROOT, mask_rel)\n                if os.path.exists(p):\n                    m = np.load(p).astype(np.float32)\n                    m = np.nan_to_num(m, nan=0.0, posinf=1.0, neginf=0.0)\n                    return (m > 0).astype(np.float32), False\n            elif mask_rel.startswith('pseudo_masks/'):\n                p = os.path.join(Config.PREBUILT_ROOT, mask_rel)\n                if os.path.exists(p):\n                    m = np.load(p).astype(np.float32)\n                    m = np.nan_to_num(m, nan=0.0, posinf=1.0, neginf=0.0)\n                    return (m > 0).astype(np.float32), True\n        # Fallbacks\n        D,H,W = Config.TARGET_SIZE\n        if int(label) == 1:\n            return np.zeros((D,H,W), dtype=np.float32), True\n        else:\n            return np.zeros((D,H,W), dtype=np.float32), False\n\n    def __getitem__(self, idx):\n        r = self.df.iloc[idx]\n        sid = str(r['series_id'])\n        label = int(r['label'])\n        mask_rel = r.get('mask_relpath', '') if isinstance(r.get('mask_relpath', ''), str) else ''\n        is_synth_col = int(r.get('is_synthetic', 0))\n        real_seg_weight = r.get('real_seg_weight', np.nan)\n        brain_frac = r.get('brain_voxel_fraction', np.nan)\n\n        vol = self._load_volume(sid)  # (D,H,W)\n        bm = self._load_brainmask(sid, brain_frac if pd.notna(brain_frac) else None, vol.shape)\n        if bm is not None:\n            vol = vol * bm  # gate\n\n        mask, is_synth = self._load_mask(sid, mask_rel, is_synth_col, label)\n\n        # per-sample seg weight\n        if is_synth:\n            seg_w = self.phase_synth_w\n        else:\n            if pd.notna(real_seg_weight):\n                try:\n                    rsw = float(real_seg_weight)\n                except Exception:\n                    rsw = Config.REAL_SEG_DEFAULT_W\n            else:\n                rsw = Config.REAL_SEG_DEFAULT_W\n            seg_w = float(np.clip(rsw, 0.2, 1.0))\n\n        # to tensors\n        vol_t = torch.from_numpy(vol).unsqueeze(0)         # [1,D,H,W]\n        mask_t = torch.from_numpy((mask > 0).astype(np.float32)).unsqueeze(0)\n        label_t = torch.tensor([float(label)], dtype=torch.float32)\n        segw_t  = torch.tensor([float(seg_w)], dtype=torch.float32)\n\n        return {\n            'series_id': sid,\n            'volume': vol_t,\n            'mask': mask_t,\n            'label': label_t,\n            'seg_weight': segw_t,\n            'is_synthetic_mask': torch.tensor([1.0 if is_synth else 0.0], dtype=torch.float32),\n        }\n\n# ====================================================\n# Simple 3D UNet + classifier head\n# ====================================================\nclass ConvBlock3D(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv3d(in_ch, out_ch, 3, padding=1), nn.GroupNorm(num_groups=8, num_channels=out_ch), nn.ReLU(inplace=True),\n            nn.Conv3d(out_ch, out_ch, 3, padding=1), nn.GroupNorm(num_groups=8, num_channels=out_ch), nn.ReLU(inplace=True)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass UNet3D(nn.Module):\n    def __init__(self, in_ch=1, base=24):\n        super().__init__()\n        b = base\n        self.enc1 = ConvBlock3D(in_ch, b)\n        self.pool1 = nn.MaxPool3d(2)\n        self.enc2 = ConvBlock3D(b, b*2)\n        self.pool2 = nn.MaxPool3d(2)\n        self.enc3 = ConvBlock3D(b*2, b*4)\n        self.pool3 = nn.MaxPool3d((2,2,2))\n        self.bott = ConvBlock3D(b*4, b*8)\n        self.up3 = nn.ConvTranspose3d(b*8, b*4, 2, stride=2)\n        self.dec3 = ConvBlock3D(b*8, b*4)\n        self.up2 = nn.ConvTranspose3d(b*4, b*2, 2, stride=2)\n        self.dec2 = ConvBlock3D(b*4, b*2)\n        self.up1 = nn.ConvTranspose3d(b*2, b, 2, stride=2)\n        self.dec1 = ConvBlock3D(b*2, b)\n        self.seg_head = nn.Conv3d(b, 1, 1)\n        # classification head from bottleneck features\n        self.cls_pool = nn.AdaptiveAvgPool3d(1)\n        self.cls_head = nn.Linear(b*8, 1)\n\n    def forward(self, x):  # x: [B,1,D,H,W]\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool1(e1))\n        e3 = self.enc3(self.pool2(e2))\n        b  = self.bott(self.pool3(e3))\n        # decoder\n        d3 = self.up3(b)\n        d3 = torch.cat([d3, e3], dim=1)\n        d3 = self.dec3(d3)\n        d2 = self.up2(d3)\n        d2 = torch.cat([d2, e2], dim=1)\n        d2 = self.dec2(d2)\n        d1 = self.up1(d2)\n        d1 = torch.cat([d1, e1], dim=1)\n        d1 = self.dec1(d1)\n        seg = self.seg_head(d1)  # [B,1,D,H,W]\n        # classifier from bottleneck\n        cls = self.cls_head(self.cls_pool(b).flatten(1))  # [B,1]\n        return seg, cls\n\n# ====================================================\n# Losses\n# ====================================================\nclass DiceLoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n    def forward(self, logits, targets, reduction='mean'):\n        probs = torch.sigmoid(logits)\n        num = 2 * (probs * targets).sum(dim=(2,3,4)) + self.eps\n        den = (probs.pow(2) + targets.pow(2)).sum(dim=(2,3,4)) + self.eps\n        dice = 1 - (num / den)  # per-sample\n        if reduction == 'none':\n            return dice\n        return dice.mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0, eps=1e-6):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.eps = eps\n    def forward(self, logits, targets, reduction='mean'):\n        probs = torch.sigmoid(logits).clamp(self.eps, 1-self.eps)\n        ce = -(targets*torch.log(probs) + (1-targets)*torch.log(1-probs))\n        pt = torch.where(targets==1, probs, 1-probs)\n        loss = self.alpha * (1-pt).pow(self.gamma) * ce\n        loss = loss.mean(dim=(2,3,4))  # per-sample\n        if reduction == 'none':\n            return loss\n        return loss.mean()\n\nclass EnhancedCombinedLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dice_loss = DiceLoss()\n        self.focal_loss = FocalLoss(alpha=0.25, gamma=2.0)\n        self.bce_vox = nn.BCEWithLogitsLoss(reduction='none')\n        self.bce_cls = nn.BCEWithLogitsLoss()\n    def forward(self, seg_logits, cls_logits, seg_targets, cls_targets, seg_weights: torch.Tensor):\n        # clamp seg logits to avoid AMP overflow\n        seg_logits = torch.nan_to_num(seg_logits, nan=0.0, posinf=20.0, neginf=-20.0)\n        B = seg_logits.shape[0]\n        dice_ps = self.dice_loss(seg_logits, seg_targets, reduction='none')\n        focal_ps= self.focal_loss(seg_logits, seg_targets, reduction='none')\n        bce_elem= self.bce_vox(seg_logits, seg_targets)\n        bce_ps  = bce_elem.view(B, -1).mean(dim=1)\n        dice_ps = torch.nan_to_num(dice_ps, nan=0.0)\n        focal_ps= torch.nan_to_num(focal_ps, nan=0.0)\n        bce_ps  = torch.nan_to_num(bce_ps,  nan=0.0)\n        seg_ps  = 0.5*dice_ps + 0.3*bce_ps + Config.FOCAL_LOSS_WEIGHT*focal_ps\n        seg_ps  = torch.nan_to_num(seg_ps, nan=0.0)\n        seg_w   = seg_weights.view(-1)\n        if (seg_w == 0).all():\n            seg_loss = seg_ps.new_tensor(0.0)\n        else:\n            seg_loss = (seg_ps * seg_w).mean()\n        # classification loss\n        cls_logits = torch.nan_to_num(cls_logits, nan=0.0, posinf=20.0, neginf=-20.0)\n        cls_loss= self.bce_cls(cls_logits.view(-1), cls_targets.view(-1))\n        total   = seg_loss + cls_loss\n        return total, seg_loss.detach(), cls_loss.detach()\n\n# ====================================================\n# Train / Validate with progress bars\n# ====================================================\n\ndef train_epoch(model, loader, optimizer, criterion, scaler, epoch=None, phase_name=\"P1\"):\n    model.train()\n    t_loss = t_seg = t_cls = 0.0\n    n = 0\n    pbar = tqdm(loader, desc=f\"Train {phase_name}{'' if epoch is None else f' [ep {epoch}]'}\", leave=False)\n    for batch in pbar:\n        vol   = batch['volume'].to(Config.DEVICE, non_blocking=True)\n        mask  = batch['mask'].to(Config.DEVICE, non_blocking=True)\n        label = batch['label'].to(Config.DEVICE, non_blocking=True)\n        segw  = batch['seg_weight'].to(Config.DEVICE, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        with torch.amp.autocast('cuda', enabled=Config.MIXED_PRECISION):\n            seg_logits, cls_logits = model(vol)\n            loss, seg_loss, cls_loss = criterion(seg_logits, cls_logits, mask, label, segw)\n        try:\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        except Exception:\n            optimizer.zero_grad(set_to_none=True)\n            continue\n        bs = vol.size(0)\n        t_loss += loss.item()*bs; t_seg += seg_loss.item()*bs; t_cls += cls_loss.item()*bs; n += bs\n        pbar.set_postfix(loss=f\"{t_loss/max(n,1):.4f}\", seg=f\"{t_seg/max(n,1):.4f}\", cls=f\"{t_cls/max(n,1):.4f}\", lr=f\"{optimizer.param_groups[0]['lr']:.2e}\")\n    return t_loss/n, t_seg/n, t_cls/n\n\n@torch.no_grad()\ndef validate_epoch(model, loader, criterion, epoch=None, phase_name=\"P1\"):\n    model.eval()\n    t_loss = t_seg = t_cls = 0.0\n    n = 0\n    all_probs = []\n    all_labels= []\n    pbar = tqdm(loader, desc=f\"Valid {phase_name}{'' if epoch is None else f' [ep {epoch}]'}\", leave=False)\n    for batch in pbar:\n        vol   = batch['volume'].to(Config.DEVICE, non_blocking=True)\n        mask  = batch['mask'].to(Config.DEVICE, non_blocking=True)\n        label = batch['label'].to(Config.DEVICE, non_blocking=True)\n        segw  = batch['seg_weight'].to(Config.DEVICE, non_blocking=True)\n        seg_logits, cls_logits = model(vol)\n        loss, seg_loss, cls_loss = criterion(seg_logits, cls_logits, mask, label, segw)\n        bs = vol.size(0)\n        t_loss += loss.item()*bs; t_seg += seg_loss.item()*bs; t_cls += cls_loss.item()*bs; n += bs\n        pbar.set_postfix(loss=f\"{t_loss/max(n,1):.4f}\", seg=f\"{t_seg/max(n,1):.4f}\", cls=f\"{t_cls/max(n,1):.4f}\")\n        all_probs.append(torch.sigmoid(cls_logits).detach().cpu().view(-1).numpy())\n        all_labels.append(label.detach().cpu().view(-1).numpy())\n    all_probs = np.concatenate(all_probs) if len(all_probs)>0 else np.array([])\n    all_labels = np.concatenate(all_labels) if len(all_labels)>0 else np.array([])\n    auc = np.nan\n    try:\n        if len(all_probs)>0 and len(np.unique(all_labels)) > 1:\n            auc = float(roc_auc_score(all_labels, all_probs))\n    except Exception:\n        pass\n    return t_loss/n, t_seg/n, t_cls/n, auc\n\n# ====================================================\n# Main\n# ====================================================\n\ndef run_training():\n    set_seed(Config.SEED)\n    df = load_manifest_df()\n\n    # Build a single stratified split (can expand to CV later)\n    y = df['label'].astype(int).values\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=Config.SEED)\n    train_idx, val_idx = next(skf.split(np.zeros_like(y), y))\n    train_df = df.iloc[train_idx].reset_index(drop=True)\n    val_df   = df.iloc[val_idx].reset_index(drop=True)\n\n    # Phase 1 datasets/loads\n    ds_train_p1 = PrebuiltDataset(train_df, phase_synth_w=Config.PHASE1_SYNTH_SEG_W)\n    ds_val_p1   = PrebuiltDataset(val_df,   phase_synth_w=Config.PHASE1_SYNTH_SEG_W)\n    dl_train = DataLoader(ds_train_p1, batch_size=Config.STAGE1_BATCH_SIZE, shuffle=True,\n                          num_workers=Config.NUM_WORKERS, pin_memory=True,\n                          prefetch_factor=Config.PREFETCH_FACTOR,\n                          persistent_workers=Config.PERSISTENT_WORKERS)\n    dl_val   = DataLoader(ds_val_p1,   batch_size=Config.STAGE1_BATCH_SIZE, shuffle=False,\n                          num_workers=Config.NUM_WORKERS, pin_memory=True,\n                          prefetch_factor=Config.PREFETCH_FACTOR,\n                          persistent_workers=Config.PERSISTENT_WORKERS)\n\n    model = UNet3D(in_ch=1, base=24).to(Config.DEVICE)\n    # Multi-GPU (if available)\n    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs via DataParallel\")\n        model = nn.DataParallel(model)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.STAGE1_LR, weight_decay=Config.WEIGHT_DECAY)\n    criterion = EnhancedCombinedLoss().to(Config.DEVICE)\n    scaler = torch.amp.GradScaler('cuda', enabled=Config.MIXED_PRECISION)\n\n    best_loss = float('inf'); best_state = None; patience = 0\n    for epoch in range(1, Config.EPOCHS_PHASE1+1):\n        print(f\"\\n[Phase 1] Epoch {epoch}/{Config.EPOCHS_PHASE1}\")\n        tr_loss, tr_seg, tr_cls = train_epoch(model, dl_train, optimizer, criterion, scaler, epoch=epoch, phase_name='P1')\n        va_loss, va_seg, va_cls, va_auc = validate_epoch(model, dl_val, criterion, epoch=epoch, phase_name='P1')\n        print(f\"Train Loss: {tr_loss:.4f} | Seg: {tr_seg:.4f} | Cls: {tr_cls:.4f} | GPU {gpu_mem_str()}\")\n        print(f\" Val  Loss: {va_loss:.4f} | Seg: {va_seg:.4f} | Cls: {va_cls:.4f} | AUC: {va_auc if not np.isnan(va_auc) else 'NA'} | GPU {gpu_mem_str()}\")\n        if va_loss < best_loss - 1e-5:\n            best_loss = va_loss; best_state = {k:v.detach().cpu() for k,v in (model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()).items()}; patience = 0\n            torch.save(best_state, 'stage1_phase1_best.pth')\n            print(\"ðŸ’¾ Saved Phase 1 best checkpoint\")\n        else:\n            patience += 1\n            if patience >= Config.EARLY_STOP_PATIENCE:\n                print(\"Early stopping Phase 1\")\n                break\n\n    if best_state is not None:\n        if isinstance(model, nn.DataParallel):\n            model.module.load_state_dict(best_state)\n        else:\n            model.load_state_dict(best_state)\n\n    # Phase 2: small synthetic weight\n    print(\"\\n====== PHASE 2: enabling small synthetic seg supervision ======\")\n    ds_train_p2 = PrebuiltDataset(train_df, phase_synth_w=Config.PHASE2_SYNTH_SEG_W)\n    ds_val_p2   = PrebuiltDataset(val_df,   phase_synth_w=Config.PHASE2_SYNTH_SEG_W)\n    dl_train2 = DataLoader(ds_train_p2, batch_size=Config.STAGE1_BATCH_SIZE, shuffle=True,\n                           num_workers=Config.NUM_WORKERS, pin_memory=True,\n                           prefetch_factor=Config.PREFETCH_FACTOR,\n                           persistent_workers=Config.PERSISTENT_WORKERS)\n    dl_val2   = DataLoader(ds_val_p2,   batch_size=Config.STAGE1_BATCH_SIZE, shuffle=False,\n                           num_workers=Config.NUM_WORKERS, pin_memory=True,\n                           prefetch_factor=Config.PREFETCH_FACTOR,\n                           persistent_workers=Config.PERSISTENT_WORKERS)\n\n    # optional: lower LR a bit for fine-tune\n    for g in optimizer.param_groups:\n        g['lr'] = Config.STAGE1_LR * 0.5\n\n    best2 = float('inf'); best2_state = None; patience = 0\n    for epoch in range(1, Config.EPOCHS_PHASE2+1):\n        print(f\"\\n[Phase 2] Epoch {epoch}/{Config.EPOCHS_PHASE2}\")\n        tr_loss, tr_seg, tr_cls = train_epoch(model, dl_train2, optimizer, criterion, scaler, epoch=epoch, phase_name='P2')\n        va_loss, va_seg, va_cls, va_auc = validate_epoch(model, dl_val2, criterion, epoch=epoch, phase_name='P2')\n        print(f\"Train Loss: {tr_loss:.4f} | Seg: {tr_seg:.4f} | Cls: {tr_cls:.4f} | GPU {gpu_mem_str()}\")\n        print(f\" Val  Loss: {va_loss:.4f} | Seg: {va_seg:.4f} | Cls: {va_cls:.4f} | AUC: {va_auc if not np.isnan(va_auc) else 'NA'} | GPU {gpu_mem_str()}\")\n        if va_loss < best2 - 1e-5:\n            best2 = va_loss; best2_state = {k:v.detach().cpu() for k,v in (model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()).items()}; patience = 0\n            torch.save(best2_state, 'stage1_phase2_best.pth')\n            print(\"ðŸ’¾ Saved Phase 2 best checkpoint\")\n        else:\n            patience += 1\n            if patience >= Config.EARLY_STOP_PATIENCE:\n                print(\"Early stopping Phase 2\")\n                break\n\n    final_state = best2_state or best_state or (model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict())\n    torch.save(final_state, 'stage1_segmentation_best.pth')\n    print(\"\\nâœ… Stage 1 complete. Saved: stage1_segmentation_best.pth\")\n\n\nif __name__ == '__main__':\n    run_training()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os, numpy as np, torch, matplotlib.pyplot as plt, cv2, pandas as pd\n# from torch.utils.data import DataLoader, Subset\n\n# device = Config.DEVICE\n\n# # Rebuild the same validation split used in training (first 20% of rows)\n# df = pd.read_csv(Config.TRAIN_CSV_PATH)\n# if Config.DEBUG_MODE:\n#     df = df.head(Config.DEBUG_SAMPLES)\n# val_size = len(df) // 5\n# val_df = df[:val_size].reset_index(drop=True)\n\n# # If you exported a prebuilt dataset, enable this for faster loading\n# # Config.USE_EXTERNAL_CACHE = True\n# # Config.EXTERNAL_CACHE_DIR = '/kaggle/input/<your-prebuilt-dataset-name>/stage1_prebuilt'\n\n# processor = SimpleDICOMProcessor()\n# val_dataset = SimpleSegmentationDataset(val_df, Config.SERIES_DIR, processor, 'val')\n\n# def collate_segmentation_batch(batch_list):\n#     volumes = torch.stack([sample['volume'].contiguous() for sample in batch_list], dim=0)\n#     masks = torch.stack([sample['mask'].contiguous() for sample in batch_list], dim=0)\n#     has = torch.stack([\n#         sample['has_aneurysm'] if isinstance(sample['has_aneurysm'], torch.Tensor)\n#         else torch.tensor(sample['has_aneurysm'], dtype=torch.float32)\n#         for sample in batch_list\n#     ], dim=0)\n#     synth = torch.stack([\n#         sample.get('is_synthetic_mask', torch.tensor(0.0, dtype=torch.float32))\n#         for sample in batch_list\n#     ], dim=0)\n#     series_ids = [sample['series_id'] for sample in batch_list]\n#     return {'volume': volumes, 'mask': masks, 'has_aneurysm': has, 'is_synthetic_mask': synth, 'series_id': series_ids}\n\n# loader_kwargs = dict(batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=4)\n# val_loader_small = DataLoader(Subset(val_dataset, list(range(min(20, len(val_dataset))))),\n#                               shuffle=False, drop_last=False, collate_fn=collate_segmentation_batch, **loader_kwargs)\n\n# # Load model\n# model = Simple3DSegmentationNet().to(device)\n# ckpt = torch.load('stage1_segmentation_best.pth', map_location=device)\n# state = ckpt.get('model_state_dict', ckpt)\n# if any(k.startswith('module.') for k in state.keys()):\n#     state = {k.replace('module.', '', 1): v for k, v in state.items()}\n# _ = model.load_state_dict(state, strict=False)\n# model.eval()\n# print(\"Model loaded for inference.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# to_show = 20\n# shown = 0\n# thr = 0.5\n\n# for batch in val_loader_small:\n#     with torch.no_grad():\n#         vol = batch['volume'].to(device, non_blocking=True)\n#         seg_logits, cls_logits = model(vol)\n#         seg_prob = torch.sigmoid(seg_logits).float().cpu().numpy()  # [B,1,D,H,W]\n#         volumes = batch['volume'].float().cpu().numpy()              # [B,1,D,H,W]\n#     B = volumes.shape[0]\n#     for i in range(B):\n#         series_id = batch['series_id'][i]\n#         v = volumes[i, 0]  # [D,H,W]\n#         p = seg_prob[i, 0] # [D,H,W]\n\n#         # choose best slice by max predicted prob\n#         slice_idx = int(np.argmax(p.max(axis=(1, 2))))\n#         img = v[slice_idx]\n#         mask = p[slice_idx]\n\n#         # overlay heatmap\n#         overlay = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n#         heat = (np.clip(mask, 0, 1) * 255).astype(np.uint8)\n#         heat_color = cv2.applyColorMap(heat, cv2.COLORMAP_JET)\n#         heat_color = cv2.cvtColor(heat_color, cv2.COLOR_BGR2RGB)\n#         overlay_rgb = np.stack([overlay]*3, axis=-1)\n#         alpha = 0.4\n#         blended = ((1 - alpha) * overlay_rgb + alpha * heat_color).astype(np.uint8)\n\n#         # thresholded contours\n#         binmask = (mask > thr).astype(np.uint8)\n#         contours, _ = cv2.findContours(binmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n#         blended_contour = blended.copy()\n#         cv2.drawContours(blended_contour, contours, -1, (0, 255, 0), 1)\n\n#         # quick stats\n#         has_an = int(batch['has_aneurysm'][i].item())\n#         print(f\"{series_id} | label={has_an} | slice={slice_idx} | maxprob={mask.max():.3f} | posfrac={binmask.mean():.4f} | n_comp={len(contours)}\")\n\n#         # show and save\n#         plt.figure(figsize=(9, 3))\n#         plt.subplot(1, 3, 1); plt.imshow(img, cmap='gray'); plt.axis('off'); plt.title('Slice')\n#         plt.subplot(1, 3, 2); plt.imshow(blended); plt.axis('off'); plt.title('Heatmap overlay')\n#         plt.subplot(1, 3, 3); plt.imshow(blended_contour); plt.axis('off'); plt.title(f'> {thr} contours')\n#         plt.tight_layout()\n#         out_path = f\"/kaggle/working/viz_{series_id}_s{slice_idx}.png\"\n#         plt.savefig(out_path, dpi=120, bbox_inches='tight'); plt.show()\n\n#         shown += 1\n#         if shown >= to_show:\n#             break\n#     if shown >= to_show:\n#         break\n\n# print(\"Saved visualizations to /kaggle/working for later inspection.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from scipy import ndimage\n\n# def quick_3d_summary(prob_3d, thr=0.5):\n#     bin3d = (prob_3d > thr).astype(np.uint8)\n#     if bin3d.max() == 0:\n#         return dict(posfrac=0.0, n_comp=0, largest=0)\n#     lab, n = ndimage.label(bin3d)\n#     sizes = [(lab==i).sum() for i in range(1, n+1)]\n#     return dict(posfrac=bin3d.mean(), n_comp=n, largest=max(sizes) if sizes else 0)\n\n# # Example on first few from val_loader_small\n# with torch.no_grad():\n#     for batch in val_loader_small:\n#         vol = batch['volume'].to(device)\n#         seg_prob = torch.sigmoid(model(vol)[0]).float().cpu().numpy()\n#         for i in range(seg_prob.shape[0]):\n#             s = quick_3d_summary(seg_prob[i,0], thr=0.5)\n#             print(batch['series_id'][i], s)\n#         break","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}