{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":99552,"databundleVersionId":13190393,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/02-aneurysmnet-cnn-intracranial-nb153?scriptVersionId=254293128\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install monai\n\n# ====================================================\n# RSNA INTRACRANIAL ANEURYSM DETECTION - TRAINING PIPELINE\n# ====================================================\n\nimport os\nimport gc\nimport warnings\nimport json\nimport time\nimport numpy as np\nimport pandas as pd\nfrom typing import Tuple, Dict, List\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport albumentations as A\n\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.amp import autocast, GradScaler\n\nimport pydicom\nimport pydicom.errors\nfrom scipy import ndimage\nimport nibabel as nib\nfrom monai.transforms import (\n    Compose, RandRotate90d, RandFlipd, RandAffined,\n    RandGaussianNoised, RandAdjustContrastd, ToTensord\n)\nfrom monai.networks.nets import BasicUNet\nfrom monai.losses import DiceCELoss, FocalLoss\nfrom tqdm import tqdm\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================\n# CONFIGURATION\n# ====================================================\n\nclass Config:\n    # Paths\n    TRAIN_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv'\n    LOCALIZER_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train_localizers.csv'\n    SERIES_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/'\n    SEGMENTATION_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/segmentations/'\n    \n    # Model parameters\n    TARGET_SIZE = (32, 64, 64)  # Increased resolution\n    EPOCHS = 10\n    BATCH_SIZE = 8  # Reduced due to larger input size\n    LEARNING_RATE = 1e-3\n    WEIGHT_DECAY = 1e-4\n    N_FOLDS = 3\n    \n    # Training parameters\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MIXED_PRECISION = True\n    GRADIENT_ACCUMULATION = 4\n    \n    # Competition constants\n    ID_COL = 'SeriesInstanceUID'\n    LABEL_COLS = [\n        'Left Infraclinoid Internal Carotid Artery', 'Right Infraclinoid Internal Carotid Artery',\n        'Left Supraclinoid Internal Carotid Artery', 'Right Supraclinoid Internal Carotid Artery',\n        'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery', 'Anterior Communicating Artery',\n        'Left Anterior Cerebral Artery', 'Right Anterior Cerebral Artery',\n        'Left Posterior Communicating Artery', 'Right Posterior Communicating Artery',\n        'Basilar Tip', 'Other Posterior Circulation', 'Aneurysm Present',\n    ]\n    \n    # Class weights for imbalanced data\n    ANEURYSM_PRESENT_WEIGHT = 13.0  # Match evaluation metric weighting\n\n# ====================================================\n# ENHANCED DATA PREPROCESSING\n# ====================================================\n\nclass AdvancedDICOMProcessor:\n    def __init__(self, target_size: Tuple[int, int, int] = Config.TARGET_SIZE):\n        self.target_size = target_size\n        self.stats = {\n            'total_loaded': 0,\n            'successful_loads': 0,\n            'shape_errors': 0,\n            'empty_volumes': 0,\n            'preprocessing_errors': 0\n        }\n        \n    def load_dicom_series(self, series_path: str) -> Tuple[np.ndarray, Dict]:\n        \"\"\"Load DICOM series with robust error handling\"\"\"\n        self.stats['total_loaded'] += 1\n        try:\n            dicom_files = [os.path.join(series_path, f) for f in os.listdir(series_path) if f.endswith('.dcm')]\n            if not dicom_files:\n                series_id = os.path.basename(series_path)\n                print(f\"No DICOM files found in {series_path}, using mean volume fallback\")\n                self.stats['empty_volumes'] += 1\n                # Log series with no DICOM files\n                log_message = f\"{series_id}: No DICOM files found\\n\"\n                try:\n                    with open('corrupted_series.txt', 'a') as f:\n                        f.write(log_message)\n                        f.flush()  # Ensure immediate write\n                    print(f\"üìù Logged series with no DICOMs: {series_id}\")\n                except Exception as e:\n                    print(f\"‚ö†Ô∏è  Failed to log series: {e}\")\n                return self._get_fallback_volume(), {}\n                \n            dicoms = []\n            corrupted_count = 0\n            \n            for f in dicom_files:\n                try:\n                    ds = pydicom.dcmread(f, force=True)\n                    if hasattr(ds, 'pixel_array') and ds.pixel_array.size > 0:\n                        dicoms.append(ds)\n                    else:\n                        corrupted_count += 1\n                        if corrupted_count <= 3:  # Only log first few\n                            print(f\"  Skipping empty pixel array: {os.path.basename(f)}\")\n                except pydicom.errors.InvalidDicomError:\n                    corrupted_count += 1\n                    if corrupted_count <= 3:\n                        print(f\"  Invalid DICOM file: {os.path.basename(f)}\")\n                    continue\n                except Exception as e:\n                    corrupted_count += 1\n                    if corrupted_count <= 3:\n                        print(f\"  Error reading {os.path.basename(f)}: {e}\")\n                    continue\n                    \n            if not dicoms:\n                series_id = os.path.basename(series_path)\n                print(f\"‚ùå No valid DICOMs in series ({corrupted_count} corrupted files), using mean volume fallback\")\n                self.stats['empty_volumes'] += 1\n                # Log corrupted series for inspection\n                log_message = f\"{series_id}: No valid DICOMs ({corrupted_count} corrupted files)\\n\"\n                try:\n                    with open('corrupted_series.txt', 'a') as f:\n                        f.write(log_message)\n                        f.flush()  # Ensure immediate write\n                    print(f\"üìù Logged corrupted series: {series_id}\")\n                except Exception as e:\n                    print(f\"‚ö†Ô∏è  Failed to log corrupted series: {e}\")\n                return self._get_fallback_volume(), {}\n                \n            if corrupted_count > 0:\n                print(f\"‚ö†Ô∏è  Series loaded with {corrupted_count} corrupted files (kept {len(dicoms)} valid)\")\n                \n            # Extract metadata from first DICOM\n            first_ds = dicoms[0]\n            metadata = {\n                'modality': getattr(first_ds, 'Modality', 'UNKNOWN'),\n                'spacing': getattr(first_ds, 'PixelSpacing', [1.0, 1.0]),\n                'slice_thickness': getattr(first_ds, 'SliceThickness', 1.0),\n                'rescale_slope': getattr(first_ds, 'RescaleSlope', 1.0),\n                'rescale_intercept': getattr(first_ds, 'RescaleIntercept', 0.0),\n            }\n            \n            # Sort by instance number \n            dicoms.sort(key=lambda x: int(getattr(x, 'InstanceNumber', 0)))\n            \n            # Process pixel arrays (minimal logging)\n            pixel_arrays = []\n            shapes = []\n            \n            for d in dicoms:\n                if hasattr(d, 'pixel_array'):\n                    try:\n                        arr = d.pixel_array\n                        if arr.ndim == 2 and arr.size > 0:  # Valid 2D slice\n                            pixel_arrays.append(arr)\n                            shapes.append(arr.shape)\n                    except:\n                        continue  # Skip corrupted slices\n            \n            if len(pixel_arrays) == 0:\n                print(f\"‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\")\n                self.stats['shape_errors'] += 1\n                return self._get_fallback_volume(), metadata\n            \n            # Handle shape consistency\n            unique_shapes = list(set(shapes))\n            if len(unique_shapes) == 1:\n                # All same shape - direct stacking\n                volume = np.stack(pixel_arrays, axis=0).astype(np.float32)\n            else:\n                # Multiple shapes - resize to most common\n                most_common_shape = Counter(shapes).most_common(1)[0][0]\n                resized_arrays = []\n                for arr in pixel_arrays:\n                    if arr.shape == most_common_shape:\n                        resized_arrays.append(arr.astype(np.float32))\n                    else:\n                        zoom_factors = (most_common_shape[0] / arr.shape[0], \n                                      most_common_shape[1] / arr.shape[1])\n                        resized_arr = ndimage.zoom(arr, zoom_factors, order=1, prefilter=False)\n                        resized_arrays.append(resized_arr.astype(np.float32))\n                volume = np.stack(resized_arrays, axis=0).astype(np.float32)\n            \n            # Log successful loads (first few only)\n            if self.stats['total_loaded'] <= 10:\n                print(f\"‚úÖ Loaded: {volume.shape} from {len(pixel_arrays)} slices\")\n            \n            # Apply rescale if available\n            if metadata['rescale_slope'] != 1.0 or metadata['rescale_intercept'] != 0.0:\n                volume = volume * metadata['rescale_slope'] + metadata['rescale_intercept']\n\n            self.stats['successful_loads'] += 1\n            return volume, metadata\n            \n        except Exception as e:\n            print(f\"Error loading {series_path}: {e}, using mean volume fallback\")\n            self.stats['shape_errors'] += 1\n            return self._get_fallback_volume(), {}\n\n    def _get_fallback_volume(self):\n        \"\"\"Get mean volume fallback or zeros if no dataset reference\"\"\"\n        if hasattr(self, 'dataset') and hasattr(self.dataset, 'mean_volume'):\n            return self.dataset.mean_volume.copy()\n        return np.zeros(self.target_size, dtype=np.float32)\n\n    def print_stats(self):\n        \"\"\"Print loading statistics\"\"\"\n        total = self.stats['total_loaded']\n        successful = self.stats['successful_loads']\n        empty = self.stats['empty_volumes']\n        shape_errors = self.stats['shape_errors']\n        \n        if total > 0:\n            success_rate = (successful / total) * 100\n            print(f\"\\nüìä === DICOM Loading Stats ===\")\n            print(f\"‚úÖ Successful loads: {successful}/{total} ({success_rate:.1f}%)\")\n            print(f\"‚ùå Corrupted/empty: {empty} ({empty/total*100:.1f}%)\")\n            print(f\"‚ö†Ô∏è  Shape errors: {shape_errors} ({shape_errors/total*100:.1f}%)\")\n            \n            if success_rate < 70:\n                print(f\"üö® SUCCESS RATE TOO LOW ({success_rate:.1f}%)!\")\n                print(f\"   Most volumes are corrupted - check dataset quality!\")\n            elif success_rate < 85:\n                print(f\"‚ö†Ô∏è  Moderate success rate ({success_rate:.1f}%) - some data quality issues\")\n            else:\n                print(f\"‚úÖ Good success rate ({success_rate:.1f}%)\")\n            print(f\"===============================\")\n\n    def preprocess_volume(self, volume: np.ndarray, metadata: Dict) -> np.ndarray:\n        \"\"\"Enhanced preprocessing with modality-specific handling\"\"\"\n        if volume.ndim != 3 or volume.size == 0:\n            print(f\"Warning: Received a non-3D volume. Returning empty target volume.\")\n            return np.zeros(self.target_size, dtype=np.float32)\n        \n        # Default windowing\n        p1, p99 = np.percentile(volume, [5, 95])\n        volume = np.clip(volume, p1, p99)\n        \n        # Normalization\n        vol_min, vol_max = volume.min(), volume.max()\n        if vol_max > vol_min:\n            volume = (volume - vol_min) / (vol_max - vol_min)\n        \n        # Resize to target size\n        if volume.shape != self.target_size:\n            zoom_factors = [self.target_size[i] / volume.shape[i] for i in range(3)]\n            volume = ndimage.zoom(volume, zoom_factors, order=1, prefilter=False)\n        \n        return volume.astype(np.float32)\n\n    def load_localization_mask(self, series_id: str, localizer_df: pd.DataFrame) -> np.ndarray:\n        return np.zeros(self.target_size, dtype=np.float32)\n\n\n# ====================================================\n# ENHANCED DATASET\n# ====================================================\n\nclass EnhancedAneurysmDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, localizer_df: pd.DataFrame, \n                 series_dir: str, processor: AdvancedDICOMProcessor, \n                 mode: str = 'train', fold: int = None):\n        self.df = df\n        self.localizer_df = localizer_df\n        self.series_dir = series_dir\n        self.processor = processor\n        self.mode = mode\n        self.fold = fold\n        \n        # Data augmentation for training\n        if mode == 'train':\n            self.transform = Compose([\n                # Lightweight augmentations to improve generalization\n                RandRotate90d(keys=['volume'], prob=0.2, spatial_axes=(0, 1)),  # 90-degree rotations only\n                RandFlipd(keys=['volume'], prob=0.5, spatial_axis=0),           # Axial flip\n                RandFlipd(keys=['volume'], prob=0.5, spatial_axis=1),           # Sagittal flip\n                RandGaussianNoised(keys=['volume'], prob=0.2, std=0.03),        # Very low noise level\n                ToTensord(keys=['volume'])\n            ])\n        else:\n            self.transform = Compose([ToTensord(keys=['volume'])])\n\n        # Set dataset reference in processor for mean volume access\n        self.processor.dataset = self\n        \n        # Precompute mean volume for fallback on corrupted DICOM files\n        print(\"üîÑ Computing mean volume fallback from valid series...\")\n        valid_volumes = []\n        sample_size = min(10, len(df))  # Sample 10 series for speed\n        \n        for i, series_id in enumerate(df[Config.ID_COL][:sample_size]):\n            series_path = os.path.join(series_dir, series_id)\n            try:\n                volume, _ = processor.load_dicom_series(series_path)\n                if not np.all(volume == 0) and volume.size > 0:\n                    volume = processor.preprocess_volume(volume, {})\n                    valid_volumes.append(volume)\n                    print(f\"  ‚úÖ Valid volume {i+1}/{sample_size}: {volume.shape}\")\n            except Exception as e:\n                print(f\"  ‚ùå Skipped corrupted volume {i+1}/{sample_size}: {e}\")\n                continue\n        \n        if valid_volumes:\n            self.mean_volume = np.mean(valid_volumes, axis=0).astype(np.float32)\n            print(f\"üìä Mean volume computed from {len(valid_volumes)} valid series: {self.mean_volume.shape}\")\n        else:\n            self.mean_volume = np.zeros(processor.target_size, dtype=np.float32)\n            print(\"‚ö†Ô∏è  No valid volumes found, using zero fallback\")\n        \n        print(f\"üéØ Mean volume fallback ready (shape: {self.mean_volume.shape})\")\n\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        load_start = time.time()\n        row = self.df.iloc[idx]\n        series_id = row[Config.ID_COL]\n        series_path = os.path.join(self.series_dir, series_id)\n        \n        # Load and process volume\n        volume, metadata = self.processor.load_dicom_series(series_path)\n        dicom_time = time.time() - load_start\n        preprocess_start = time.time()\n        volume = self.processor.preprocess_volume(volume, metadata)\n        \n        # Create localization mask (for auxiliary loss)\n        loc_mask = self.processor.load_localization_mask(series_id, self.localizer_df)\n        \n        # Get labels\n        labels = row[Config.LABEL_COLS].values.astype(np.float32)\n        \n        # Apply transforms\n        data_dict = {'volume': volume}\n        if self.transform:\n            data_dict = self.transform(data_dict)\n        \n        volume_tensor = data_dict['volume'].unsqueeze(0)  # Add channel dimension\n        loc_mask_tensor = torch.from_numpy(loc_mask).unsqueeze(0)\n        labels_tensor = torch.from_numpy(labels)\n        \n        # Add metadata features\n        modality_encoding = self._encode_modality(metadata.get('modality', 'UNKNOWN'))\n        metadata_tensor = torch.tensor(modality_encoding, dtype=torch.float32)\n        \n        preprocess_time = time.time() - preprocess_start\n        # Print timing for first few samples to debug\n        if idx < 5:\n            print(f\"Sample {idx}: DICOM load: {dicom_time:.2f}s, Preprocess: {preprocess_time:.2f}s\")\n        \n        return {\n            'volume': volume_tensor,\n            'localization_mask': loc_mask_tensor,\n            'labels': labels_tensor,\n            'metadata': metadata_tensor,\n            'series_id': series_id\n        }\n    \n    def _encode_modality(self, modality: str) -> List[float]:\n        \"\"\"One-hot encode modality\"\"\"\n        modalities = ['CTA', 'MRA', 'MRI', 'MR', 'UNKNOWN']\n        encoding = [0.0] * len(modalities)\n        if modality in modalities:\n            encoding[modalities.index(modality)] = 1.0\n        else:\n            encoding[-1] = 1.0  # UNKNOWN\n        return encoding\n\n\n# ====================================================\n# ADVANCED MODEL ARCHITECTURE\n# ====================================================\n\n#class MultiModalAneurysmNet(nn.Module):\nclass SimplifiedAneurysmNet(nn.Module):\n    def __init__(self, num_classes: int = len(Config.LABEL_COLS), \n                 spatial_dims: int = 3, in_channels: int = 1, \n        #          features: Tuple = (32, 64, 128, 256, 512, 1024)):\n        # super(MultiModalAneurysmNet, self).__init__()\n                 features: Tuple = (16, 32, 64, 128, 256, 51)):\n        super(SimplifiedAneurysmNet, self).__init__()\n        \n        # Main 3D U-Net backbone\n        self.backbone = BasicUNet(\n            spatial_dims=spatial_dims,\n            in_channels=in_channels,\n            out_channels=features[0],\n            features=features,\n            dropout=0.1 #Reduced dropout \n        )\n        \n        # Global average pooling\n        self.global_pool = nn.AdaptiveAvgPool3d(1)\n        \n        # # Metadata processing\n        # self.metadata_mlp = nn.Sequential(\n        #     nn.Linear(5, 32),  # 5 modality categories\n        #     nn.ReLU(),\n        #     nn.Dropout(0.3),\n        #     nn.Linear(32, 64),\n        #     nn.ReLU()\n        # )\n        \n        # Classification head\n        #feature_size = features[0] + 64  # backbone features + metadata features\n        self.classifier = nn.Sequential(\n            # nn.Linear(feature_size, 512),\n            # nn.ReLU(),\n            # nn.Dropout(0.5),\n            # nn.Linear(512, 256),\n            nn.Linear(features[0], 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, volume, metadata=None):\n        # Extract features from 3D volume\n        features = self.backbone(volume)\n        # Global features for classification\n        global_features = self.global_pool(features).flatten(1)\n        classification_logits = self.classifier(global_features)\n        return classification_logits, None\n\n# ====================================================\n# WEIGHTED LOSS FUNCTION\n# ====================================================\n\nclass WeightedMultiLabelLoss(nn.Module):\n    def __init__(self, pos_weights=None, aneurysm_weight=13.0):\n        super().__init__()\n        self.pos_weights = pos_weights\n        self.aneurysm_weight = aneurysm_weight\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n        \n    def forward(self, logits, targets):\n        bce_loss = self.bce(logits, targets)\n        \n        # Apply position weights if provided\n        if self.pos_weights is not None:\n            bce_loss = bce_loss * self.pos_weights.to(logits.device)\n        \n        # Weight the \"Aneurysm Present\" class higher (last column)\n        weights = torch.ones_like(bce_loss)\n        weights[:, -1] = self.aneurysm_weight\n        \n        weighted_loss = bce_loss * weights\n        return weighted_loss.mean()\n\n\n# ====================================================\n# TRAINING FUNCTIONS\n# ====================================================\n\ndef compute_weighted_auc(y_true, y_pred):\n    \"\"\"Compute weighted AUC matching competition metric\"\"\"\n    aucs = []\n    weights = []\n    \n    for i in range(len(Config.LABEL_COLS)):\n            auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n            aucs.append(auc)\n            # Weight \"Aneurysm Present\" (last column) higher\n            weights.append(13.0 if i == len(Config.LABEL_COLS) - 1 else 1.0)\n        except ValueError:\n            aucs.append(0.5)  # Default for no positive cases\n            weights.append(13.0 if i == len(Config.LABEL_COLS) - 1 else 1.0)\n    \n    weighted_auc = sum(a * w for a, w in zip(aucs, weights)) / sum(weights)\n    return weighted_auc, aucs\n\ndef train_epoch(model, train_loader, optimizer, criterion, scaler, device):\n    model.train()\n    total_loss = 0\n    num_batches = 0\n    skipped_batches = 0\n    \n    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch\")):\n        start_time = time.time()\n        volume = batch['volume'].to(device)\n        metadata = batch['metadata'].to(device)\n        labels = batch['labels'].to(device)\n        loc_mask = batch['localization_mask'].to(device)\n\n        # CRITICAL FIX: Skip batches with zero-filled volumes\n        if torch.all(volume == 0) or torch.var(volume) < 1e-6:\n            skipped_batches += 1\n            if batch_idx < 5:  # Log first few skips\n                print(f\"‚ö†Ô∏è  Skipping batch {batch_idx}: zero-filled or low-variance volume\")\n            continue\n        \n        with autocast(device_type=device.type, enabled=Config.MIXED_PRECISION):\n            class_logits, _ = model(volume, metadata)\n            total_loss_batch = criterion(class_logits, labels)\n\n            # DEBUG: Check for extreme loss values\n            if total_loss_batch.item() > 1e7:\n                print(f\"üö® Warning: Extreme loss in training batch {batch_idx}: {total_loss_batch.item():.2e}\")\n                print(f\"   Labels: {labels[0].cpu().numpy()}\")  # Print first sample's labels\n        \n        # Gradient accumulation\n        scaled_loss = total_loss_batch / Config.GRADIENT_ACCUMULATION\n        scaler.scale(scaled_loss).backward()\n        \n        if (batch_idx + 1) % Config.GRADIENT_ACCUMULATION == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n        \n        total_loss += total_loss_batch.item()\n        num_batches += 1\n\n        # Print timing for first few batches to identify bottlenecks\n        if batch_idx < 5:\n            batch_time = time.time() - start_time\n            print(f\"Batch {batch_idx}: {batch_time:.2f}s\")\n    \n    if skipped_batches > 0:\n        print(f\"‚ö†Ô∏è  Skipped {skipped_batches} batches with corrupted/zero volumes\")\n    \n    return total_loss / max(num_batches, 1) if num_batches > 0 else float('inf')\n\ndef validate_epoch(model, val_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    num_batches = 0\n    skipped_batches = 0\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Validating\")):\n            volume = batch['volume'].to(device)\n            metadata = batch['metadata'].to(device)\n            labels = batch['labels'].to(device)\n            \n            # CRITICAL FIX: Skip batches with zero-filled volumes\n            if torch.all(volume == 0) or torch.var(volume) < 1e-6:\n                skipped_batches += 1\n                if batch_idx < 3:  # Log first few skips\n                    print(f\"‚ö†Ô∏è  Skipping validation batch {batch_idx}: zero-filled or low-variance volume\")\n                continue\n            \n            with autocast(device_type=device.type, enabled=Config.MIXED_PRECISION):\n                class_logits, _ = model(volume, metadata)\n                loss = criterion(class_logits, labels)\n\n                # DEBUG: Check for extreme loss values\n                if loss.item() > 1e7:\n                    print(f\"üö® Warning: Extreme loss in validation batch {batch_idx}: {loss.item():.2e}\")\n                    print(f\"   Labels: {labels[0].cpu().numpy()}\")  # Print first sample's labels\n            \n            total_loss += loss.item()\n            num_batches += 1\n            \n            # Collect predictions for AUC calculation\n            probs = torch.sigmoid(class_logits).cpu().numpy()\n            all_preds.append(probs)\n            all_labels.append(labels.cpu().numpy())\n\n    if skipped_batches > 0:\n        print(f\"‚ö†Ô∏è  Skipped {skipped_batches} validation batches with corrupted/zero volumes\")\n    \n    if len(all_preds) == 0:\n        print(\"üö® WARNING: No valid validation batches - all were corrupted!\")\n        return float('inf'), 0.5, [0.5] * len(Config.LABEL_COLS)\n    \n    all_preds = np.vstack(all_preds)\n    all_labels = np.vstack(all_labels)\n    \n    weighted_auc, individual_aucs = compute_weighted_auc(all_labels, all_preds)\n    \n    return total_loss / max(num_batches, 1), weighted_auc, individual_aucs\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================\n# MAIN TRAINING EXECUTION\n# ====================================================\n\ndef main():\n    print(f\"Using device: {Config.DEVICE}\")\n    print(f\"Mixed precision: {Config.MIXED_PRECISION}\")\n    \n    # Load data\n    train_df = pd.read_csv(Config.TRAIN_CSV_PATH)\n    localizer_df = pd.read_csv(Config.LOCALIZER_CSV_PATH)\n\n    print(\"---!!! RUNNING IN DEBUG MODE ON A SMALL SUBSET !!!---\")\n    #print(f\"Training samples: {len(train_df)}\")\n    train_df = train_df.head(100)  # Limit to 100 samples for speed testing\n    print(f\"Training samples: {len(train_df)} (limited for speed testing)\")\n    print(f\"Positive aneurysm cases: {train_df['Aneurysm Present'].sum()}\")\n\n    # Calculate class weights for imbalanced data\n    pos_counts = train_df[Config.LABEL_COLS].sum()\n    neg_counts = len(train_df) - pos_counts\n    pos_weights = neg_counts / (pos_counts + 1e-8)  # Add small epsilon\n    pos_weights = np.minimum(pos_weights, 100.0)  # Cap weights at 100\n    pos_weights = torch.tensor(pos_weights, dtype=torch.float32)  # Convert to tensor\n    print(\"Class weights (capped at 100):\", pos_weights)\n\n    # DATASET INTEGRITY CHECK\n    print(\"\\nüîç Checking dataset integrity...\")\n    valid_series = 0\n    invalid_series = []\n    \n    for series_id in train_df[Config.ID_COL]:\n        series_path = os.path.join(Config.SERIES_DIR, series_id)\n        if os.path.exists(series_path):\n            dicom_files = [f for f in os.listdir(series_path) if f.endswith('.dcm')]\n            if dicom_files:\n                valid_series += 1\n            else:\n                invalid_series.append(f\"No DICOMs: {series_id}\")\n        else:\n            invalid_series.append(f\"Missing path: {series_id}\")\n    \n    success_rate = valid_series / len(train_df)\n    print(f\"üìä Dataset check: {valid_series}/{len(train_df)} series valid ({success_rate:.1%})\")\n    \n    if success_rate < 0.7:\n        print(f\"üö® WARNING: Only {success_rate:.1%} of series are accessible!\")\n        print(\"First few issues:\")\n        for issue in invalid_series[:5]:\n            print(f\"  - {issue}\")\n        print(f\"‚ö†Ô∏è  Training will proceed, but expect many corrupted DICOM errors\")\n    else:\n        print(f\"‚úÖ Good dataset integrity ({success_rate:.1%} valid)\")\n        \n    print()\n\n    # Initialize corrupted series log\n    with open('corrupted_series.txt', 'w') as f:\n        f.write(\"# Corrupted series log - check this file to identify problematic DICOM series\\n\")\n    print(\"üìù Initialized 'corrupted_series.txt' for logging corrupted series\")\n    \n    # Create stratified group k-fold split\n    # Use patient-level grouping to prevent data leakage\n    train_df['patient_group'] = train_df['PatientID'] if 'PatientID' in train_df.columns else range(len(train_df))\n    \n    skf = StratifiedGroupKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=42)\n    train_df['fold'] = -1\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(\n        train_df, train_df['Aneurysm Present'], groups=train_df['patient_group']\n    )):\n        train_df.loc[val_idx, 'fold'] = fold\n    \n    # Initialize processor\n    processor = AdvancedDICOMProcessor()\n    \n    # Train models for each fold\n    fold_scores = []\n    \n    for fold in range(Config.N_FOLDS):\n        print(f\"\\n{'='*50}\")\n        print(f\"FOLD {fold + 1}/{Config.N_FOLDS}\")\n        print(f\"{'='*50}\")\n        \n        # Split data\n        train_fold_df = train_df[train_df['fold'] != fold].reset_index(drop=True)\n        val_fold_df = train_df[train_df['fold'] == fold].reset_index(drop=True)\n        \n        print(f\"Train: {len(train_fold_df)}, Validation: {len(val_fold_df)}\")\n        \n        # Create datasets\n        train_dataset = EnhancedAneurysmDataset(\n            train_fold_df, localizer_df, Config.SERIES_DIR, processor, mode='train', fold=fold\n        )\n        val_dataset = EnhancedAneurysmDataset(\n            val_fold_df, localizer_df, Config.SERIES_DIR, processor, mode='val', fold=fold\n        )\n        \n        # Create data loaders\n        train_loader = DataLoader(\n            train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, \n            num_workers=4, pin_memory=True, drop_last=True\n        )\n        val_loader = DataLoader(\n            val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, \n            num_workers=4, pin_memory=True\n        )\n        \n        # Initialize model\n        model = SimplifiedAneurysmNet().to(Config.DEVICE)\n        criterion = WeightedMultiLabelLoss(pos_weights=pos_weights)\n        \n        # Optimizer with different learning rates for different parts\n        # optimizer = optim.AdamW([\n        #     {'params': model.backbone.parameters(), 'lr': Config.LEARNING_RATE},\n        #     {'params': model.classifier.parameters(), 'lr': Config.LEARNING_RATE * 2},\n        #     {'params': model.metadata_mlp.parameters(), 'lr': Config.LEARNING_RATE * 2}\n        # ], weight_decay=Config.WEIGHT_DECAY)\n        optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)\n        \n        # Learning rate scheduler\n        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            optimizer, T_0=10, T_mult=2, eta_min=1e-6\n        )\n        \n        scaler = GradScaler(enabled=Config.MIXED_PRECISION)\n        \n        # Training loop\n        best_auc = 0\n        patience = 10\n        patience_counter = 0\n        \n        for epoch in range(Config.EPOCHS):\n            # Train\n            train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, Config.DEVICE)\n            \n            # Validate\n            val_loss, val_auc, individual_aucs = validate_epoch(model, val_loader, criterion, Config.DEVICE)\n            \n            # Step scheduler\n            scheduler.step()\n            \n            print(f\"Epoch {epoch+1:3d} | \"\n                  f\"Train Loss: {train_loss:.4f} | \"\n                  f\"Val Loss: {val_loss:.4f} | \"\n                  f\"Val AUC: {val_auc:.4f}\")\n\n            processor.print_stats() # Print DICOM loading stats after each epoch\n\n            # Check corrupted series log\n            try:\n                with open('corrupted_series.txt', 'r') as f:\n                    lines = f.readlines()\n                    corrupted_count = len([l for l in lines if not l.startswith('#')])\n                    if corrupted_count > 0:\n                        print(f\"üìÑ Corrupted series logged: {corrupted_count} entries in 'corrupted_series.txt'\")\n                    else:\n                        print(f\"‚úÖ No corrupted series logged this epoch\")\n            except FileNotFoundError:\n                print(f\"üìÑ No corrupted series log file found\")\n\n            # SANITY CHECK: Stop if data loading is fundamentally broken\n            if processor.stats['total_loaded'] > 20:  # Only check after some attempts\n                success_rate = processor.stats['successful_loads'] / processor.stats['total_loaded']\n                if success_rate < 0.5:  # Less than 50% success rate\n                    print(f\"\\nüö® STOPPING TRAINING: Data loading success rate is {success_rate:.1%}\")\n                    print(\"Fix the DICOM loading issues before continuing training!\")\n                    print(\"Most volumes are returning empty - this is a waste of time!\")\n                    break\n            \n            # Save best model\n            if val_auc > best_auc:\n                best_auc = val_auc\n                patience_counter = 0\n                torch.save({\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                    'val_auc': val_auc,\n                    'epoch': epoch,\n                    'fold': fold,\n                    'individual_aucs': individual_aucs\n                }, f'best_model_fold_{fold}.pth')\n            else:\n                patience_counter += 1\n                \n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n                \n            # Memory cleanup\n            if epoch % 5 == 0:\n                gc.collect()\n                torch.cuda.empty_cache()\n        \n        fold_scores.append(best_auc)\n        print(f\"Fold {fold + 1} best AUC: {best_auc:.4f}\")\n    \n    # Final results\n    mean_cv_score = np.mean(fold_scores)\n    std_cv_score = np.std(fold_scores)\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"CROSS-VALIDATION RESULTS\")\n    print(f\"{'='*50}\")\n    print(f\"Mean CV AUC: {mean_cv_score:.4f} ¬± {std_cv_score:.4f}\")\n    print(f\"Individual fold scores: {fold_scores}\")\n    \n    # Save training summary\n    results = {\n        'cv_scores': fold_scores,\n        'mean_cv_score': mean_cv_score,\n        'std_cv_score': std_cv_score,\n        'config': vars(Config())\n    }\n    \n    with open('training_results.json', 'w') as f:\n        json.dump(results, f, indent=2, default=str)\n    \n    print(\"Training complete! Models saved as 'best_model_fold_X.pth'\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}