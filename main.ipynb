{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":99552,"databundleVersionId":13441085},{"sourceType":"datasetVersion","sourceId":12837944,"datasetId":8119423,"databundleVersionId":13471588}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/stage1-aneurysmnet-intracranial-training-nb153?scriptVersionId=258225840\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# ====================================================\n# RSNA INTRACRANIAL ANEURYSM - STAGE 1 TRAINING (v2)\n# Uses Stage-0 prebuilt v2 cache (volumes, masks, pseudo_masks, brainmasks, manifest)\n# Two-phase training with per-sample segmentation weights and rich progress logs:\n#   Phase 1: real masks weighted (real_seg_weight), synthetic seg weight = 0.0\n#   Phase 2: real same, synthetic seg weight = small (default 0.075)\n# Saves: stage1_phase1_best.pth, stage1_phase2_best.pth, stage1_segmentation_best.pth\n# ====================================================\n\nimport os\nimport math\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T05:16:58.684682Z","iopub.execute_input":"2025-08-26T05:16:58.685387Z","iopub.status.idle":"2025-08-26T05:17:05.485367Z","shell.execute_reply.started":"2025-08-26T05:16:58.685329Z","shell.execute_reply":"2025-08-26T05:17:05.484663Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ====================================================\n# Config\n# ====================================================\nclass Config:\n    # --- Paths ---\n    PREBUILT_ROOT = \"/kaggle/input/rsna2025-v2-intracranial-aneurysm-detection-nb153/stage1_AneurysmNet_prebuilt_v2\"\n    MANIFEST_PATH = os.path.join(PREBUILT_ROOT, \"meta/manifest.csv\")\n    VOLUMES_DIR   = os.path.join(PREBUILT_ROOT, \"volumes\")\n    MASKS_DIR     = os.path.join(PREBUILT_ROOT, \"masks\")          # real\n    PSEUDO_DIR    = os.path.join(PREBUILT_ROOT, \"pseudo_masks\")   # synthetic\n    BRAINMASKS_DIR= os.path.join(PREBUILT_ROOT, \"brainmasks\")\n\n    # --- Data ---\n    TARGET_SIZE = (48, 112, 112)  # (D,H,W)\n    USE_BRAINMASKS = True\n    BRAINMASK_KEY = 'm'\n    BRAINMASK_MIN_FRAC = 0.02  # if below, skip masking\n\n    # --- Training ---\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MIXED_PRECISION = True\n    STAGE1_BATCH_SIZE = 12  # bump to 12/16 if GPU mem allows\n    NUM_WORKERS = 4        # bump to 4/8 to reduce CPU bottlenecks\n    PREFETCH_FACTOR = 4\n    PERSISTENT_WORKERS = True\n    STAGE1_LR = 2e-4\n    WEIGHT_DECAY = 1e-4\n    EPOCHS_PHASE1 = 15\n    EPOCHS_PHASE2 = 15\n    EARLY_STOP_PATIENCE = 5\n    GRAD_ACCUM_STEPS = 8\n    # Validation throughput\n    VAL_BATCH_MULT = 1  # keep validation batch moderate to avoid I/O stalls\n    VAL_NUM_WORKERS = 4  # allow more workers for validation to feed GPUs\n\n    # --- Segmentation weights ---\n    REAL_SEG_DEFAULT_W = 0.7      # used if manifest lacks real_seg_weight\n    PHASE1_SYNTH_SEG_W = 0.0\n    PHASE2_SYNTH_SEG_W = 0.075\n    FOCAL_LOSS_WEIGHT = 0.2\n\n    # --- Splits ---\n    FOLDS = 1   # set >1 later if you want CV here\n    SEED = 42\n\n# ====================================================\n# Utils\n# ====================================================\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True  # speeds up fixed-size convs\n\n\ndef load_manifest_df() -> pd.DataFrame:\n    df = pd.read_csv(Config.MANIFEST_PATH)\n    # Required columns: series_id, label, vol_relpath\n    for col in [\"series_id\", \"label\", \"vol_relpath\"]:\n        if col not in df.columns:\n            raise RuntimeError(f\"Manifest missing required column: {col}\")\n    return df\n\n\ndef gpu_mem_str():\n    if not torch.cuda.is_available():\n        return \"cpu\"\n    try:\n        a = torch.cuda.memory_allocated() / (1024**3)\n        r = torch.cuda.memory_reserved() / (1024**3)\n        return f\"{a:.2f}G/{r:.2f}G\"\n    except Exception:\n        return \"gpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T05:17:14.983951Z","iopub.execute_input":"2025-08-26T05:17:14.98451Z","iopub.status.idle":"2025-08-26T05:17:15.090004Z","shell.execute_reply.started":"2025-08-26T05:17:14.984484Z","shell.execute_reply":"2025-08-26T05:17:15.089219Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\nclass PrebuiltDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, phase_synth_w: float):\n        self.df = df.reset_index(drop=True)\n        self.phase_synth_w = float(phase_synth_w)\n\n    def __len__(self):\n        return len(self.df)\n\n    def _load_volume(self, sid: str) -> np.ndarray:\n        path = os.path.join(Config.VOLUMES_DIR, f\"{sid}.npy\")\n        v = np.load(path).astype(np.float32)  # (D,H,W), 0..1\n        v = np.nan_to_num(v, nan=0.0, posinf=1.0, neginf=0.0)\n        return v\n\n    def _load_brainmask(self, sid: str, frac: Optional[float], shp: Tuple[int,int,int]):\n        if not Config.USE_BRAINMASKS:\n            return None\n        if frac is not None and float(frac) < Config.BRAINMASK_MIN_FRAC:\n            return None\n        p = os.path.join(Config.BRAINMASKS_DIR, f\"{sid}_brainmask.npz\")\n        if not os.path.exists(p):\n            return None\n        try:\n            bm = np.load(p)[Config.BRAINMASK_KEY].astype(np.float32)\n            bm = np.nan_to_num(bm, nan=0.0, posinf=1.0, neginf=0.0)\n            if bm.shape != shp or bm.sum() <= 0:\n                return None\n            return bm\n        except Exception:\n            return None\n\n    def _load_mask(self, sid: str, mask_rel: str, is_synth: int, label: int) -> Tuple[np.ndarray, bool]:\n        # Returns (mask[D,H,W] float32 in {0,1}, is_synthetic: bool)\n        if isinstance(mask_rel, str) and len(mask_rel) > 0:\n            if mask_rel.startswith('masks/'):\n                p = os.path.join(Config.PREBUILT_ROOT, mask_rel)\n                if os.path.exists(p):\n                    m = np.load(p).astype(np.float32)\n                    m = np.nan_to_num(m, nan=0.0, posinf=1.0, neginf=0.0)\n                    return (m > 0).astype(np.float32), False\n            elif mask_rel.startswith('pseudo_masks/'):\n                p = os.path.join(Config.PREBUILT_ROOT, mask_rel)\n                if os.path.exists(p):\n                    m = np.load(p).astype(np.float32)\n                    m = np.nan_to_num(m, nan=0.0, posinf=1.0, neginf=0.0)\n                    return (m > 0).astype(np.float32), True\n        # Fallbacks\n        D,H,W = Config.TARGET_SIZE\n        if int(label) == 1:\n            return np.zeros((D,H,W), dtype=np.float32), True\n        else:\n            return np.zeros((D,H,W), dtype=np.float32), False\n\n    def __getitem__(self, idx):\n        r = self.df.iloc[idx]\n        sid = str(r['series_id'])\n        label = int(r['label'])\n        mask_rel = r.get('mask_relpath', '') if isinstance(r.get('mask_relpath', ''), str) else ''\n        is_synth_col = int(r.get('is_synthetic', 0))\n        real_seg_weight = r.get('real_seg_weight', np.nan)\n        brain_frac = r.get('brain_voxel_fraction', np.nan)\n\n        vol = self._load_volume(sid)  # (D,H,W)\n        bm = self._load_brainmask(sid, brain_frac if pd.notna(brain_frac) else None, vol.shape)\n        if bm is not None:\n            vol = vol * bm  # gate\n\n        mask, is_synth = self._load_mask(sid, mask_rel, is_synth_col, label)\n\n        # per-sample seg weight\n        if is_synth:\n            seg_w = self.phase_synth_w\n        else:\n            if pd.notna(real_seg_weight):\n                try:\n                    rsw = float(real_seg_weight)\n                except Exception:\n                    rsw = Config.REAL_SEG_DEFAULT_W\n            else:\n                rsw = Config.REAL_SEG_DEFAULT_W\n            seg_w = float(np.clip(rsw, 0.2, 1.0))\n\n        # to tensors\n        vol_t = torch.from_numpy(vol).unsqueeze(0)         # [1,D,H,W]\n        mask_t = torch.from_numpy((mask > 0).astype(np.float32)).unsqueeze(0)\n        label_t = torch.tensor([float(label)], dtype=torch.float32)\n        segw_t  = torch.tensor([float(seg_w)], dtype=torch.float32)\n\n        return {\n            'series_id': sid,\n            'volume': vol_t,\n            'mask': mask_t,\n            'label': label_t,\n            'seg_weight': segw_t,\n            'is_synthetic_mask': torch.tensor([1.0 if is_synth else 0.0], dtype=torch.float32),\n        }\n\n# ====================================================\n# Simple 3D UNet + classifier head\n# ====================================================\nclass ConvBlock3D(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv3d(in_ch, out_ch, 3, padding=1), nn.GroupNorm(num_groups=8, num_channels=out_ch), nn.ReLU(inplace=True),\n            nn.Conv3d(out_ch, out_ch, 3, padding=1), nn.GroupNorm(num_groups=8, num_channels=out_ch), nn.ReLU(inplace=True)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass UNet3D(nn.Module):\n    def __init__(self, in_ch=1, base=24):\n        super().__init__()\n        b = base\n        self.enc1 = ConvBlock3D(in_ch, b)\n        self.pool1 = nn.MaxPool3d(2)\n        self.enc2 = ConvBlock3D(b, b*2)\n        self.pool2 = nn.MaxPool3d(2)\n        self.enc3 = ConvBlock3D(b*2, b*4)\n        self.pool3 = nn.MaxPool3d((2,2,2))\n        self.bott = ConvBlock3D(b*4, b*8)\n        self.up3 = nn.ConvTranspose3d(b*8, b*4, 2, stride=2)\n        self.dec3 = ConvBlock3D(b*8, b*4)\n        self.up2 = nn.ConvTranspose3d(b*4, b*2, 2, stride=2)\n        self.dec2 = ConvBlock3D(b*4, b*2)\n        self.up1 = nn.ConvTranspose3d(b*2, b, 2, stride=2)\n        self.dec1 = ConvBlock3D(b*2, b)\n        self.seg_head = nn.Conv3d(b, 1, 1)\n        # classification head from bottleneck features\n        self.cls_pool = nn.AdaptiveAvgPool3d(1)\n        self.cls_head = nn.Linear(b*8, 1)\n\n    def forward(self, x):  # x: [B,1,D,H,W]\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool1(e1))\n        e3 = self.enc3(self.pool2(e2))\n        b  = self.bott(self.pool3(e3))\n        # decoder\n        d3 = self.up3(b)\n        d3 = torch.cat([d3, e3], dim=1)\n        d3 = self.dec3(d3)\n        d2 = self.up2(d3)\n        d2 = torch.cat([d2, e2], dim=1)\n        d2 = self.dec2(d2)\n        d1 = self.up1(d2)\n        d1 = torch.cat([d1, e1], dim=1)\n        d1 = self.dec1(d1)\n        seg = self.seg_head(d1)  # [B,1,D,H,W]\n        # classifier from bottleneck\n        cls = self.cls_head(self.cls_pool(b).flatten(1))  # [B,1]\n        return seg, cls\n\n# ====================================================\n# Losses\n# ====================================================\nclass DiceLoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n    def forward(self, logits, targets, reduction='mean'):\n        probs = torch.sigmoid(logits)\n        num = 2 * (probs * targets).sum(dim=(2,3,4)) + self.eps\n        den = (probs.pow(2) + targets.pow(2)).sum(dim=(2,3,4)) + self.eps\n        dice = 1 - (num / den)  # per-sample\n        if reduction == 'none':\n            return dice\n        return dice.mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0, eps=1e-6):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.eps = eps\n    def forward(self, logits, targets, reduction='mean'):\n        probs = torch.sigmoid(logits).clamp(self.eps, 1-self.eps)\n        ce = -(targets*torch.log(probs) + (1-targets)*torch.log(1-probs))\n        pt = torch.where(targets==1, probs, 1-probs)\n        loss = self.alpha * (1-pt).pow(self.gamma) * ce\n        loss = loss.mean(dim=(2,3,4))  # per-sample\n        if reduction == 'none':\n            return loss\n        return loss.mean()\n\nclass EnhancedCombinedLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dice_loss = DiceLoss()\n        self.focal_loss = FocalLoss(alpha=0.25, gamma=2.0)\n        self.bce_vox = nn.BCEWithLogitsLoss(reduction='none')\n        self.bce_cls = nn.BCEWithLogitsLoss()\n    def forward(self, seg_logits, cls_logits, seg_targets, cls_targets, seg_weights: torch.Tensor):\n        # clamp seg logits to avoid AMP overflow\n        seg_logits = torch.nan_to_num(seg_logits, nan=0.0, posinf=20.0, neginf=-20.0)\n        B = seg_logits.shape[0]\n        dice_ps = self.dice_loss(seg_logits, seg_targets, reduction='none')\n        focal_ps= self.focal_loss(seg_logits, seg_targets, reduction='none')\n        bce_elem= self.bce_vox(seg_logits, seg_targets)\n        bce_ps  = bce_elem.view(B, -1).mean(dim=1)\n        dice_ps = torch.nan_to_num(dice_ps, nan=0.0)\n        focal_ps= torch.nan_to_num(focal_ps, nan=0.0)\n        bce_ps  = torch.nan_to_num(bce_ps,  nan=0.0)\n        seg_ps  = 0.5*dice_ps + 0.3*bce_ps + Config.FOCAL_LOSS_WEIGHT*focal_ps\n        seg_ps  = torch.nan_to_num(seg_ps, nan=0.0)\n        seg_w   = seg_weights.view(-1)\n        if (seg_w == 0).all():\n            seg_loss = seg_ps.new_tensor(0.0)\n        else:\n            seg_loss = (seg_ps * seg_w).mean()\n        # classification loss\n        cls_logits = torch.nan_to_num(cls_logits, nan=0.0, posinf=20.0, neginf=-20.0)\n        cls_loss= self.bce_cls(cls_logits.view(-1), cls_targets.view(-1))\n        total   = seg_loss + cls_loss\n        return total, seg_loss.detach(), cls_loss.detach()\n\n# ====================================================\n# Train / Validate with progress bars\n# ====================================================\n\ndef train_epoch(model, loader, optimizer, criterion, scaler, epoch=None, phase_name=\"P1\"):\n    model.train()\n    t_loss = t_seg = t_cls = 0.0\n    n = 0\n    pbar = tqdm(loader, desc=f\"Train {phase_name}{'' if epoch is None else f' [ep {epoch}]'}\", leave=False, mininterval=0.1)\n    for batch in pbar:\n        vol   = batch['volume'].to(Config.DEVICE, non_blocking=True)\n        try:\n            vol = vol.to(memory_format=torch.channels_last_3d)\n        except Exception:\n            pass\n        mask  = batch['mask'].to(Config.DEVICE, non_blocking=True)\n        label = batch['label'].to(Config.DEVICE, non_blocking=True)\n        segw  = batch['seg_weight'].to(Config.DEVICE, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        with torch.amp.autocast('cuda', enabled=Config.MIXED_PRECISION):\n            seg_logits, cls_logits = model(vol)\n            loss, seg_loss, cls_loss = criterion(seg_logits, cls_logits, mask, label, segw)\n        try:\n            scaler.scale(loss / Config.GRAD_ACCUM_STEPS).backward()\n            if ((n + 1) % Config.GRAD_ACCUM_STEPS) == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad(set_to_none=True)\n        except Exception:\n            optimizer.zero_grad(set_to_none=True)\n            continue\n        bs = vol.size(0)\n        t_loss += loss.item()*bs; t_seg += seg_loss.item()*bs; t_cls += cls_loss.item()*bs; n += bs\n        pbar.set_postfix(loss=f\"{t_loss/max(n,1):.4f}\", seg=f\"{t_seg/max(n,1):.4f}\", cls=f\"{t_cls/max(n,1):.4f}\", lr=f\"{optimizer.param_groups[0]['lr']:.2e}\")\n    return t_loss/n, t_seg/n, t_cls/n\n\n@torch.no_grad()\ndef validate_epoch(model, loader, criterion, epoch=None, phase_name=\"P1\"):\n    model.eval()\n    t_loss = t_seg = t_cls = 0.0\n    n = 0\n    all_probs = []\n    all_labels= []\n    pbar = tqdm(loader, desc=f\"Valid {phase_name}{'' if epoch is None else f' [ep {epoch}]'}\", leave=False, mininterval=0.1)\n    for batch in pbar:\n        vol   = batch['volume'].to(Config.DEVICE, non_blocking=True)\n        try:\n            vol = vol.to(memory_format=torch.channels_last_3d)\n        except Exception:\n            pass\n        mask  = batch['mask'].to(Config.DEVICE, non_blocking=True)\n        label = batch['label'].to(Config.DEVICE, non_blocking=True)\n        segw  = batch['seg_weight'].to(Config.DEVICE, non_blocking=True)\n        with torch.amp.autocast('cuda', enabled=Config.MIXED_PRECISION):\n            seg_logits, cls_logits = model(vol)\n            loss, seg_loss, cls_loss = criterion(seg_logits, cls_logits, mask, label, segw)\n        bs = vol.size(0)\n        t_loss += loss.item()*bs; t_seg += seg_loss.item()*bs; t_cls += cls_loss.item()*bs; n += bs\n        pbar.set_postfix(loss=f\"{t_loss/max(n,1):.4f}\", seg=f\"{t_seg/max(n,1):.4f}\", cls=f\"{t_cls/max(n,1):.4f}\")\n        all_probs.append(torch.sigmoid(cls_logits).detach().cpu().view(-1).numpy())\n        all_labels.append(label.detach().cpu().view(-1).numpy())\n    all_probs = np.concatenate(all_probs) if len(all_probs)>0 else np.array([])\n    all_labels = np.concatenate(all_labels) if len(all_labels)>0 else np.array([])\n    auc = np.nan\n    try:\n        if len(all_probs)>0 and len(np.unique(all_labels)) > 1:\n            auc = float(roc_auc_score(all_labels, all_probs))\n    except Exception:\n        pass\n    return t_loss/n, t_seg/n, t_cls/n, auc\n\n# ====================================================\n# Main\n# ====================================================\n\ndef run_training():\n    set_seed(Config.SEED)\n    # TF32 for better throughput\n    try:\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n        torch.set_float32_matmul_precision('high')\n    except Exception:\n        pass\n    df = load_manifest_df()\n\n    # Build a single stratified split (can expand to CV later)\n    y = df['label'].astype(int).values\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=Config.SEED)\n    train_idx, val_idx = next(skf.split(np.zeros_like(y), y))\n    train_df = df.iloc[train_idx].reset_index(drop=True)\n    val_df   = df.iloc[val_idx].reset_index(drop=True)\n\n    # Phase 1 datasets/loads\n    ds_train_p1 = PrebuiltDataset(train_df, phase_synth_w=Config.PHASE1_SYNTH_SEG_W)\n    ds_val_p1   = PrebuiltDataset(val_df,   phase_synth_w=Config.PHASE1_SYNTH_SEG_W)\n    dl_train = DataLoader(ds_train_p1, batch_size=Config.STAGE1_BATCH_SIZE, shuffle=True,\n                          num_workers=Config.NUM_WORKERS, pin_memory=True,\n                          prefetch_factor=Config.PREFETCH_FACTOR,\n                          persistent_workers=Config.PERSISTENT_WORKERS)\n    dl_val   = DataLoader(ds_val_p1,   batch_size=Config.STAGE1_BATCH_SIZE * Config.VAL_BATCH_MULT, shuffle=False,\n                          num_workers=Config.VAL_NUM_WORKERS, pin_memory=True,\n                          prefetch_factor=Config.PREFETCH_FACTOR,\n                          persistent_workers=Config.PERSISTENT_WORKERS)\n\n    model = UNet3D(in_ch=1, base=24).to(Config.DEVICE)\n    try:\n        model = model.to(memory_format=torch.channels_last_3d)\n    except Exception:\n        pass\n    # Multi-GPU (if available): enable DP for train and val\n    dp_enabled = False\n    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n        dp_enabled = True\n        print(f\"Using {torch.cuda.device_count()} GPUs via DataParallel\")\n        model = nn.DataParallel(model)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.STAGE1_LR, weight_decay=Config.WEIGHT_DECAY)\n    criterion = EnhancedCombinedLoss().to(Config.DEVICE)\n    scaler = torch.amp.GradScaler('cuda', enabled=Config.MIXED_PRECISION)\n\n    best_loss = float('inf'); best_state = None; patience = 0\n    for epoch in range(1, Config.EPOCHS_PHASE1+1):\n        print(f\"\\n[Phase 1] Epoch {epoch}/{Config.EPOCHS_PHASE1}\")\n        tr_loss, tr_seg, tr_cls = train_epoch(model, dl_train, optimizer, criterion, scaler, epoch=epoch, phase_name='P1')\n        # Validate every 2 epochs until epoch 10, then every epoch\n        do_validate = (epoch % 2 == 0) or (epoch >= 10)\n        if do_validate:\n            va_loss, va_seg, va_cls, va_auc = validate_epoch(model, dl_val, criterion, epoch=epoch, phase_name='P1')\n            print(f\"Train Loss: {tr_loss:.4f} | Seg: {tr_seg:.4f} | Cls: {tr_cls:.4f} | GPU {gpu_mem_str()}\")\n            print(f\" Val  Loss: {va_loss:.4f} | Seg: {va_seg:.4f} | Cls: {va_cls:.4f} | AUC: {va_auc if not np.isnan(va_auc) else 'NA'} | GPU {gpu_mem_str()}\")\n            if va_loss < best_loss - 1e-5:\n                best_loss = va_loss; best_state = {k:v.detach().cpu() for k,v in (model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()).items()}; patience = 0\n                torch.save(best_state, 'stage1_phase1_best.pth')\n                print(\"💾 Saved Phase 1 best checkpoint\")\n            else:\n                patience += 1\n                if patience >= Config.EARLY_STOP_PATIENCE:\n                    print(\"Early stopping Phase 1\")\n                    break\n        else:\n            # Skip validation this epoch to save time\n            print(f\"Train Loss: {tr_loss:.4f} | Seg: {tr_seg:.4f} | Cls: {tr_cls:.4f} | (validation skipped) | GPU {gpu_mem_str()}\")\n\n    if best_state is not None:\n        if isinstance(model, nn.DataParallel):\n            model.module.load_state_dict(best_state)\n        else:\n            model.load_state_dict(best_state)\n\n    # Phase 2: small synthetic weight\n    print(\"\\n====== PHASE 2: enabling small synthetic seg supervision ======\")\n    ds_train_p2 = PrebuiltDataset(train_df, phase_synth_w=Config.PHASE2_SYNTH_SEG_W)\n    ds_val_p2   = PrebuiltDataset(val_df,   phase_synth_w=Config.PHASE2_SYNTH_SEG_W)\n    dl_train2 = DataLoader(ds_train_p2, batch_size=Config.STAGE1_BATCH_SIZE, shuffle=True,\n                           num_workers=Config.NUM_WORKERS, pin_memory=True,\n                           prefetch_factor=Config.PREFETCH_FACTOR,\n                           persistent_workers=Config.PERSISTENT_WORKERS)\n    dl_val2   = DataLoader(ds_val_p2,   batch_size=Config.STAGE1_BATCH_SIZE * Config.VAL_BATCH_MULT, shuffle=False,\n                           num_workers=Config.VAL_NUM_WORKERS, pin_memory=True,\n                           prefetch_factor=Config.PREFETCH_FACTOR,\n                           persistent_workers=Config.PERSISTENT_WORKERS)\n\n    # optional: lower LR a bit for fine-tune\n    for g in optimizer.param_groups:\n        g['lr'] = Config.STAGE1_LR * 0.5\n\n    best2 = float('inf'); best2_state = None; patience = 0\n    for epoch in range(1, Config.EPOCHS_PHASE2+1):\n        print(f\"\\n[Phase 2] Epoch {epoch}/{Config.EPOCHS_PHASE2}\")\n        tr_loss, tr_seg, tr_cls = train_epoch(model, dl_train2, optimizer, criterion, scaler, epoch=epoch, phase_name='P2')\n        va_loss, va_seg, va_cls, va_auc = validate_epoch(model, dl_val2, criterion, epoch=epoch, phase_name='P2')\n        print(f\"Train Loss: {tr_loss:.4f} | Seg: {tr_seg:.4f} | Cls: {tr_cls:.4f} | GPU {gpu_mem_str()}\")\n        print(f\" Val  Loss: {va_loss:.4f} | Seg: {va_seg:.4f} | Cls: {va_cls:.4f} | AUC: {va_auc if not np.isnan(va_auc) else 'NA'} | GPU {gpu_mem_str()}\")\n        if va_loss < best2 - 1e-5:\n            best2 = va_loss; best2_state = {k:v.detach().cpu() for k,v in (model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()).items()}; patience = 0\n            torch.save(best2_state, 'stage1_phase2_best.pth')\n            print(\"💾 Saved Phase 2 best checkpoint\")\n        else:\n            patience += 1\n            if patience >= Config.EARLY_STOP_PATIENCE:\n                print(\"Early stopping Phase 2\")\n                break\n\n    final_state = best2_state or best_state or (model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict())\n    torch.save(final_state, 'stage1_segmentation_best.pth')\n    print(\"\\n✅ Stage 1 complete. Saved: stage1_segmentation_best.pth\")\n\n\nif __name__ == '__main__':\n    run_training()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-26T05:17:22.096316Z","iopub.execute_input":"2025-08-26T05:17:22.096662Z","execution_failed":"2025-08-26T06:11:28.866Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs via DataParallel\n\n[Phase 1] Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9588 | Seg: 0.2767 | Cls: 0.6821 | (validation skipped) | GPU 0.05G/0.27G\n\n[Phase 1] Epoch 2/15\n","output_type":"stream"},{"name":"stderr","text":"Valid P1 [ep 2]:  16%|█▌        | 6/37 [24:00<2:03:57, 239.90s/it, cls=0.7116, loss=0.9381, seg=0.2265]             ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# --- Config for quick eval ---\nCKPT = \"stage1_phase1_best.pth\"  # later compare with \"stage1_phase2_best.pth\"\nDEVICE = Config.DEVICE\nBATCH = Config.STAGE1_BATCH_SIZE\nWORKERS = max(2, Config.NUM_WORKERS)\n\n# Val split identical to training: 5-fold StratifiedKFold, same SEED\ndf = load_manifest_df()\ny = df[\"label\"].astype(int).values\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=Config.SEED)\ntrain_idx, val_idx = next(skf.split(np.zeros_like(y), y))\nval_df = df.iloc[val_idx].reset_index(drop=True)\n\n# Dataloader (no synthetic seg weight needed for eval)\nval_ds = PrebuiltDataset(val_df, phase_synth_w=0.0)\nval_dl = DataLoader(val_ds, batch_size=BATCH, shuffle=False,\n                    num_workers=WORKERS, pin_memory=True,\n                    prefetch_factor=getattr(Config, \"PREFETCH_FACTOR\", 2),\n                    persistent_workers=getattr(Config, \"PERSISTENT_WORKERS\", True))\n\n# Load model\nmodel = UNet3D(in_ch=1, base=24)\nstate = torch.load(CKPT, map_location=\"cpu\")\nmodel.load_state_dict(state)\nmodel.to(DEVICE)\nmodel.eval()\nprint(f\"Loaded {CKPT}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import roc_auc_score, roc_curve, precision_recall_fscore_support\n\nprobs = []\nlabels = []\n\nwith torch.no_grad():\n    for batch in val_dl:\n        vol = batch[\"volume\"].to(DEVICE, non_blocking=True)\n        _, cls_logits = model(vol)\n        p = torch.sigmoid(cls_logits).squeeze(1).detach().cpu().numpy()\n        probs.append(p)\n        labels.append(batch[\"label\"].view(-1).numpy())\n\nprobs = np.concatenate(probs)\nlabels = np.concatenate(labels).astype(int)\n\nauc = roc_auc_score(labels, probs) if len(np.unique(labels))>1 else np.nan\nfpr, tpr, thr = roc_curve(labels, probs)\nyouden = tpr - fpr\nbest_idx = int(np.argmax(youden))\nbest_thr = float(thr[best_idx]) if thr.size else 0.5\n\npred = (probs >= best_thr).astype(int)\nprec, rec, f1, _ = precision_recall_fscore_support(labels, pred, average=\"binary\", zero_division=0)\n\nprint(f\"AUC: {auc:.4f}\")\nprint(f\"Best threshold (Youden): {best_thr:.3f} | Precision {prec:.3f} | Recall {rec:.3f} | F1 {f1:.3f}\")\nprint(\"Confusion @ best thr:\",\n      f\"TN={(pred==0)&(labels==0).sum()}\",\n      f\"FP={(pred==1)&(labels==0).sum()}\",\n      f\"FN={(pred==0)&(labels==1).sum()}\",\n      f\"TP={(pred==1)&(labels==1).sum()}\", sep=\"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter val rows that have REAL masks\nval_real = val_df[val_df[\"mask_relpath\"].fillna(\"\").str.startswith(\"masks/\")].reset_index(drop=True)\nprint(\"Val real-mask subset size:\", len(val_real))\n\nreal_ds = PrebuiltDataset(val_real, phase_synth_w=0.0)\nreal_dl = DataLoader(real_ds, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n\ndef soft_dice(logits, target, eps=1e-6):\n    probs = torch.sigmoid(logits)\n    num = 2*(probs*target).sum()\n    den = (probs*probs + target*target).sum() + eps\n    return (num/den).item()\n\ndice_scores = []\nwith torch.no_grad():\n    for batch in real_dl:\n        vol  = batch[\"volume\"].to(DEVICE)\n        mask = batch[\"mask\"].to(DEVICE)\n        seg_logits, _ = model(vol)\n        d = soft_dice(seg_logits, mask)\n        dice_scores.append(d)\n\nif len(dice_scores):\n    qs = np.percentile(dice_scores, [0,10,25,50,75,90,95])\n    print(\"Soft Dice on real-mask subset:\")\n    print(\"mean:\", np.mean(dice_scores), \"| median:\", np.median(dice_scores))\n    print(\"percentiles [0,10,25,50,75,90,95]:\", qs)\nelse:\n    print(\"No real-mask samples in this fold’s validation set.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\n\nos.makedirs(\"/kaggle/working/qual\", exist_ok=True)\n\n# pick some examples: top TP, FP, FN by distance to threshold\nd = np.abs(probs - best_thr)\norder = np.argsort(d)\npick_idx = list(order[:3])  # closest to decision boundary\n# add a confident TP/FP if available\ntp = np.where((labels==1) & (probs>=best_thr))[0]\nfp = np.where((labels==0) & (probs>=best_thr))[0]\nif tp.size: pick_idx.append(int(tp[np.argmax(probs[tp])]))\nif fp.size: pick_idx.append(int(fp[np.argmax(probs[fp])]))\n\ndef render_one(i):\n    sid = val_df.iloc[i][\"series_id\"]\n    ds_one = PrebuiltDataset(val_df.iloc[[i]], phase_synth_w=0.0)\n    sample = ds_one[0]\n    vol  = sample[\"volume\"].unsqueeze(0).to(DEVICE)\n    seg_logits, cls_logits = model(vol)\n    prob = torch.sigmoid(cls_logits).item()\n    segp = torch.sigmoid(seg_logits)[0,0].detach().cpu().numpy()  # (D,H,W)\n    vol0 = sample[\"volume\"][0].numpy()  # (D,H,W)\n\n    # choose mid slice or slice with max predicted area\n    z = int(np.argmax(segp.sum(axis=(1,2))))\n    vsl = vol0[z]; msl = segp[z]\n\n    plt.figure(figsize=(6,6))\n    plt.imshow(vsl, cmap=\"gray\")\n    plt.imshow((msl>0.5).astype(float), alpha=0.35)  # predicted mask\n    plt.title(f\"{sid}\\nprob={prob:.3f}\")\n    out = f\"/kaggle/working/qual/{sid}_z{z}.png\"\n    plt.axis(\"off\"); plt.tight_layout(); plt.savefig(out, dpi=120); plt.close()\n    return out\n\nouts = [render_one(i) for i in pick_idx]\nprint(\"Saved overlays:\", *outs, sep=\"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay\n\nplt.figure(figsize=(6,4))\nplt.hist(probs[labels==0], bins=40, alpha=0.6, label=\"neg\")\nplt.hist(probs[labels==1], bins=40, alpha=0.6, label=\"pos\")\nplt.axvline(best_thr, ls=\"--\")\nplt.title(\"Score distribution\"); plt.legend(); plt.tight_layout()\nplt.show()\n\nRocCurveDisplay.from_predictions(labels, probs)\nplt.show()\nPrecisionRecallDisplay.from_predictions(labels, probs)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}