{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/02-aneurysmnet-cnn-intracranial-training-nb153?scriptVersionId=254437480\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"c853d350","metadata":{"execution":{"iopub.execute_input":"2025-08-05T19:10:05.729241Z","iopub.status.busy":"2025-08-05T19:10:05.729041Z","iopub.status.idle":"2025-08-05T19:10:17.705784Z","shell.execute_reply":"2025-08-05T19:10:17.705219Z"},"papermill":{"duration":11.981597,"end_time":"2025-08-05T19:10:17.707161","exception":false,"start_time":"2025-08-05T19:10:05.725564","status":"completed"},"tags":[]},"outputs":[],"source":["# ====================================================\n","# RSNA INTRACRANIAL ANEURYSM DETECTION - TRAINING PIPELINE\n","# ====================================================\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pydicom\n","import nibabel as nib\n","import cv2\n","from scipy import ndimage\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":2,"id":"bd16f0fc","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-08-05T19:10:17.714767Z","iopub.status.busy":"2025-08-05T19:10:17.714412Z","iopub.status.idle":"2025-08-05T19:10:17.856729Z","shell.execute_reply":"2025-08-05T19:10:17.855791Z"},"papermill":{"duration":0.14676,"end_time":"2025-08-05T19:10:17.857883","exception":false,"start_time":"2025-08-05T19:10:17.711123","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["✅ Configuration loaded - Device: cuda\n","✅ Custom 3D UNet and transforms loaded (MONAI-free!)\n","✅ DICOM Processor loaded\n","✅ Dataset class loaded\n","✅ Model architecture loaded\n","✅ Loss functions loaded\n","✅ Training functions loaded\n","✅ ROI Extractor loaded (for Stage 2)\n"]}],"source":["# ====================================================\n","# CELL 2: CONFIGURATION\n","# ====================================================\n","\n","class Config:\n","    # Paths\n","    TRAIN_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv'\n","    LOCALIZER_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train_localizers.csv'\n","    SERIES_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/'\n","    SEGMENTATION_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/segmentations/'\n","    \n","    # Stage 1: 3D Segmentation\n","    STAGE1_TARGET_SIZE = (64, 128, 128)  # Smaller for speed\n","    STAGE1_BATCH_SIZE = 4\n","    STAGE1_EPOCHS = 5\n","    STAGE1_LR = 1e-3\n","    \n","    # General\n","    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    MIXED_PRECISION = True\n","    N_FOLDS = 3\n","    \n","    # Competition constants\n","    ID_COL = 'SeriesInstanceUID'\n","    LABEL_COLS = [\n","        'Left Infraclinoid Internal Carotid Artery', 'Right Infraclinoid Internal Carotid Artery',\n","        'Left Supraclinoid Internal Carotid Artery', 'Right Supraclinoid Internal Carotid Artery',\n","        'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery', 'Anterior Communicating Artery',\n","        'Left Anterior Cerebral Artery', 'Right Anterior Cerebral Artery',\n","        'Left Posterior Communicating Artery', 'Right Posterior Communicating Artery',\n","        'Basilar Tip', 'Other Posterior Circulation', 'Aneurysm Present',\n","    ]\n","    TARGET_COL = 'Aneurysm Present'\n","    \n","    # Debug settings\n","    DEBUG_MODE = True\n","    DEBUG_SAMPLES = 50  # Use small subset for testing\n","\n","print(f\"✅ Configuration loaded - Device: {Config.DEVICE}\")\n","\n","# ====================================================\n","# CELL 2.5: CUSTOM 3D UNET (REPLACES MONAI BASICUNET)\n","# ====================================================\n","\n","class Custom3DUNet(nn.Module):\n","    \"\"\"Pure PyTorch 3D UNet implementation to replace MONAI BasicUNet\"\"\"\n","    \n","    def __init__(self, spatial_dims=3, in_channels=1, out_channels=32, \n","                 features=(32, 64, 128, 256, 512, 32), dropout=0.1):\n","        super().__init__()\n","        \n","        self.features = features\n","        self.dropout = dropout\n","        \n","        # Encoder (downsampling path)\n","        self.encoder_blocks = nn.ModuleList()\n","        prev_channels = in_channels\n","        \n","        for i, feature_count in enumerate(features[:-1]):  # Exclude last feature (decoder output)\n","            # Each encoder block: Conv3D -> BatchNorm -> ReLU -> Conv3D -> BatchNorm -> ReLU\n","            block = nn.Sequential(\n","                nn.Conv3d(prev_channels, feature_count, kernel_size=3, padding=1),\n","                nn.BatchNorm3d(feature_count),\n","                nn.ReLU(inplace=True),\n","                nn.Conv3d(feature_count, feature_count, kernel_size=3, padding=1),\n","                nn.BatchNorm3d(feature_count),\n","                nn.ReLU(inplace=True),\n","                nn.Dropout3d(dropout) if dropout > 0 else nn.Identity()\n","            )\n","            self.encoder_blocks.append(block)\n","            prev_channels = feature_count\n","        \n","        # Downsampling layers (MaxPool)\n","        self.downsample_layers = nn.ModuleList([\n","            nn.MaxPool3d(kernel_size=2, stride=2) \n","            for _ in range(len(features) - 2)  # No downsampling after last encoder block\n","        ])\n","        \n","        # Decoder (upsampling path)\n","        self.decoder_blocks = nn.ModuleList()\n","        self.upsample_layers = nn.ModuleList()\n","        \n","        # Reverse the features for decoder (skip the input feature count)\n","        decoder_features = list(reversed(features[:-1]))  # [512, 256, 128, 64, 32]\n","        \n","        for i in range(len(decoder_features) - 1):\n","            current_features = decoder_features[i]\n","            next_features = decoder_features[i + 1]\n","            \n","            # Upsampling layer\n","            upsample = nn.ConvTranspose3d(\n","                current_features, next_features, \n","                kernel_size=2, stride=2\n","            )\n","            self.upsample_layers.append(upsample)\n","            \n","            # Decoder block (concatenation + convolutions)\n","            # Input: upsampled features + skip connection = next_features * 2\n","            decoder_block = nn.Sequential(\n","                nn.Conv3d(next_features * 2, next_features, kernel_size=3, padding=1),\n","                nn.BatchNorm3d(next_features),\n","                nn.ReLU(inplace=True),\n","                nn.Conv3d(next_features, next_features, kernel_size=3, padding=1),\n","                nn.BatchNorm3d(next_features),\n","                nn.ReLU(inplace=True),\n","                nn.Dropout3d(dropout) if dropout > 0 else nn.Identity()\n","            )\n","            self.decoder_blocks.append(decoder_block)\n","        \n","        # Final output convolution\n","        self.final_conv = nn.Conv3d(features[0], out_channels, kernel_size=1)\n","        \n","    def forward(self, x):\n","        # Store skip connections\n","        skip_connections = []\n","        \n","        # Encoder path\n","        for i, encoder_block in enumerate(self.encoder_blocks):\n","            x = encoder_block(x)\n","            skip_connections.append(x)\n","            \n","            # Downsample (except for the last encoder block)\n","            if i < len(self.downsample_layers):\n","                x = self.downsample_layers[i](x)\n","        \n","        # Decoder path\n","        skip_connections = skip_connections[:-1]  # Remove the deepest layer (no skip for bottleneck)\n","        skip_connections.reverse()  # Reverse to match decoder order\n","        \n","        for i, (upsample_layer, decoder_block) in enumerate(zip(self.upsample_layers, self.decoder_blocks)):\n","            # Upsample\n","            x = upsample_layer(x)\n","            \n","            # Get corresponding skip connection\n","            skip = skip_connections[i]\n","            \n","            # Ensure spatial dimensions match (handle odd-sized inputs)\n","            if x.shape[2:] != skip.shape[2:]:\n","                x = nn.functional.interpolate(x, size=skip.shape[2:], mode='trilinear', align_corners=False)\n","            \n","            # Concatenate skip connection\n","            x = torch.cat([x, skip], dim=1)\n","            \n","            # Apply decoder block\n","            x = decoder_block(x)\n","        \n","        # Final output\n","        x = self.final_conv(x)\n","        \n","        return x\n","\n","class CustomTransforms:\n","    \"\"\"Pure PyTorch transforms to replace MONAI transforms\"\"\"\n","    \n","    def __init__(self, keys=['volume']):\n","        self.keys = keys\n","        \n","    def __call__(self, data_dict):\n","        \"\"\"Apply transforms to data dictionary\"\"\"\n","        result = {}\n","        \n","        for key in data_dict:\n","            if key in self.keys:\n","                # Convert numpy array to tensor if needed\n","                if isinstance(data_dict[key], np.ndarray):\n","                    result[key] = torch.from_numpy(data_dict[key]).float()\n","                else:\n","                    result[key] = data_dict[key]\n","            else:\n","                result[key] = data_dict[key]\n","        \n","        return result\n","\n","print(\"✅ Custom 3D UNet and transforms loaded (MONAI-free!)\")\n","\n","# ====================================================\n","# CELL 3: SIMPLE DICOM PROCESSOR\n","# ====================================================\n","\n","class SimpleDICOMProcessor:\n","    def __init__(self, target_size=None):\n","        self.target_size = target_size or Config.STAGE1_TARGET_SIZE\n","        \n","    def load_dicom_series(self, series_path):\n","        \"\"\"Simple DICOM loading - no complex error handling\"\"\"\n","        try:\n","            dicom_files = [f for f in os.listdir(series_path) if f.endswith('.dcm')]\n","            if not dicom_files:\n","                return np.zeros(self.target_size, dtype=np.float32)\n","            \n","            # Load all DICOMs\n","            pixel_arrays = []\n","            for f in dicom_files[:50]:  # Limit to 50 files max for speed\n","                try:\n","                    ds = pydicom.dcmread(os.path.join(series_path, f), force=True)\n","                    if hasattr(ds, 'pixel_array'):\n","                        arr = ds.pixel_array\n","                        if arr.ndim == 2:  # Standard 2D slice\n","                            pixel_arrays.append(arr)\n","                        elif arr.ndim == 3:  # 3D volume - take middle slices\n","                            mid_start = arr.shape[0] // 4\n","                            mid_end = 3 * arr.shape[0] // 4\n","                            for slice_idx in range(mid_start, mid_end, 2):  # Every 2nd slice\n","                                pixel_arrays.append(arr[slice_idx])\n","                except:\n","                    continue\n","            \n","            if not pixel_arrays:\n","                return np.zeros(self.target_size, dtype=np.float32)\n","            \n","            # Resize all slices to same shape before stacking\n","            if len(pixel_arrays) > 0:\n","                # Use first slice shape as reference, or use a standard size\n","                target_slice_shape = (256, 256)  # Standard size for all slices\n","                \n","                resized_arrays = []\n","                for arr in pixel_arrays:\n","                    if arr.shape != target_slice_shape:\n","                        # Resize slice to target shape\n","                        resized_arr = ndimage.zoom(arr, \n","                                                 (target_slice_shape[0] / arr.shape[0], \n","                                                  target_slice_shape[1] / arr.shape[1]), \n","                                                 order=1)\n","                        resized_arrays.append(resized_arr)\n","                    else:\n","                        resized_arrays.append(arr)\n","                \n","                # Now stack - all arrays have same shape\n","                volume = np.stack(resized_arrays, axis=0).astype(np.float32)\n","            else:\n","                return np.zeros(self.target_size, dtype=np.float32)\n","            \n","            # Simple preprocessing\n","            volume = self.preprocess_volume(volume)\n","            return volume\n","            \n","        except Exception as e:\n","            print(f\"Failed to load {series_path}: {e}\")\n","            return np.zeros(self.target_size, dtype=np.float32)\n","    \n","    def preprocess_volume(self, volume):\n","        \"\"\"Simple preprocessing\"\"\"\n","        # Normalize\n","        p1, p99 = np.percentile(volume, [1, 99])\n","        volume = np.clip(volume, p1, p99)\n","        volume = (volume - p1) / (p99 - p1 + 1e-8)\n","        \n","        # Resize to target\n","        if volume.shape != self.target_size:\n","            zoom_factors = [self.target_size[i] / volume.shape[i] for i in range(3)]\n","            volume = ndimage.zoom(volume, zoom_factors, order=1)\n","        \n","        return volume.astype(np.float32)\n","\n","print(\"✅ DICOM Processor loaded\")\n","\n","# ====================================================\n","# CELL 4: DATASET CLASS\n","# ====================================================\n","\n","class SimpleSegmentationDataset(Dataset):\n","    def __init__(self, df, series_dir, processor, mode='train'):\n","        self.df = df\n","        self.series_dir = series_dir\n","        self.processor = processor\n","        self.mode = mode\n","        \n","        # Simple transform - using our custom PyTorch transforms\n","        self.transform = CustomTransforms(keys=['volume'])\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def load_segmentation_mask(self, series_id, volume_shape):\n","        \"\"\"Load real segmentation mask from competition data\"\"\"\n","        seg_path = os.path.join(Config.SEGMENTATION_DIR, f\"{series_id}.nii\")\n","        \n","        try:\n","            if os.path.exists(seg_path):\n","                # Load NIfTI segmentation mask\n","                import nibabel as nib\n","                nii_img = nib.load(seg_path)\n","                mask = nii_img.get_fdata().astype(np.float32)\n","                \n","                # Resize mask to match volume shape\n","                if mask.shape != volume_shape:\n","                    zoom_factors = [volume_shape[i] / mask.shape[i] for i in range(3)]\n","                    mask = ndimage.zoom(mask, zoom_factors, order=0)  # Nearest neighbor for masks\n","                \n","                # Normalize mask values to 0-1\n","                mask = (mask > 0).astype(np.float32)\n","                return mask\n","            else:\n","                # No segmentation available - create empty mask\n","                return np.zeros(volume_shape, dtype=np.float32)\n","                \n","        except Exception as e:\n","            print(f\"Error loading segmentation for {series_id}: {e}\")\n","            # Fallback: create simple mask if aneurysm present\n","            has_aneurysm = int(self.df[self.df[Config.ID_COL] == series_id][Config.TARGET_COL].iloc[0])\n","            if has_aneurysm:\n","                # Create a rough central region mask as fallback\n","                mask = np.zeros(volume_shape, dtype=np.float32)\n","                h, w, d = volume_shape\n","                mask[h//4:3*h//4, w//4:3*w//4, d//4:3*d//4] = 1.0\n","                return mask\n","            else:\n","                return np.zeros(volume_shape, dtype=np.float32)\n","    \n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        series_id = row[Config.ID_COL]\n","        series_path = os.path.join(self.series_dir, series_id)\n","        \n","        # Load volume\n","        volume = self.processor.load_dicom_series(series_path)\n","        \n","        # Load REAL segmentation mask from competition data\n","        mask = self.load_segmentation_mask(series_id, volume.shape)\n","        \n","        # Get aneurysm presence label\n","        has_aneurysm = int(row[Config.TARGET_COL])\n","        \n","        # Transform\n","        data_dict = {'volume': volume}\n","        if self.transform:\n","            data_dict = self.transform(data_dict)\n","        \n","        volume_tensor = data_dict['volume'].unsqueeze(0)  # Add channel dim\n","        mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n","        \n","        return {\n","            'volume': volume_tensor,\n","            'mask': mask_tensor,\n","            'has_aneurysm': torch.tensor(has_aneurysm, dtype=torch.float32),\n","            'series_id': series_id\n","        }\n","\n","print(\"✅ Dataset class loaded\")\n","\n","# ====================================================\n","# CELL 5: 3D U-NET MODEL\n","# ====================================================\n","\n","class Simple3DSegmentationNet(nn.Module):\n","    def __init__(self, in_channels=1, out_channels=1):\n","        super().__init__()\n","        \n","        # Use our Custom3DUNet - pure PyTorch implementation!\n","        self.backbone = Custom3DUNet(\n","            spatial_dims=3,\n","            in_channels=in_channels,\n","            out_channels=32,\n","            features=(32, 64, 128, 256, 512, 32),\n","            dropout=0.1\n","        )\n","        \n","        # Segmentation head\n","        self.seg_head = nn.Conv3d(32, out_channels, kernel_size=1)\n","        \n","        # Classification head (aneurysm presence)\n","        self.global_pool = nn.AdaptiveAvgPool3d(1)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(32, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(64, 1)\n","        )\n","        \n","    def forward(self, x):\n","        # Extract features\n","        features = self.backbone(x)\n","        \n","        # Segmentation output\n","        seg_logits = self.seg_head(features)\n","        \n","        # Classification output\n","        pooled_features = self.global_pool(features).flatten(1)\n","        cls_logits = self.classifier(pooled_features)\n","        \n","        return seg_logits, cls_logits\n","\n","print(\"✅ Model architecture loaded\")\n","\n","# ====================================================\n","# CELL 6: LOSS FUNCTIONS\n","# ====================================================\n","\n","class CombinedLoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.bce_loss = nn.BCEWithLogitsLoss()\n","        self.seg_loss = nn.BCEWithLogitsLoss()\n","        \n","    def forward(self, seg_logits, cls_logits, seg_targets, cls_targets):\n","        # Segmentation loss\n","        seg_loss = self.seg_loss(seg_logits, seg_targets)\n","        \n","        # Classification loss\n","        cls_loss = self.bce_loss(cls_logits.squeeze(), cls_targets)\n","        \n","        # Combined loss\n","        total_loss = seg_loss + 0.5 * cls_loss\n","        \n","        return total_loss, seg_loss, cls_loss\n","\n","print(\"✅ Loss functions loaded\")\n","\n","# ====================================================\n","# CELL 7: TRAINING FUNCTIONS\n","# ====================================================\n","\n","def train_epoch(model, loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0\n","    total_seg_loss = 0\n","    total_cls_loss = 0\n","    num_batches = 0\n","    \n","    for batch in tqdm(loader, desc=\"Training\"):\n","        volume = batch['volume'].to(device)\n","        mask = batch['mask'].to(device)\n","        has_aneurysm = batch['has_aneurysm'].to(device)\n","        \n","        optimizer.zero_grad()\n","        \n","        # Forward pass\n","        seg_logits, cls_logits = model(volume)\n","        \n","        # Calculate loss\n","        loss, seg_loss, cls_loss = criterion(seg_logits, cls_logits, mask, has_aneurysm)\n","        \n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","        \n","        total_loss += loss.item()\n","        total_seg_loss += seg_loss.item()\n","        total_cls_loss += cls_loss.item()\n","        num_batches += 1\n","    \n","    return (total_loss / num_batches, \n","            total_seg_loss / num_batches, \n","            total_cls_loss / num_batches)\n","\n","def validate_epoch(model, loader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    total_seg_loss = 0\n","    total_cls_loss = 0\n","    num_batches = 0\n","    \n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=\"Validating\"):\n","            volume = batch['volume'].to(device)\n","            mask = batch['mask'].to(device)\n","            has_aneurysm = batch['has_aneurysm'].to(device)\n","            \n","            # Forward pass\n","            seg_logits, cls_logits = model(volume)\n","            \n","            # Calculate loss\n","            loss, seg_loss, cls_loss = criterion(seg_logits, cls_logits, mask, has_aneurysm)\n","            \n","            total_loss += loss.item()\n","            total_seg_loss += seg_loss.item()\n","            total_cls_loss += cls_loss.item()\n","            num_batches += 1\n","    \n","    return (total_loss / num_batches, \n","            total_seg_loss / num_batches, \n","            total_cls_loss / num_batches)\n","\n","print(\"✅ Training functions loaded\")\n","\n","# ====================================================\n","# CELL 8: MAIN TRAINING LOOP\n","# ====================================================\n","\n","def main():\n","    print(f\"🚀 STAGE 1: 3D SEGMENTATION FOR REGION LOCALIZATION\")\n","    print(f\"Using device: {Config.DEVICE}\")\n","    print(f\"Target size: {Config.STAGE1_TARGET_SIZE}\")\n","    \n","    # Load data\n","    train_df = pd.read_csv(Config.TRAIN_CSV_PATH)\n","    \n","    # Load localizer data (for future use)\n","    try:\n","        localizer_df = pd.read_csv(Config.LOCALIZER_CSV_PATH)\n","        print(f\"Loaded localizer data: {len(localizer_df)} entries\")\n","    except:\n","        localizer_df = None\n","        print(\"No localizer data found - continuing without it\")\n","    \n","    # Debug mode - small subset\n","    if Config.DEBUG_MODE:\n","        train_df = train_df.head(Config.DEBUG_SAMPLES)\n","    print(f\"Training samples: {len(train_df)}\")\n","    print(f\"Aneurysm cases: {train_df[Config.TARGET_COL].sum()}\")\n","    \n","    # Simple train/val split\n","    val_size = len(train_df) // 5\n","    val_df = train_df[:val_size].reset_index(drop=True)\n","    train_df = train_df[val_size:].reset_index(drop=True)\n","    \n","    print(f\"Train: {len(train_df)}, Val: {len(val_df)}\")\n","    \n","    # Create datasets\n","    processor = SimpleDICOMProcessor()\n","    train_dataset = SimpleSegmentationDataset(train_df, Config.SERIES_DIR, processor, 'train')\n","    val_dataset = SimpleSegmentationDataset(val_df, Config.SERIES_DIR, processor, 'val')\n","    \n","    # Create loaders\n","    train_loader = DataLoader(train_dataset, batch_size=Config.STAGE1_BATCH_SIZE, shuffle=True, num_workers=2)\n","    val_loader = DataLoader(val_dataset, batch_size=Config.STAGE1_BATCH_SIZE, shuffle=False, num_workers=2)\n","    \n","    # Create model\n","    model = Simple3DSegmentationNet().to(Config.DEVICE)\n","    \n","    # Multi-GPU if available\n","    if torch.cuda.device_count() > 1:\n","        print(f\"Using {torch.cuda.device_count()} GPUs\")\n","        model = nn.DataParallel(model)\n","    \n","    # Optimizer and loss\n","    optimizer = optim.AdamW(model.parameters(), lr=Config.STAGE1_LR)\n","    criterion = CombinedLoss()\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.STAGE1_EPOCHS)\n","    \n","    # Training loop\n","    best_loss = float('inf')\n","    \n","    for epoch in range(Config.STAGE1_EPOCHS):\n","        print(f\"\\nEpoch {epoch+1}/{Config.STAGE1_EPOCHS}\")\n","        \n","        # Train\n","        train_loss, train_seg_loss, train_cls_loss = train_epoch(\n","            model, train_loader, optimizer, criterion, Config.DEVICE\n","        )\n","        \n","        # Validate\n","        val_loss, val_seg_loss, val_cls_loss = validate_epoch(\n","            model, val_loader, criterion, Config.DEVICE\n","        )\n","        \n","        # Step scheduler\n","        scheduler.step()\n","        \n","        print(f\"Train - Total: {train_loss:.4f}, Seg: {train_seg_loss:.4f}, Cls: {train_cls_loss:.4f}\")\n","        print(f\"Val   - Total: {val_loss:.4f}, Seg: {val_seg_loss:.4f}, Cls: {val_cls_loss:.4f}\")\n","        \n","        # Save best model\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            torch.save({\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'epoch': epoch,\n","                'val_loss': val_loss\n","            }, 'stage1_segmentation_best.pth')\n","            print(f\"💾 Saved best model (val_loss: {val_loss:.4f})\")\n","    \n","    print(f\"\\n✅ Stage 1 complete! Best val loss: {best_loss:.4f}\")\n","    print(\"📁 Model saved as 'stage1_segmentation_best.pth'\")\n","    \n","    return model\n","\n","# ====================================================\n","# CELL 9: ROI EXTRACTOR FOR STAGE 2 (FUTURE USE)\n","# ====================================================\n","\n","class ROIExtractor:\n","    def __init__(self, roi_size=(224, 224), confidence_threshold=0.5):\n","        self.roi_size = roi_size\n","        self.confidence_threshold = confidence_threshold\n","    \n","    def extract_rois(self, volume, segmentation_mask):\n","        \"\"\"Extract 2D ROI slices from 3D volume using segmentation mask\"\"\"\n","        rois = []\n","        \n","        # Find slices with high confidence regions\n","        for slice_idx in range(volume.shape[0]):\n","            slice_volume = volume[slice_idx]\n","            slice_mask = segmentation_mask[slice_idx]\n","            \n","            # Check if this slice has potential aneurysm regions\n","            if np.max(slice_mask) > self.confidence_threshold:\n","                # Find connected components\n","                binary_mask = (slice_mask > self.confidence_threshold).astype(np.uint8)\n","                \n","                # Find contours\n","                contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","                \n","                for contour in contours:\n","                    # Get bounding box\n","                    x, y, w, h = cv2.boundingRect(contour)\n","                    \n","                    # Expand bounding box\n","                    margin = max(w, h) // 4\n","                    x = max(0, x - margin)\n","                    y = max(0, y - margin)\n","                    w = min(slice_volume.shape[1] - x, w + 2*margin)\n","                    h = min(slice_volume.shape[0] - y, h + 2*margin)\n","                    \n","                    # Extract ROI\n","                    roi = slice_volume[y:y+h, x:x+w]\n","                    \n","                    # Resize to standard size\n","                    roi_resized = cv2.resize(roi, self.roi_size)\n","                    \n","                    rois.append({\n","                        'roi': roi_resized,\n","                        'slice_idx': slice_idx,\n","                        'bbox': (x, y, w, h),\n","                        'confidence': np.max(slice_mask[y:y+h, x:x+w])\n","                    })\n","        \n","        return rois\n","\n","print(\"✅ ROI Extractor loaded (for Stage 2)\")"]},{"cell_type":"code","execution_count":3,"id":"94c65b82","metadata":{"execution":{"iopub.execute_input":"2025-08-05T19:10:17.863094Z","iopub.status.busy":"2025-08-05T19:10:17.862488Z","iopub.status.idle":"2025-08-05T19:13:04.679151Z","shell.execute_reply":"2025-08-05T19:13:04.678001Z"},"papermill":{"duration":166.821052,"end_time":"2025-08-05T19:13:04.681077","exception":false,"start_time":"2025-08-05T19:10:17.860025","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["🚀 STAGE 1: 3D SEGMENTATION FOR REGION LOCALIZATION\n","Using device: cuda\n","Target size: (64, 128, 128)\n","Loaded localizer data: 2286 entries\n","Training samples: 50\n","Aneurysm cases: 22\n","Train: 40, Val: 10\n","Using 2 GPUs\n","\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 10/10 [00:30<00:00,  3.09s/it]\n","Validating: 100%|██████████| 3/3 [00:09<00:00,  3.01s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Train - Total: 0.9154, Seg: 0.5679, Cls: 0.6950\n","Val   - Total: 0.9742, Seg: 0.6346, Cls: 0.6791\n","💾 Saved best model (val_loss: 0.9742)\n","\n","Epoch 2/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 10/10 [00:21<00:00,  2.18s/it]\n","Validating: 100%|██████████| 3/3 [00:06<00:00,  2.08s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Train - Total: 0.7396, Seg: 0.3949, Cls: 0.6894\n","Val   - Total: 0.6267, Seg: 0.2984, Cls: 0.6567\n","💾 Saved best model (val_loss: 0.6267)\n","\n","Epoch 3/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 10/10 [00:23<00:00,  2.32s/it]\n","Validating: 100%|██████████| 3/3 [00:06<00:00,  2.08s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Train - Total: 0.6587, Seg: 0.3077, Cls: 0.7020\n","Val   - Total: 0.6000, Seg: 0.2569, Cls: 0.6861\n","💾 Saved best model (val_loss: 0.6000)\n","\n","Epoch 4/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 10/10 [00:23<00:00,  2.37s/it]\n","Validating: 100%|██████████| 3/3 [00:06<00:00,  2.13s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Train - Total: 0.5984, Seg: 0.2544, Cls: 0.6880\n","Val   - Total: 0.5821, Seg: 0.2392, Cls: 0.6859\n","💾 Saved best model (val_loss: 0.5821)\n","\n","Epoch 5/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 10/10 [00:22<00:00,  2.29s/it]\n","Validating: 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Train - Total: 0.5734, Seg: 0.2284, Cls: 0.6899\n","Val   - Total: 0.5757, Seg: 0.2326, Cls: 0.6862\n","💾 Saved best model (val_loss: 0.5757)\n","\n","✅ Stage 1 complete! Best val loss: 0.5757\n","📁 Model saved as 'stage1_segmentation_best.pth'\n","Expected training time: 1-2 hours\n","Output: stage1_segmentation_best.pth\n"]}],"source":["# ====================================================\n","# CELL 10: RUN TRAINING\n","# ====================================================\n","\n","# Start Training\n","model = main()\n","\n","print(\"Expected training time: 1-2 hours\")\n","print(\"Output: stage1_segmentation_best.pth\")"]},{"cell_type":"code","execution_count":null,"id":"5f266aea","metadata":{"papermill":{"duration":0.006071,"end_time":"2025-08-05T19:13:04.70117","exception":false,"start_time":"2025-08-05T19:13:04.695099","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":13190393,"sourceId":99552,"sourceType":"competition"}],"dockerImageVersionId":31089,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":188.23529,"end_time":"2025-08-05T19:13:07.925494","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-08-05T19:09:59.690204","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}