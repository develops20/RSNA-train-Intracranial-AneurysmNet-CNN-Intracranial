{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":99552,"databundleVersionId":13190393,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/02-aneurysmnet-cnn-intracranial-nb153?scriptVersionId=254278695\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install monai\n\n# ====================================================\n# RSNA INTRACRANIAL ANEURYSM DETECTION - TRAINING PIPELINE\n# ====================================================\n\nimport os\nimport gc\nimport warnings\nimport json\nimport time\nimport numpy as np\nimport pandas as pd\nfrom typing import Tuple, Dict, List\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport albumentations as A\n\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.amp import autocast, GradScaler\n\nimport pydicom\nimport pydicom.errors\nfrom scipy import ndimage\nimport nibabel as nib\nfrom monai.transforms import (\n    Compose, RandRotate90d, RandFlipd, RandAffined,\n    RandGaussianNoised, RandAdjustContrastd, ToTensord\n)\nfrom monai.networks.nets import BasicUNet\nfrom monai.losses import DiceCELoss, FocalLoss\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T05:33:54.383177Z","iopub.execute_input":"2025-08-05T05:33:54.383777Z","iopub.status.idle":"2025-08-05T05:35:42.946146Z","shell.execute_reply.started":"2025-08-05T05:33:54.383751Z","shell.execute_reply":"2025-08-05T05:35:42.945541Z"}},"outputs":[{"name":"stdout","text":"Collecting monai\n  Downloading monai-1.5.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai) (1.26.4)\nRequirement already satisfied: torch<2.7.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from monai) (2.6.0+cu124)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7.0,>=2.4.1->monai) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7.0,>=2.4.1->monai) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.24->monai) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\nDownloading monai-1.5.0-py3-none-any.whl (2.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, monai\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed monai-1.5.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"},{"name":"stderr","text":"2025-08-05 05:35:29.489767: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754372129.729696      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754372129.801450      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ====================================================\n# CONFIGURATION\n# ====================================================\n\nclass Config:\n    # Paths\n    TRAIN_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv'\n    LOCALIZER_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train_localizers.csv'\n    SERIES_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/'\n    SEGMENTATION_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/segmentations/'\n    \n    # Model parameters\n    TARGET_SIZE = (32, 64, 64)  # Increased resolution\n    EPOCHS = 10\n    BATCH_SIZE = 8  # Reduced due to larger input size\n    LEARNING_RATE = 1e-3\n    WEIGHT_DECAY = 1e-4\n    N_FOLDS = 3\n    \n    # Training parameters\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MIXED_PRECISION = True\n    GRADIENT_ACCUMULATION = 4\n    \n    # Competition constants\n    ID_COL = 'SeriesInstanceUID'\n    LABEL_COLS = [\n        'Left Infraclinoid Internal Carotid Artery', 'Right Infraclinoid Internal Carotid Artery',\n        'Left Supraclinoid Internal Carotid Artery', 'Right Supraclinoid Internal Carotid Artery',\n        'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery', 'Anterior Communicating Artery',\n        'Left Anterior Cerebral Artery', 'Right Anterior Cerebral Artery',\n        'Left Posterior Communicating Artery', 'Right Posterior Communicating Artery',\n        'Basilar Tip', 'Other Posterior Circulation', 'Aneurysm Present',\n    ]\n    \n    # Class weights for imbalanced data\n    ANEURYSM_PRESENT_WEIGHT = 13.0  # Match evaluation metric weighting\n\n# ====================================================\n# ENHANCED DATA PREPROCESSING\n# ====================================================\n\nclass AdvancedDICOMProcessor:\n    def __init__(self, target_size: Tuple[int, int, int] = Config.TARGET_SIZE):\n        self.target_size = target_size\n        self.stats = {\n            'total_loaded': 0,\n            'successful_loads': 0,\n            'shape_errors': 0,\n            'empty_volumes': 0,\n            'preprocessing_errors': 0\n        }\n        \n    def load_dicom_series(self, series_path: str) -> Tuple[np.ndarray, Dict]:\n        \"\"\"Load DICOM series with robust error handling\"\"\"\n        self.stats['total_loaded'] += 1\n        try:\n            dicom_files = [os.path.join(series_path, f) for f in os.listdir(series_path) if f.endswith('.dcm')]\n            if not dicom_files:\n                series_id = os.path.basename(series_path)\n                print(f\"No DICOM files found in {series_path}\")\n                self.stats['empty_volumes'] += 1\n                # Log series with no DICOM files\n                log_message = f\"{series_id}: No DICOM files found\\n\"\n                try:\n                    with open('corrupted_series.txt', 'a') as f:\n                        f.write(log_message)\n                        f.flush()  # Ensure immediate write\n                    print(f\"📝 Logged series with no DICOMs: {series_id}\")\n                except Exception as e:\n                    print(f\"⚠️  Failed to log series: {e}\")\n                return np.zeros(self.target_size, dtype=np.float32), {}\n                \n            dicoms = []\n            corrupted_count = 0\n            \n            for f in dicom_files:\n                try:\n                    ds = pydicom.dcmread(f, force=True)\n                    if hasattr(ds, 'pixel_array') and ds.pixel_array.size > 0:\n                        dicoms.append(ds)\n                    else:\n                        corrupted_count += 1\n                        if corrupted_count <= 3:  # Only log first few\n                            print(f\"  Skipping empty pixel array: {os.path.basename(f)}\")\n                except pydicom.errors.InvalidDicomError:\n                    corrupted_count += 1\n                    if corrupted_count <= 3:\n                        print(f\"  Invalid DICOM file: {os.path.basename(f)}\")\n                    continue\n                except Exception as e:\n                    corrupted_count += 1\n                    if corrupted_count <= 3:\n                        print(f\"  Error reading {os.path.basename(f)}: {e}\")\n                    continue\n                    \n            if not dicoms:\n                series_id = os.path.basename(series_path)\n                print(f\"❌ No valid DICOMs in series ({corrupted_count} corrupted files)\")\n                self.stats['empty_volumes'] += 1\n                # Log corrupted series for inspection\n                log_message = f\"{series_id}: No valid DICOMs ({corrupted_count} corrupted files)\\n\"\n                try:\n                    with open('corrupted_series.txt', 'a') as f:\n                        f.write(log_message)\n                        f.flush()  # Ensure immediate write\n                    print(f\"📝 Logged corrupted series: {series_id}\")\n                except Exception as e:\n                    print(f\"⚠️  Failed to log corrupted series: {e}\")\n                return np.zeros(self.target_size, dtype=np.float32), {}\n                \n            if corrupted_count > 0:\n                print(f\"⚠️  Series loaded with {corrupted_count} corrupted files (kept {len(dicoms)} valid)\")\n                \n            # Extract metadata from first DICOM\n            first_ds = dicoms[0]\n            metadata = {\n                'modality': getattr(first_ds, 'Modality', 'UNKNOWN'),\n                'spacing': getattr(first_ds, 'PixelSpacing', [1.0, 1.0]),\n                'slice_thickness': getattr(first_ds, 'SliceThickness', 1.0),\n                'rescale_slope': getattr(first_ds, 'RescaleSlope', 1.0),\n                'rescale_intercept': getattr(first_ds, 'RescaleIntercept', 0.0),\n            }\n            \n            # Sort by instance number \n            dicoms.sort(key=lambda x: int(getattr(x, 'InstanceNumber', 0)))\n            \n            # Process pixel arrays (minimal logging)\n            pixel_arrays = []\n            shapes = []\n            \n            for d in dicoms:\n                if hasattr(d, 'pixel_array'):\n                    try:\n                        arr = d.pixel_array\n                        if arr.ndim == 2 and arr.size > 0:  # Valid 2D slice\n                            pixel_arrays.append(arr)\n                            shapes.append(arr.shape)\n                    except:\n                        continue  # Skip corrupted slices\n            \n            if len(pixel_arrays) == 0:\n                print(f\"❌ No valid pixel arrays in series (corrupted DICOM)\")\n                self.stats['shape_errors'] += 1\n                return np.zeros(self.target_size, dtype=np.float32), metadata\n            \n            # Handle shape consistency\n            unique_shapes = list(set(shapes))\n            if len(unique_shapes) == 1:\n                # All same shape - direct stacking\n                volume = np.stack(pixel_arrays, axis=0).astype(np.float32)\n            else:\n                # Multiple shapes - resize to most common\n                most_common_shape = Counter(shapes).most_common(1)[0][0]\n                resized_arrays = []\n                for arr in pixel_arrays:\n                    if arr.shape == most_common_shape:\n                        resized_arrays.append(arr.astype(np.float32))\n                    else:\n                        zoom_factors = (most_common_shape[0] / arr.shape[0], \n                                      most_common_shape[1] / arr.shape[1])\n                        resized_arr = ndimage.zoom(arr, zoom_factors, order=1, prefilter=False)\n                        resized_arrays.append(resized_arr.astype(np.float32))\n                volume = np.stack(resized_arrays, axis=0).astype(np.float32)\n            \n            # Log successful loads (first few only)\n            if self.stats['total_loaded'] <= 10:\n                print(f\"✅ Loaded: {volume.shape} from {len(pixel_arrays)} slices\")\n            \n            # Apply rescale if available\n            if metadata['rescale_slope'] != 1.0 or metadata['rescale_intercept'] != 0.0:\n                volume = volume * metadata['rescale_slope'] + metadata['rescale_intercept']\n\n            self.stats['successful_loads'] += 1\n            return volume, metadata\n            \n        except Exception as e:\n            print(f\"Error loading {series_path}: {e}\")\n            self.stats['shape_errors'] += 1\n            return np.zeros(self.target_size, dtype=np.float32), {}\n\n    def print_stats(self):\n        \"\"\"Print loading statistics\"\"\"\n        total = self.stats['total_loaded']\n        successful = self.stats['successful_loads']\n        empty = self.stats['empty_volumes']\n        shape_errors = self.stats['shape_errors']\n        \n        if total > 0:\n            success_rate = (successful / total) * 100\n            print(f\"\\n📊 === DICOM Loading Stats ===\")\n            print(f\"✅ Successful loads: {successful}/{total} ({success_rate:.1f}%)\")\n            print(f\"❌ Corrupted/empty: {empty} ({empty/total*100:.1f}%)\")\n            print(f\"⚠️  Shape errors: {shape_errors} ({shape_errors/total*100:.1f}%)\")\n            \n            if success_rate < 70:\n                print(f\"🚨 SUCCESS RATE TOO LOW ({success_rate:.1f}%)!\")\n                print(f\"   Most volumes are corrupted - check dataset quality!\")\n            elif success_rate < 85:\n                print(f\"⚠️  Moderate success rate ({success_rate:.1f}%) - some data quality issues\")\n            else:\n                print(f\"✅ Good success rate ({success_rate:.1f}%)\")\n            print(f\"===============================\")\n\n    def preprocess_volume(self, volume: np.ndarray, metadata: Dict) -> np.ndarray:\n        \"\"\"Enhanced preprocessing with modality-specific handling\"\"\"\n        if volume.ndim != 3 or volume.size == 0:\n            print(f\"Warning: Received a non-3D volume. Returning empty target volume.\")\n            return np.zeros(self.target_size, dtype=np.float32)\n        \n        # Default windowing\n        p1, p99 = np.percentile(volume, [5, 95])\n        volume = np.clip(volume, p1, p99)\n        \n        # Normalization\n        vol_min, vol_max = volume.min(), volume.max()\n        if vol_max > vol_min:\n            volume = (volume - vol_min) / (vol_max - vol_min)\n        \n        # Resize to target size\n        if volume.shape != self.target_size:\n            zoom_factors = [self.target_size[i] / volume.shape[i] for i in range(3)]\n            volume = ndimage.zoom(volume, zoom_factors, order=1, prefilter=False)\n        \n        return volume.astype(np.float32)\n\n    def load_localization_mask(self, series_id: str, localizer_df: pd.DataFrame) -> np.ndarray:\n        return np.zeros(self.target_size, dtype=np.float32)\n\n\n# ====================================================\n# ENHANCED DATASET\n# ====================================================\n\nclass EnhancedAneurysmDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, localizer_df: pd.DataFrame, \n                 series_dir: str, processor: AdvancedDICOMProcessor, \n                 mode: str = 'train', fold: int = None):\n        self.df = df\n        self.localizer_df = localizer_df\n        self.series_dir = series_dir\n        self.processor = processor\n        self.mode = mode\n        self.fold = fold\n        \n        # Data augmentation for training\n        if mode == 'train':\n            self.transform = Compose([\n                # RandRotate90d(keys=['volume'], prob=0.3, spatial_axes=(0, 1)),\n                # RandFlipd(keys=['volume'], prob=0.3, spatial_axis=0),\n                # RandFlipd(keys=['volume'], prob=0.3, spatial_axis=1),\n                # RandFlipd(keys=['volume'], prob=0.3, spatial_axis=2),\n                # RandAffined(keys=['volume'], prob=0.3, rotate_range=0.1, scale_range=0.1),\n                # RandGaussianNoised(keys=['volume'], prob=0.3, std=0.05),\n                # RandAdjustContrastd(keys=['volume'], prob=0.3, gamma=(0.8, 1.2)),\n                RandFlipd(keys=['volume'], prob=0.5, spatial_axis=0),  # Only basic flips\n                RandFlipd(keys=['volume'], prob=0.5, spatial_axis=1),\n                ToTensord(keys=['volume'])\n            ])\n        else:\n            self.transform = Compose([ToTensord(keys=['volume'])])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        load_start = time.time()\n        row = self.df.iloc[idx]\n        series_id = row[Config.ID_COL]\n        series_path = os.path.join(self.series_dir, series_id)\n        \n        # Load and process volume\n        volume, metadata = self.processor.load_dicom_series(series_path)\n        dicom_time = time.time() - load_start\n        preprocess_start = time.time()\n        volume = self.processor.preprocess_volume(volume, metadata)\n        \n        # Create localization mask (for auxiliary loss)\n        loc_mask = self.processor.load_localization_mask(series_id, self.localizer_df)\n        \n        # Get labels\n        labels = row[Config.LABEL_COLS].values.astype(np.float32)\n        \n        # Apply transforms\n        data_dict = {'volume': volume}\n        if self.transform:\n            data_dict = self.transform(data_dict)\n        \n        volume_tensor = data_dict['volume'].unsqueeze(0)  # Add channel dimension\n        loc_mask_tensor = torch.from_numpy(loc_mask).unsqueeze(0)\n        labels_tensor = torch.from_numpy(labels)\n        \n        # Add metadata features\n        modality_encoding = self._encode_modality(metadata.get('modality', 'UNKNOWN'))\n        metadata_tensor = torch.tensor(modality_encoding, dtype=torch.float32)\n        \n        preprocess_time = time.time() - preprocess_start\n        # Print timing for first few samples to debug\n        if idx < 5:\n            print(f\"Sample {idx}: DICOM load: {dicom_time:.2f}s, Preprocess: {preprocess_time:.2f}s\")\n        \n        return {\n            'volume': volume_tensor,\n            'localization_mask': loc_mask_tensor,\n            'labels': labels_tensor,\n            'metadata': metadata_tensor,\n            'series_id': series_id\n        }\n    \n    def _encode_modality(self, modality: str) -> List[float]:\n        \"\"\"One-hot encode modality\"\"\"\n        modalities = ['CTA', 'MRA', 'MRI', 'MR', 'UNKNOWN']\n        encoding = [0.0] * len(modalities)\n        if modality in modalities:\n            encoding[modalities.index(modality)] = 1.0\n        else:\n            encoding[-1] = 1.0  # UNKNOWN\n        return encoding\n\n\n# ====================================================\n# ADVANCED MODEL ARCHITECTURE\n# ====================================================\n\n#class MultiModalAneurysmNet(nn.Module):\nclass SimplifiedAneurysmNet(nn.Module):\n    def __init__(self, num_classes: int = len(Config.LABEL_COLS), \n                 spatial_dims: int = 3, in_channels: int = 1, \n        #          features: Tuple = (32, 64, 128, 256, 512, 1024)):\n        # super(MultiModalAneurysmNet, self).__init__()\n                 features: Tuple = (16, 32, 64, 128, 256, 51)):\n        super(SimplifiedAneurysmNet, self).__init__()\n        \n        # Main 3D U-Net backbone\n        self.backbone = BasicUNet(\n            spatial_dims=spatial_dims,\n            in_channels=in_channels,\n            out_channels=features[0],\n            features=features,\n            dropout=0.1 #Reduced dropout \n        )\n        \n        # Global average pooling\n        self.global_pool = nn.AdaptiveAvgPool3d(1)\n        \n        # # Metadata processing\n        # self.metadata_mlp = nn.Sequential(\n        #     nn.Linear(5, 32),  # 5 modality categories\n        #     nn.ReLU(),\n        #     nn.Dropout(0.3),\n        #     nn.Linear(32, 64),\n        #     nn.ReLU()\n        # )\n        \n        # Classification head\n        #feature_size = features[0] + 64  # backbone features + metadata features\n        self.classifier = nn.Sequential(\n            # nn.Linear(feature_size, 512),\n            # nn.ReLU(),\n            # nn.Dropout(0.5),\n            # nn.Linear(512, 256),\n            nn.Linear(features[0], 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, volume, metadata=None):\n        # Extract features from 3D volume\n        features = self.backbone(volume)\n        # Global features for classification\n        global_features = self.global_pool(features).flatten(1)\n        classification_logits = self.classifier(global_features)\n        return classification_logits, None\n\n# ====================================================\n# WEIGHTED LOSS FUNCTION\n# ====================================================\n\nclass WeightedMultiLabelLoss(nn.Module):\n    def __init__(self, pos_weights=None, aneurysm_weight=13.0):\n        super().__init__()\n        self.pos_weights = pos_weights\n        self.aneurysm_weight = aneurysm_weight\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n        \n    def forward(self, logits, targets):\n        bce_loss = self.bce(logits, targets)\n        \n        # Apply position weights if provided\n        if self.pos_weights is not None:\n            bce_loss = bce_loss * self.pos_weights.to(logits.device)\n        \n        # Weight the \"Aneurysm Present\" class higher (last column)\n        weights = torch.ones_like(bce_loss)\n        weights[:, -1] = self.aneurysm_weight\n        \n        weighted_loss = bce_loss * weights\n        return weighted_loss.mean()\n\n\n# ====================================================\n# TRAINING FUNCTIONS\n# ====================================================\n\ndef compute_weighted_auc(y_true, y_pred):\n    \"\"\"Compute weighted AUC matching competition metric\"\"\"\n    aucs = []\n    weights = []\n    \n    for i in range(len(Config.LABEL_COLS)):\n        try:\n            auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n            aucs.append(auc)\n            # Weight \"Aneurysm Present\" (last column) higher\n            weights.append(13.0 if i == len(Config.LABEL_COLS) - 1 else 1.0)\n        except ValueError:\n            aucs.append(0.5)  # Default for no positive cases\n            weights.append(13.0 if i == len(Config.LABEL_COLS) - 1 else 1.0)\n    \n    weighted_auc = sum(a * w for a, w in zip(aucs, weights)) / sum(weights)\n    return weighted_auc, aucs\n\ndef train_epoch(model, train_loader, optimizer, criterion, scaler, device):\n    model.train()\n    total_loss = 0\n    num_batches = 0\n    skipped_batches = 0\n    \n    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch\")):\n        start_time = time.time()\n        volume = batch['volume'].to(device)\n        metadata = batch['metadata'].to(device)\n        labels = batch['labels'].to(device)\n        loc_mask = batch['localization_mask'].to(device)\n\n        # CRITICAL FIX: Skip batches with zero-filled volumes\n        if torch.all(volume == 0) or torch.var(volume) < 1e-6:\n            skipped_batches += 1\n            if batch_idx < 5:  # Log first few skips\n                print(f\"⚠️  Skipping batch {batch_idx}: zero-filled or low-variance volume\")\n            continue\n        \n        with autocast(device_type=device.type, enabled=Config.MIXED_PRECISION):\n            class_logits, _ = model(volume, metadata)\n            total_loss_batch = criterion(class_logits, labels)\n        \n        # Gradient accumulation\n        scaled_loss = total_loss_batch / Config.GRADIENT_ACCUMULATION\n        scaler.scale(scaled_loss).backward()\n        \n        if (batch_idx + 1) % Config.GRADIENT_ACCUMULATION == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n        \n        total_loss += total_loss_batch.item()\n        num_batches += 1\n\n        # Print timing for first few batches to identify bottlenecks\n        if batch_idx < 5:\n            batch_time = time.time() - start_time\n            print(f\"Batch {batch_idx}: {batch_time:.2f}s\")\n    \n    if skipped_batches > 0:\n        print(f\"⚠️  Skipped {skipped_batches} batches with corrupted/zero volumes\")\n    \n    return total_loss / max(num_batches, 1) if num_batches > 0 else float('inf')\n\ndef validate_epoch(model, val_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    num_batches = 0\n    skipped_batches = 0\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Validating\")):\n            volume = batch['volume'].to(device)\n            metadata = batch['metadata'].to(device)\n            labels = batch['labels'].to(device)\n            \n            # CRITICAL FIX: Skip batches with zero-filled volumes\n            if torch.all(volume == 0) or torch.var(volume) < 1e-6:\n                skipped_batches += 1\n                if batch_idx < 3:  # Log first few skips\n                    print(f\"⚠️  Skipping validation batch {batch_idx}: zero-filled or low-variance volume\")\n                continue\n            \n            with autocast(device_type=device.type, enabled=Config.MIXED_PRECISION):\n                class_logits, _ = model(volume, metadata)\n                loss = criterion(class_logits, labels)\n\n                # DEBUG: Check for extreme loss values\n            if total_loss_batch.item() > 1e7:\n                print(f\"🚨 Warning: Extreme loss in batch {batch_idx}: {total_loss_batch.item():.2e}\")\n                print(f\"   Labels: {labels[0].cpu().numpy()}\")  # Print first sample's labels\n            \n            total_loss += loss.item()\n            num_batches += 1\n            \n            # Collect predictions for AUC calculation\n            probs = torch.sigmoid(class_logits).cpu().numpy()\n            all_preds.append(probs)\n            all_labels.append(labels.cpu().numpy())\n\n    if skipped_batches > 0:\n        print(f\"⚠️  Skipped {skipped_batches} validation batches with corrupted/zero volumes\")\n    \n    if len(all_preds) == 0:\n        print(\"🚨 WARNING: No valid validation batches - all were corrupted!\")\n        return float('inf'), 0.5, [0.5] * len(Config.LABEL_COLS)\n    \n    all_preds = np.vstack(all_preds)\n    all_labels = np.vstack(all_labels)\n    \n    weighted_auc, individual_aucs = compute_weighted_auc(all_labels, all_preds)\n    \n    return total_loss / max(num_batches, 1), weighted_auc, individual_aucs\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T05:39:59.940853Z","iopub.execute_input":"2025-08-05T05:39:59.94157Z","iopub.status.idle":"2025-08-05T05:39:59.984045Z","shell.execute_reply.started":"2025-08-05T05:39:59.94154Z","shell.execute_reply":"2025-08-05T05:39:59.983149Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ====================================================\n# MAIN TRAINING EXECUTION\n# ====================================================\n\ndef main():\n    print(f\"Using device: {Config.DEVICE}\")\n    print(f\"Mixed precision: {Config.MIXED_PRECISION}\")\n    \n    # Load data\n    train_df = pd.read_csv(Config.TRAIN_CSV_PATH)\n    localizer_df = pd.read_csv(Config.LOCALIZER_CSV_PATH)\n\n    print(\"---!!! RUNNING IN DEBUG MODE ON A SMALL SUBSET !!!---\")\n    #print(f\"Training samples: {len(train_df)}\")\n    train_df = train_df.head(100)  # Limit to 100 samples for speed testing\n    print(f\"Training samples: {len(train_df)} (limited for speed testing)\")\n    print(f\"Positive aneurysm cases: {train_df['Aneurysm Present'].sum()}\")\n\n    # Calculate class weights for imbalanced data\n    pos_counts = train_df[Config.LABEL_COLS].sum()\n    neg_counts = len(train_df) - pos_counts\n    pos_weights = neg_counts / (pos_counts + 1e-8)  # Add small epsilon\n    pos_weights = np.minimum(pos_weights, 100.0)  # Cap weights at 100\n    pos_weights = torch.tensor(pos_weights, dtype=torch.float32)  # Convert to tensor\n    print(\"Class weights (capped at 100):\", pos_weights)\n\n    # DATASET INTEGRITY CHECK\n    print(\"\\n🔍 Checking dataset integrity...\")\n    valid_series = 0\n    invalid_series = []\n    \n    for series_id in train_df[Config.ID_COL]:\n        series_path = os.path.join(Config.SERIES_DIR, series_id)\n        if os.path.exists(series_path):\n            dicom_files = [f for f in os.listdir(series_path) if f.endswith('.dcm')]\n            if dicom_files:\n                valid_series += 1\n            else:\n                invalid_series.append(f\"No DICOMs: {series_id}\")\n        else:\n            invalid_series.append(f\"Missing path: {series_id}\")\n    \n    success_rate = valid_series / len(train_df)\n    print(f\"📊 Dataset check: {valid_series}/{len(train_df)} series valid ({success_rate:.1%})\")\n    \n    if success_rate < 0.7:\n        print(f\"🚨 WARNING: Only {success_rate:.1%} of series are accessible!\")\n        print(\"First few issues:\")\n        for issue in invalid_series[:5]:\n            print(f\"  - {issue}\")\n        print(f\"⚠️  Training will proceed, but expect many corrupted DICOM errors\")\n    else:\n        print(f\"✅ Good dataset integrity ({success_rate:.1%} valid)\")\n        \n    print()\n\n    # Initialize corrupted series log\n    with open('corrupted_series.txt', 'w') as f:\n        f.write(\"# Corrupted series log - check this file to identify problematic DICOM series\\n\")\n    print(\"📝 Initialized 'corrupted_series.txt' for logging corrupted series\")\n    \n    # Create stratified group k-fold split\n    # Use patient-level grouping to prevent data leakage\n    train_df['patient_group'] = train_df['PatientID'] if 'PatientID' in train_df.columns else range(len(train_df))\n    \n    skf = StratifiedGroupKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=42)\n    train_df['fold'] = -1\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(\n        train_df, train_df['Aneurysm Present'], groups=train_df['patient_group']\n    )):\n        train_df.loc[val_idx, 'fold'] = fold\n    \n    # Initialize processor\n    processor = AdvancedDICOMProcessor()\n    \n    # Train models for each fold\n    fold_scores = []\n    \n    for fold in range(Config.N_FOLDS):\n        print(f\"\\n{'='*50}\")\n        print(f\"FOLD {fold + 1}/{Config.N_FOLDS}\")\n        print(f\"{'='*50}\")\n        \n        # Split data\n        train_fold_df = train_df[train_df['fold'] != fold].reset_index(drop=True)\n        val_fold_df = train_df[train_df['fold'] == fold].reset_index(drop=True)\n        \n        print(f\"Train: {len(train_fold_df)}, Validation: {len(val_fold_df)}\")\n        \n        # Create datasets\n        train_dataset = EnhancedAneurysmDataset(\n            train_fold_df, localizer_df, Config.SERIES_DIR, processor, mode='train', fold=fold\n        )\n        val_dataset = EnhancedAneurysmDataset(\n            val_fold_df, localizer_df, Config.SERIES_DIR, processor, mode='val', fold=fold\n        )\n        \n        # Create data loaders\n        train_loader = DataLoader(\n            train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, \n            num_workers=4, pin_memory=True, drop_last=True\n        )\n        val_loader = DataLoader(\n            val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, \n            num_workers=4, pin_memory=True\n        )\n        \n        # Initialize model\n        model = SimplifiedAneurysmNet().to(Config.DEVICE)\n        criterion = WeightedMultiLabelLoss(pos_weights=pos_weights)\n        \n        # Optimizer with different learning rates for different parts\n        # optimizer = optim.AdamW([\n        #     {'params': model.backbone.parameters(), 'lr': Config.LEARNING_RATE},\n        #     {'params': model.classifier.parameters(), 'lr': Config.LEARNING_RATE * 2},\n        #     {'params': model.metadata_mlp.parameters(), 'lr': Config.LEARNING_RATE * 2}\n        # ], weight_decay=Config.WEIGHT_DECAY)\n        optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)\n        \n        # Learning rate scheduler\n        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            optimizer, T_0=10, T_mult=2, eta_min=1e-6\n        )\n        \n        scaler = GradScaler(enabled=Config.MIXED_PRECISION)\n        \n        # Training loop\n        best_auc = 0\n        patience = 10\n        patience_counter = 0\n        \n        for epoch in range(Config.EPOCHS):\n            # Train\n            train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, Config.DEVICE)\n            \n            # Validate\n            val_loss, val_auc, individual_aucs = validate_epoch(model, val_loader, criterion, Config.DEVICE)\n            \n            # Step scheduler\n            scheduler.step()\n            \n            print(f\"Epoch {epoch+1:3d} | \"\n                  f\"Train Loss: {train_loss:.4f} | \"\n                  f\"Val Loss: {val_loss:.4f} | \"\n                  f\"Val AUC: {val_auc:.4f}\")\n\n            processor.print_stats() # Print DICOM loading stats after each epoch\n\n            # Check corrupted series log\n            try:\n                with open('corrupted_series.txt', 'r') as f:\n                    lines = f.readlines()\n                    corrupted_count = len([l for l in lines if not l.startswith('#')])\n                    if corrupted_count > 0:\n                        print(f\"📄 Corrupted series logged: {corrupted_count} entries in 'corrupted_series.txt'\")\n                    else:\n                        print(f\"✅ No corrupted series logged this epoch\")\n            except FileNotFoundError:\n                print(f\"📄 No corrupted series log file found\")\n\n            # SANITY CHECK: Stop if data loading is fundamentally broken\n            if processor.stats['total_loaded'] > 20:  # Only check after some attempts\n                success_rate = processor.stats['successful_loads'] / processor.stats['total_loaded']\n                if success_rate < 0.5:  # Less than 50% success rate\n                    print(f\"\\n🚨 STOPPING TRAINING: Data loading success rate is {success_rate:.1%}\")\n                    print(\"Fix the DICOM loading issues before continuing training!\")\n                    print(\"Most volumes are returning empty - this is a waste of time!\")\n                    break\n            \n            # Save best model\n            if val_auc > best_auc:\n                best_auc = val_auc\n                patience_counter = 0\n                torch.save({\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                    'val_auc': val_auc,\n                    'epoch': epoch,\n                    'fold': fold,\n                    'individual_aucs': individual_aucs\n                }, f'best_model_fold_{fold}.pth')\n            else:\n                patience_counter += 1\n                \n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n                \n            # Memory cleanup\n            if epoch % 5 == 0:\n                gc.collect()\n                torch.cuda.empty_cache()\n        \n        fold_scores.append(best_auc)\n        print(f\"Fold {fold + 1} best AUC: {best_auc:.4f}\")\n    \n    # Final results\n    mean_cv_score = np.mean(fold_scores)\n    std_cv_score = np.std(fold_scores)\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"CROSS-VALIDATION RESULTS\")\n    print(f\"{'='*50}\")\n    print(f\"Mean CV AUC: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")\n    print(f\"Individual fold scores: {fold_scores}\")\n    \n    # Save training summary\n    results = {\n        'cv_scores': fold_scores,\n        'mean_cv_score': mean_cv_score,\n        'std_cv_score': std_cv_score,\n        'config': vars(Config())\n    }\n    \n    with open('training_results.json', 'w') as f:\n        json.dump(results, f, indent=2, default=str)\n    \n    print(\"Training complete! Models saved as 'best_model_fold_X.pth'\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T05:40:25.423877Z","iopub.execute_input":"2025-08-05T05:40:25.4242Z","iopub.status.idle":"2025-08-05T05:40:30.475196Z","shell.execute_reply.started":"2025-08-05T05:40:25.424174Z","shell.execute_reply":"2025-08-05T05:40:30.474217Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nMixed precision: True\n---!!! RUNNING IN DEBUG MODE ON A SMALL SUBSET !!!---\nTraining samples: 100 (limited for speed testing)\nPositive aneurysm cases: 48\n\n🔍 Checking dataset integrity...\n📊 Dataset check: 100/100 series valid (100.0%)\n✅ Good dataset integrity (100.0% valid)\n\n📝 Initialized 'corrupted_series.txt' for logging corrupted series\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1527335754.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/1527335754.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Cap extreme class weights to prevent loss explosion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mpos_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Cap weights at 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mpos_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Class weights (capped at 100):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'pos_weights' where it is not associated with a value"],"ename":"UnboundLocalError","evalue":"cannot access local variable 'pos_weights' where it is not associated with a value","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}