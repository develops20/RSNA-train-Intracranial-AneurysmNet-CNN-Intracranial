{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":99552,"databundleVersionId":13190393,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/02-aneurysmnet-cnn-intracranial-nb153?scriptVersionId=254264443\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install monai\n\n# ====================================================\n# RSNA INTRACRANIAL ANEURYSM DETECTION - TRAINING PIPELINE\n# ====================================================\n\nimport os\nimport gc\nimport warnings\nimport json\nimport time\nimport numpy as np\nimport pandas as pd\nfrom typing import Tuple, Dict, List\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport albumentations as A\n\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.amp import autocast, GradScaler\n\nimport pydicom\nfrom scipy import ndimage\nimport nibabel as nib\nfrom monai.transforms import (\n    Compose, RandRotate90d, RandFlipd, RandAffined,\n    RandGaussianNoised, RandAdjustContrastd, ToTensord\n)\nfrom monai.networks.nets import BasicUNet\nfrom monai.losses import DiceCELoss, FocalLoss\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T03:42:13.647445Z","iopub.execute_input":"2025-08-05T03:42:13.648254Z","iopub.status.idle":"2025-08-05T03:42:46.879107Z","shell.execute_reply.started":"2025-08-05T03:42:13.648221Z","shell.execute_reply":"2025-08-05T03:42:46.878533Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: monai in /usr/local/lib/python3.11/dist-packages (1.5.0)\nRequirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai) (1.26.4)\nRequirement already satisfied: torch<2.7.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from monai) (2.6.0+cu124)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7.0,>=2.4.1->monai) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7.0,>=2.4.1->monai) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.24->monai) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"2025-08-05 03:42:35.471244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754365355.662727      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754365355.715688      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ====================================================\n# CONFIGURATION\n# ====================================================\n\nclass Config:\n    # Paths\n    TRAIN_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv'\n    LOCALIZER_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train_localizers.csv'\n    SERIES_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/'\n    SEGMENTATION_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/segmentations/'\n    \n    # Model parameters\n    TARGET_SIZE = (32, 64, 64)  # Increased resolution\n    EPOCHS = 10\n    BATCH_SIZE = 8  # Reduced due to larger input size\n    LEARNING_RATE = 1e-3\n    WEIGHT_DECAY = 1e-4\n    N_FOLDS = 3\n    \n    # Training parameters\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MIXED_PRECISION = True\n    GRADIENT_ACCUMULATION = 4\n    \n    # Competition constants\n    ID_COL = 'SeriesInstanceUID'\n    LABEL_COLS = [\n        'Left Infraclinoid Internal Carotid Artery', 'Right Infraclinoid Internal Carotid Artery',\n        'Left Supraclinoid Internal Carotid Artery', 'Right Supraclinoid Internal Carotid Artery',\n        'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery', 'Anterior Communicating Artery',\n        'Left Anterior Cerebral Artery', 'Right Anterior Cerebral Artery',\n        'Left Posterior Communicating Artery', 'Right Posterior Communicating Artery',\n        'Basilar Tip', 'Other Posterior Circulation', 'Aneurysm Present',\n    ]\n    \n    # Class weights for imbalanced data\n    ANEURYSM_PRESENT_WEIGHT = 13.0  # Match evaluation metric weighting","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T03:45:20.038834Z","iopub.execute_input":"2025-08-05T03:45:20.040148Z","iopub.status.idle":"2025-08-05T03:45:20.045761Z","shell.execute_reply.started":"2025-08-05T03:45:20.040115Z","shell.execute_reply":"2025-08-05T03:45:20.044909Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ====================================================\n# ENHANCED DATA PREPROCESSING\n# ====================================================\n\nclass AdvancedDICOMProcessor:\n    def __init__(self, target_size: Tuple[int, int, int] = Config.TARGET_SIZE):\n        self.target_size = target_size\n        self.stats = {\n            'total_loaded': 0,\n            'successful_loads': 0,\n            'shape_errors': 0,\n            'empty_volumes': 0,\n            'preprocessing_errors': 0\n        }\n        \n    def load_dicom_series(self, series_path: str) -> Tuple[np.ndarray, Dict]:\n        \"\"\"Load DICOM series with FIXED shape handling\"\"\"\n        self.stats['total_loaded'] += 1\n        try:\n            dicom_files = [os.path.join(series_path, f) for f in os.listdir(series_path) if f.endswith('.dcm')]\n            if not dicom_files:\n                print(f\"No DICOM files found in {series_path}\")\n                return np.zeros(self.target_size, dtype=np.float32), {}\n                \n            dicoms = []\n            \n            for f in dicom_files:\n                ds = pydicom.dcmread(f, force=True)\n                dicoms.append(ds)\n                \n            # Extract metadata from first DICOM\n            first_ds = dicoms[0]\n            metadata = {\n                'modality': getattr(first_ds, 'Modality', 'UNKNOWN'),\n                'spacing': getattr(first_ds, 'PixelSpacing', [1.0, 1.0]),\n                'slice_thickness': getattr(first_ds, 'SliceThickness', 1.0),\n                'rescale_slope': getattr(first_ds, 'RescaleSlope', 1.0),\n                'rescale_intercept': getattr(first_ds, 'RescaleIntercept', 0.0),\n            }\n            \n            # Sort by instance number \n            dicoms.sort(key=lambda x: int(getattr(x, 'InstanceNumber', 0)))\n\n            \n            # DEBUG: Let's see what we're actually getting (only for first few volumes)\n            pixel_arrays = []\n            shapes = []\n            debug_this_volume = self.stats['total_loaded'] <= 3  # Only debug first 3 volumes\n            \n            for i, d in enumerate(dicoms):\n                if hasattr(d, 'pixel_array'):\n                    arr = d.pixel_array\n                    if debug_this_volume:\n                        print(f\"  Slice {i}: shape={arr.shape}, dtype={arr.dtype}, ndim={arr.ndim}\")\n                    \n                    # Only accept 2D arrays\n                    if arr.ndim == 2:\n                        pixel_arrays.append(arr)\n                        shapes.append(arr.shape)\n                    else:\n                        if debug_this_volume:\n                            print(f\"  SKIPPING slice {i}: not 2D (ndim={arr.ndim})\")\n                else:\n                    if debug_this_volume:\n                        print(f\"  SKIPPING slice {i}: no pixel_array attribute\")\n            \n            if len(pixel_arrays) == 0:\n                print(f\"CRITICAL: No valid 2D pixel arrays found in {series_path}\")\n                self.stats['shape_errors'] += 1\n                return np.zeros(self.target_size, dtype=np.float32), metadata\n\n            if debug_this_volume:\n                print(f\"  Found {len(pixel_arrays)} valid slices, unique shapes: {set(shapes)}\")\n            \n            # Check if all shapes are the same\n            unique_shapes = list(set(shapes))\n            if len(unique_shapes) == 1:\n                if debug_this_volume:\n                    print(f\"  All slices have same shape {unique_shapes[0]} - using original stacking\")\n                volume = np.stack(pixel_arrays, axis=0).astype(np.float32)\n            else:\n                if debug_this_volume:\n                    print(f\"  Multiple shapes found: {unique_shapes}\")\n                # Find most common shape\n                most_common_shape = Counter(shapes).most_common(1)[0][0]\n                if debug_this_volume:\n                    print(f\"  Resizing all to most common shape: {most_common_shape}\")\n                \n                resized_arrays = []\n                for i, arr in enumerate(pixel_arrays):\n                    if arr.shape == most_common_shape:\n                        resized_arrays.append(arr.astype(np.float32))\n                    else:\n                        if debug_this_volume:\n                            print(f\"    Resizing slice {i} from {arr.shape} to {most_common_shape}\")\n                        zoom_factors = (most_common_shape[0] / arr.shape[0], \n                                      most_common_shape[1] / arr.shape[1])\n                        resized_arr = ndimage.zoom(arr, zoom_factors, order=1, prefilter=False)\n                        resized_arrays.append(resized_arr.astype(np.float32))\n                \n                volume = np.stack(resized_arrays, axis=0).astype(np.float32)\n\n            if debug_this_volume:\n                print(f\"  ✅ Final volume shape: {volume.shape}, dtype: {volume.dtype}\")\n            \n            # Apply rescale if available\n            if metadata['rescale_slope'] != 1.0 or metadata['rescale_intercept'] != 0.0:\n                volume = volume * metadata['rescale_slope'] + metadata['rescale_intercept']\n\n            self.stats['successful_loads'] += 1\n            return volume, metadata\n            \n        except Exception as e:\n            print(f\"Error loading {series_path}: {e}\")\n            self.stats['shape_errors'] += 1\n            return np.zeros(self.target_size, dtype=np.float32), {}\n\n    def print_stats(self):\n        \"\"\"Print loading statistics\"\"\"\n        total = self.stats['total_loaded']\n        successful = self.stats['successful_loads']\n        if total > 0:\n            success_rate = (successful / total) * 100\n            print(f\"\\n=== DICOM Loading Stats ===\")\n            print(f\"Total attempts: {total}\")\n            print(f\"Successful loads: {successful} ({success_rate:.1f}%)\")\n            print(f\"Shape errors: {self.stats['shape_errors']}\")\n            print(f\"Empty volumes: {self.stats['empty_volumes']}\")\n            print(f\"Preprocessing errors: {self.stats['preprocessing_errors']}\")\n            print(f\"===========================\")\n\n    def preprocess_volume(self, volume: np.ndarray, metadata: Dict) -> np.ndarray:\n        \"\"\"Enhanced preprocessing with modality-specific handling\"\"\"\n        if volume.ndim != 3 or volume.size == 0:\n            print(f\"Warning: Received a non-3D volume. Returning empty target volume.\")\n            return np.zeros(self.target_size, dtype=np.float32)\n        \n        # Default windowing\n        p1, p99 = np.percentile(volume, [5, 95])\n        volume = np.clip(volume, p1, p99)\n        \n        # Normalization\n        vol_min, vol_max = volume.min(), volume.max()\n        if vol_max > vol_min:\n            volume = (volume - vol_min) / (vol_max - vol_min)\n        \n        # Resize to target size\n        if volume.shape != self.target_size:\n            zoom_factors = [self.target_size[i] / volume.shape[i] for i in range(3)]\n            volume = ndimage.zoom(volume, zoom_factors, order=1, prefilter=False)\n        \n        return volume.astype(np.float32)\n\n    def load_localization_mask(self, series_id: str, localizer_df: pd.DataFrame) -> np.ndarray:\n        return np.zeros(self.target_size, dtype=np.float32)\n\n\n# ====================================================\n# ENHANCED DATASET\n# ====================================================\n\nclass EnhancedAneurysmDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, localizer_df: pd.DataFrame, \n                 series_dir: str, processor: AdvancedDICOMProcessor, \n                 mode: str = 'train', fold: int = None):\n        self.df = df\n        self.localizer_df = localizer_df\n        self.series_dir = series_dir\n        self.processor = processor\n        self.mode = mode\n        self.fold = fold\n        \n        # Data augmentation for training\n        if mode == 'train':\n            self.transform = Compose([\n                # RandRotate90d(keys=['volume'], prob=0.3, spatial_axes=(0, 1)),\n                # RandFlipd(keys=['volume'], prob=0.3, spatial_axis=0),\n                # RandFlipd(keys=['volume'], prob=0.3, spatial_axis=1),\n                # RandFlipd(keys=['volume'], prob=0.3, spatial_axis=2),\n                # RandAffined(keys=['volume'], prob=0.3, rotate_range=0.1, scale_range=0.1),\n                # RandGaussianNoised(keys=['volume'], prob=0.3, std=0.05),\n                # RandAdjustContrastd(keys=['volume'], prob=0.3, gamma=(0.8, 1.2)),\n                RandFlipd(keys=['volume'], prob=0.5, spatial_axis=0),  # Only basic flips\n                RandFlipd(keys=['volume'], prob=0.5, spatial_axis=1),\n                ToTensord(keys=['volume'])\n            ])\n        else:\n            self.transform = Compose([ToTensord(keys=['volume'])])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        load_start = time.time()\n        row = self.df.iloc[idx]\n        series_id = row[Config.ID_COL]\n        series_path = os.path.join(self.series_dir, series_id)\n        \n        # Load and process volume\n        volume, metadata = self.processor.load_dicom_series(series_path)\n        dicom_time = time.time() - load_start\n        preprocess_start = time.time()\n        volume = self.processor.preprocess_volume(volume, metadata)\n        \n        # Create localization mask (for auxiliary loss)\n        loc_mask = self.processor.load_localization_mask(series_id, self.localizer_df)\n        \n        # Get labels\n        labels = row[Config.LABEL_COLS].values.astype(np.float32)\n        \n        # Apply transforms\n        data_dict = {'volume': volume}\n        if self.transform:\n            data_dict = self.transform(data_dict)\n        \n        volume_tensor = data_dict['volume'].unsqueeze(0)  # Add channel dimension\n        loc_mask_tensor = torch.from_numpy(loc_mask).unsqueeze(0)\n        labels_tensor = torch.from_numpy(labels)\n        \n        # Add metadata features\n        modality_encoding = self._encode_modality(metadata.get('modality', 'UNKNOWN'))\n        metadata_tensor = torch.tensor(modality_encoding, dtype=torch.float32)\n        \n        preprocess_time = time.time() - preprocess_start\n        # Print timing for first few samples to debug\n        if idx < 5:\n            print(f\"Sample {idx}: DICOM load: {dicom_time:.2f}s, Preprocess: {preprocess_time:.2f}s\")\n        \n        return {\n            'volume': volume_tensor,\n            'localization_mask': loc_mask_tensor,\n            'labels': labels_tensor,\n            'metadata': metadata_tensor,\n            'series_id': series_id\n        }\n    \n    def _encode_modality(self, modality: str) -> List[float]:\n        \"\"\"One-hot encode modality\"\"\"\n        modalities = ['CTA', 'MRA', 'MRI', 'MR', 'UNKNOWN']\n        encoding = [0.0] * len(modalities)\n        if modality in modalities:\n            encoding[modalities.index(modality)] = 1.0\n        else:\n            encoding[-1] = 1.0  # UNKNOWN\n        return encoding\n\n\n# ====================================================\n# ADVANCED MODEL ARCHITECTURE\n# ====================================================\n\n#class MultiModalAneurysmNet(nn.Module):\nclass SimplifiedAneurysmNet(nn.Module):\n    def __init__(self, num_classes: int = len(Config.LABEL_COLS), \n                 spatial_dims: int = 3, in_channels: int = 1, \n        #          features: Tuple = (32, 64, 128, 256, 512, 1024)):\n        # super(MultiModalAneurysmNet, self).__init__()\n                 features: Tuple = (16, 32, 64, 128, 256, 51)):\n        super(SimplifiedAneurysmNet, self).__init__()\n        \n        # Main 3D U-Net backbone\n        self.backbone = BasicUNet(\n            spatial_dims=spatial_dims,\n            in_channels=in_channels,\n            out_channels=features[0],\n            features=features,\n            dropout=0.1 #Reduced dropout \n        )\n        \n        # Global average pooling\n        self.global_pool = nn.AdaptiveAvgPool3d(1)\n        \n        # # Metadata processing\n        # self.metadata_mlp = nn.Sequential(\n        #     nn.Linear(5, 32),  # 5 modality categories\n        #     nn.ReLU(),\n        #     nn.Dropout(0.3),\n        #     nn.Linear(32, 64),\n        #     nn.ReLU()\n        # )\n        \n        # Classification head\n        #feature_size = features[0] + 64  # backbone features + metadata features\n        self.classifier = nn.Sequential(\n            # nn.Linear(feature_size, 512),\n            # nn.ReLU(),\n            # nn.Dropout(0.5),\n            # nn.Linear(512, 256),\n            nn.Linear(features[0], 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, volume, metadata=None):\n        # Extract features from 3D volume\n        features = self.backbone(volume)\n        # Global features for classification\n        global_features = self.global_pool(features).flatten(1)\n        classification_logits = self.classifier(global_features)\n        return classification_logits, None\n\n# ====================================================\n# WEIGHTED LOSS FUNCTION\n# ====================================================\n\nclass WeightedMultiLabelLoss(nn.Module):\n    def __init__(self, pos_weights=None, aneurysm_weight=13.0):\n        super().__init__()\n        self.pos_weights = pos_weights\n        self.aneurysm_weight = aneurysm_weight\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n        \n    def forward(self, logits, targets):\n        bce_loss = self.bce(logits, targets)\n        \n        # Apply position weights if provided\n        if self.pos_weights is not None:\n            bce_loss = bce_loss * self.pos_weights.to(logits.device)\n        \n        # Weight the \"Aneurysm Present\" class higher (last column)\n        weights = torch.ones_like(bce_loss)\n        weights[:, -1] = self.aneurysm_weight\n        \n        weighted_loss = bce_loss * weights\n        return weighted_loss.mean()\n\n\n# ====================================================\n# TRAINING FUNCTIONS\n# ====================================================\n\ndef compute_weighted_auc(y_true, y_pred):\n    \"\"\"Compute weighted AUC matching competition metric\"\"\"\n    aucs = []\n    weights = []\n    \n    for i in range(len(Config.LABEL_COLS)):\n        try:\n            auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n            aucs.append(auc)\n            # Weight \"Aneurysm Present\" (last column) higher\n            weights.append(13.0 if i == len(Config.LABEL_COLS) - 1 else 1.0)\n        except ValueError:\n            aucs.append(0.5)  # Default for no positive cases\n            weights.append(13.0 if i == len(Config.LABEL_COLS) - 1 else 1.0)\n    \n    weighted_auc = sum(a * w for a, w in zip(aucs, weights)) / sum(weights)\n    return weighted_auc, aucs\n\ndef train_epoch(model, train_loader, optimizer, criterion, scaler, device):\n    model.train()\n    total_loss = 0\n    num_batches = 0\n    \n    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch\")):\n        start_time = time.time()\n        volume = batch['volume'].to(device)\n        metadata = batch['metadata'].to(device)\n        labels = batch['labels'].to(device)\n        loc_mask = batch['localization_mask'].to(device)\n        \n        with autocast(device_type=device.type, enabled=Config.MIXED_PRECISION):\n            class_logits, _ = model(volume, metadata)\n            total_loss_batch = criterion(class_logits, labels)\n        \n        # Gradient accumulation\n        scaled_loss = total_loss_batch / Config.GRADIENT_ACCUMULATION\n        scaler.scale(scaled_loss).backward()\n        \n        if (batch_idx + 1) % Config.GRADIENT_ACCUMULATION == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n        \n        total_loss += total_loss_batch.item()\n        num_batches += 1\n\n        # Print timing for first few batches to identify bottlenecks\n        if batch_idx < 5:\n            batch_time = time.time() - start_time\n            print(f\"Batch {batch_idx}: {batch_time:.2f}s\")\n    \n    return total_loss / num_batches\n\ndef validate_epoch(model, val_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            volume = batch['volume'].to(device)\n            metadata = batch['metadata'].to(device)\n            labels = batch['labels'].to(device)\n            \n            with autocast(device_type=device.type, enabled=Config.MIXED_PRECISION):\n                class_logits, _ = model(volume, metadata)\n                loss = criterion(class_logits, labels)\n            \n            total_loss += loss.item()\n            \n            # Collect predictions for AUC calculation\n            probs = torch.sigmoid(class_logits).cpu().numpy()\n            all_preds.append(probs)\n            all_labels.append(labels.cpu().numpy())\n    \n    all_preds = np.vstack(all_preds)\n    all_labels = np.vstack(all_labels)\n    \n    weighted_auc, individual_aucs = compute_weighted_auc(all_labels, all_preds)\n    \n    return total_loss / len(val_loader), weighted_auc, individual_aucs\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T03:45:22.473508Z","iopub.execute_input":"2025-08-05T03:45:22.474173Z","iopub.status.idle":"2025-08-05T03:45:22.529569Z","shell.execute_reply.started":"2025-08-05T03:45:22.474139Z","shell.execute_reply":"2025-08-05T03:45:22.528375Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ====================================================\n# MAIN TRAINING EXECUTION\n# ====================================================\n\ndef main():\n    print(f\"Using device: {Config.DEVICE}\")\n    print(f\"Mixed precision: {Config.MIXED_PRECISION}\")\n    \n    # Load data\n    train_df = pd.read_csv(Config.TRAIN_CSV_PATH)\n    localizer_df = pd.read_csv(Config.LOCALIZER_CSV_PATH)\n\n    print(\"---!!! RUNNING IN DEBUG MODE ON A SMALL SUBSET !!!---\")\n    #print(f\"Training samples: {len(train_df)}\")\n    train_df = train_df.head(100)  # Limit to 100 samples for speed testing\n    print(f\"Training samples: {len(train_df)} (limited for speed testing)\")\n    print(f\"Positive aneurysm cases: {train_df['Aneurysm Present'].sum()}\")\n    \n    # Create stratified group k-fold split\n    # Use patient-level grouping to prevent data leakage\n    train_df['patient_group'] = train_df['PatientID'] if 'PatientID' in train_df.columns else range(len(train_df))\n    \n    skf = StratifiedGroupKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=42)\n    train_df['fold'] = -1\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(\n        train_df, train_df['Aneurysm Present'], groups=train_df['patient_group']\n    )):\n        train_df.loc[val_idx, 'fold'] = fold\n    \n    # Calculate class weights for imbalanced data\n    pos_counts = train_df[Config.LABEL_COLS].sum()\n    neg_counts = len(train_df) - pos_counts\n    pos_weights = neg_counts / (pos_counts + 1e-8)  # Add small epsilon\n    pos_weights = torch.tensor(pos_weights.values, dtype=torch.float32)\n    \n    print(\"Class weights:\", pos_weights)\n    \n    # Initialize processor\n    processor = AdvancedDICOMProcessor()\n    \n    # Train models for each fold\n    fold_scores = []\n    \n    for fold in range(Config.N_FOLDS):\n        print(f\"\\n{'='*50}\")\n        print(f\"FOLD {fold + 1}/{Config.N_FOLDS}\")\n        print(f\"{'='*50}\")\n        \n        # Split data\n        train_fold_df = train_df[train_df['fold'] != fold].reset_index(drop=True)\n        val_fold_df = train_df[train_df['fold'] == fold].reset_index(drop=True)\n        \n        print(f\"Train: {len(train_fold_df)}, Validation: {len(val_fold_df)}\")\n        \n        # Create datasets\n        train_dataset = EnhancedAneurysmDataset(\n            train_fold_df, localizer_df, Config.SERIES_DIR, processor, mode='train', fold=fold\n        )\n        val_dataset = EnhancedAneurysmDataset(\n            val_fold_df, localizer_df, Config.SERIES_DIR, processor, mode='val', fold=fold\n        )\n        \n        # Create data loaders\n        train_loader = DataLoader(\n            train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, \n            num_workers=4, pin_memory=True, drop_last=True\n        )\n        val_loader = DataLoader(\n            val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, \n            num_workers=4, pin_memory=True\n        )\n        \n        # Initialize model\n        model = SimplifiedAneurysmNet().to(Config.DEVICE)\n        criterion = WeightedMultiLabelLoss(pos_weights=pos_weights)\n        \n        # Optimizer with different learning rates for different parts\n        # optimizer = optim.AdamW([\n        #     {'params': model.backbone.parameters(), 'lr': Config.LEARNING_RATE},\n        #     {'params': model.classifier.parameters(), 'lr': Config.LEARNING_RATE * 2},\n        #     {'params': model.metadata_mlp.parameters(), 'lr': Config.LEARNING_RATE * 2}\n        # ], weight_decay=Config.WEIGHT_DECAY)\n        optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)\n        \n        # Learning rate scheduler\n        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            optimizer, T_0=10, T_mult=2, eta_min=1e-6\n        )\n        \n        scaler = GradScaler(enabled=Config.MIXED_PRECISION)\n        \n        # Training loop\n        best_auc = 0\n        patience = 10\n        patience_counter = 0\n        \n        for epoch in range(Config.EPOCHS):\n            # Train\n            train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, Config.DEVICE)\n            \n            # Validate\n            val_loss, val_auc, individual_aucs = validate_epoch(model, val_loader, criterion, Config.DEVICE)\n            \n            # Step scheduler\n            scheduler.step()\n            \n            print(f\"Epoch {epoch+1:3d} | \"\n                  f\"Train Loss: {train_loss:.4f} | \"\n                  f\"Val Loss: {val_loss:.4f} | \"\n                  f\"Val AUC: {val_auc:.4f}\")\n\n            processor.print_stats() # Print DICOM loading stats after each epoch\n\n            # SANITY CHECK: Stop if data loading is fundamentally broken\n            if processor.stats['total_loaded'] > 20:  # Only check after some attempts\n                success_rate = processor.stats['successful_loads'] / processor.stats['total_loaded']\n                if success_rate < 0.5:  # Less than 50% success rate\n                    print(f\"\\n🚨 STOPPING TRAINING: Data loading success rate is {success_rate:.1%}\")\n                    print(\"Fix the DICOM loading issues before continuing training!\")\n                    print(\"Most volumes are returning empty - this is a waste of time!\")\n                    break\n            \n            # Save best model\n            if val_auc > best_auc:\n                best_auc = val_auc\n                patience_counter = 0\n                torch.save({\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                    'val_auc': val_auc,\n                    'epoch': epoch,\n                    'fold': fold,\n                    'individual_aucs': individual_aucs\n                }, f'best_model_fold_{fold}.pth')\n            else:\n                patience_counter += 1\n                \n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n                \n            # Memory cleanup\n            if epoch % 5 == 0:\n                gc.collect()\n                torch.cuda.empty_cache()\n        \n        fold_scores.append(best_auc)\n        print(f\"Fold {fold + 1} best AUC: {best_auc:.4f}\")\n    \n    # Final results\n    mean_cv_score = np.mean(fold_scores)\n    std_cv_score = np.std(fold_scores)\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"CROSS-VALIDATION RESULTS\")\n    print(f\"{'='*50}\")\n    print(f\"Mean CV AUC: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")\n    print(f\"Individual fold scores: {fold_scores}\")\n    \n    # Save training summary\n    results = {\n        'cv_scores': fold_scores,\n        'mean_cv_score': mean_cv_score,\n        'std_cv_score': std_cv_score,\n        'config': vars(Config())\n    }\n    \n    with open('training_results.json', 'w') as f:\n        json.dump(results, f, indent=2, default=str)\n    \n    print(\"Training complete! Models saved as 'best_model_fold_X.pth'\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T03:45:42.127164Z","iopub.execute_input":"2025-08-05T03:45:42.127708Z","execution_failed":"2025-08-05T03:52:37.301Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nMixed precision: True\n---!!! RUNNING IN DEBUG MODE ON A SMALL SUBSET !!!---\nTraining samples: 100 (limited for speed testing)\nPositive aneurysm cases: 48\nClass weights: tensor([3.2333e+01, 4.9000e+01, 1.3286e+01, 9.0000e+00, 4.9000e+01, 1.1500e+01,\n        1.0111e+01, 9.9000e+01, 4.9000e+01, 4.9000e+01, 1.0000e+10, 3.2333e+01,\n        4.9000e+01, 1.0833e+00])\n\n==================================================\nFOLD 1/3\n==================================================\nTrain: 66, Validation: 34\nBasicUNet features: (16, 32, 64, 128, 256, 51).\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:   0%|          | 0/8 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Sample 0: DICOM load: 3.27s, Preprocess: 0.98s\nWarning: Received a non-3D volume. Returning empty target volume.\nWarning: Received a non-3D volume. Returning empty target volume.\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:  12%|█▎        | 1/8 [00:32<03:44, 32.04s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0: 1.68s\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:  25%|██▌       | 2/8 [00:32<01:19, 13.32s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 1: 0.21s\nSample 4: DICOM load: 10.50s, Preprocess: 2.01s\nSample 3: DICOM load: 24.33s, Preprocess: 4.81s\nSample 1: DICOM load: 7.14s, Preprocess: 1.21s\nWarning: Received a non-3D volume. Returning empty target volume.\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:  38%|███▊      | 3/8 [01:09<02:01, 24.40s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 2: 0.26s\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:  50%|█████     | 4/8 [01:10<00:59, 14.85s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 3: 0.22s\nWarning: Received a non-3D volume. Returning empty target volume.\nWarning: Received a non-3D volume. Returning empty target volume.\nWarning: Received a non-3D volume. Returning empty target volume.\nSample 2: DICOM load: 2.88s, Preprocess: 0.01s\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:  75%|███████▌  | 6/8 [01:31<00:22, 11.39s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 4: 0.26s\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 8/8 [02:15<00:00, 16.98s/it]\nValidating:   0%|          | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Warning: Received a non-3D volume. Returning empty target volume.\nSample 0: DICOM load: 6.06s, Preprocess: 0.79s\nSample 1: DICOM load: 10.46s, Preprocess: 2.64s\nSample 2: DICOM load: 7.79s, Preprocess: 1.20s\nSample 3: DICOM load: 10.30s, Preprocess: 1.94s\nSample 4: DICOM load: 4.90s, Preprocess: 1.13s\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 5/5 [01:42<00:00, 20.52s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch   1 | Train Loss: 452429748.0000 | Val Loss: 451950188.8000 | Val AUC: 0.4726\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:   0%|          | 0/8 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Sample 4: DICOM load: 9.80s, Preprocess: 3.30s\nSample 3: DICOM load: 20.40s, Preprocess: 4.98s\nSample 0: DICOM load: 2.33s, Preprocess: 0.91s\nSample 1: DICOM load: 6.97s, Preprocess: 1.56s\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:  12%|█▎        | 1/8 [01:03<07:24, 63.57s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0: 0.29s\nWarning: Received a non-3D volume. Returning empty target volume.\nWarning: Received a non-3D volume. Returning empty target volume.\nWarning: Received a non-3D volume. Returning empty target volume.\nSample 2: DICOM load: 2.91s, Preprocess: 0.01s\nWarning: Received a non-3D volume. Returning empty target volume.\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:  25%|██▌       | 2/8 [01:22<03:43, 37.32s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 1: 0.28s\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:  50%|█████     | 4/8 [01:22<00:49, 12.41s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 2: 0.21s\nBatch 3: 0.19s\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:  62%|██████▎   | 5/8 [01:27<00:28,  9.42s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 4: 0.24s\nWarning: Received a non-3D volume. Returning empty target volume.\nWarning: Received a non-3D volume. Returning empty target volume.\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 8/8 [01:50<00:00, 13.85s/it]\nValidating:   0%|          | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Warning: Received a non-3D volume. Returning empty target volume.\nSample 0: DICOM load: 4.82s, Preprocess: 0.99s\nSample 1: DICOM load: 11.21s, Preprocess: 2.70s\nSample 2: DICOM load: 4.52s, Preprocess: 1.15s\nSample 3: DICOM load: 8.60s, Preprocess: 1.95s\nSample 4: DICOM load: 2.88s, Preprocess: 1.10s\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}