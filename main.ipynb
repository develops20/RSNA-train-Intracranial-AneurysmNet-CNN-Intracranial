{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":99552,"databundleVersionId":13190393,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/02-aneurysmnet-cnn-intracranial-nb153?scriptVersionId=254317107\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install monai\n\n# ====================================================\n# RSNA INTRACRANIAL ANEURYSM DETECTION - TRAINING PIPELINE\n# ====================================================\n\nimport os\nimport gc\nimport warnings\nimport json\nimport time\nimport numpy as np\nimport pandas as pd\nfrom typing import Tuple, Dict, List\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport albumentations as A\n\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.amp import autocast, GradScaler\n\nimport pydicom\nimport pydicom.errors\nfrom scipy import ndimage\nimport nibabel as nib\nfrom monai.transforms import (\n    Compose, RandRotate90d, RandFlipd, RandAffined,\n    RandGaussianNoised, RandAdjustContrastd, ToTensord\n)\nfrom monai.networks.nets import BasicUNet\nfrom monai.losses import DiceCELoss, FocalLoss\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T09:18:29.174336Z","iopub.execute_input":"2025-08-05T09:18:29.17466Z","iopub.status.idle":"2025-08-05T09:20:30.033238Z","shell.execute_reply.started":"2025-08-05T09:18:29.174635Z","shell.execute_reply":"2025-08-05T09:20:30.032617Z"}},"outputs":[{"name":"stdout","text":"Collecting monai\n  Downloading monai-1.5.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai) (1.26.4)\nRequirement already satisfied: torch<2.7.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from monai) (2.6.0+cu124)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7.0,>=2.4.1->monai) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7.0,>=2.4.1->monai) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.24->monai) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\nDownloading monai-1.5.0-py3-none-any.whl (2.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, monai\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed monai-1.5.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"},{"name":"stderr","text":"2025-08-05 09:20:19.507071: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754385619.628876      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754385619.664782      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ====================================================\n# CONFIGURATION\n# ====================================================\n\nclass Config:\n    # Paths\n    TRAIN_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv'\n    LOCALIZER_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train_localizers.csv'\n    SERIES_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/'\n    SEGMENTATION_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/segmentations/'\n    \n    # Model parameters\n    TARGET_SIZE = (32, 64, 64)  # Increased resolution\n    EPOCHS = 2\n    BATCH_SIZE = 16  # Reduced due to larger input size\n    LEARNING_RATE = 1e-3\n    WEIGHT_DECAY = 1e-4\n    N_FOLDS = 3\n    \n    # Training parameters\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MIXED_PRECISION = True\n    GRADIENT_ACCUMULATION = 4\n    \n    # Competition constants\n    ID_COL = 'SeriesInstanceUID'\n    LABEL_COLS = [\n        'Left Infraclinoid Internal Carotid Artery', 'Right Infraclinoid Internal Carotid Artery',\n        'Left Supraclinoid Internal Carotid Artery', 'Right Supraclinoid Internal Carotid Artery',\n        'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery', 'Anterior Communicating Artery',\n        'Left Anterior Cerebral Artery', 'Right Anterior Cerebral Artery',\n        'Left Posterior Communicating Artery', 'Right Posterior Communicating Artery',\n        'Basilar Tip', 'Other Posterior Circulation', 'Aneurysm Present',\n    ]\n    \n    # Class weights for imbalanced data\n    ANEURYSM_PRESENT_WEIGHT = 13.0  # Match evaluation metric weighting\n\n# ====================================================\n# ENHANCED DATA PREPROCESSING\n# ====================================================\n\nclass AdvancedDICOMProcessor:\n    def __init__(self, target_size: Tuple[int, int, int] = Config.TARGET_SIZE):\n        self.target_size = target_size\n        self.stats = {\n            'total_loaded': 0,\n            'successful_loads': 0,\n            'shape_errors': 0,\n            'empty_volumes': 0,\n            'preprocessing_errors': 0\n        }\n        \n    def load_dicom_series(self, series_path: str) -> Tuple[np.ndarray, Dict]:\n        \"\"\"Load DICOM series with robust error handling\"\"\"\n        self.stats['total_loaded'] += 1\n        try:\n            dicom_files = [os.path.join(series_path, f) for f in os.listdir(series_path) if f.endswith('.dcm')]\n            if not dicom_files:\n                series_id = os.path.basename(series_path)\n                print(f\"No DICOM files found in {series_path}, using mean volume fallback\")\n                self.stats['empty_volumes'] += 1\n                # Log series with no DICOM files\n                log_message = f\"{series_id}: No DICOM files found\\n\"\n                try:\n                    with open('corrupted_series.txt', 'a') as f:\n                        f.write(log_message)\n                        f.flush()  # Ensure immediate write\n                    print(f\"üìù Logged series with no DICOMs: {series_id}\")\n                except Exception as e:\n                    print(f\"‚ö†Ô∏è  Failed to log series: {e}\")\n                return self._get_fallback_volume(), {}\n                \n            dicoms = []\n            corrupted_count = 0\n            \n            for f in dicom_files:\n                try:\n                    ds = pydicom.dcmread(f, force=True)\n                    if hasattr(ds, 'pixel_array') and ds.pixel_array.size > 0:\n                        dicoms.append(ds)\n                    else:\n                        corrupted_count += 1\n                        if corrupted_count <= 3:  # Only log first few\n                            print(f\"  Skipping empty pixel array: {os.path.basename(f)}\")\n                except pydicom.errors.InvalidDicomError:\n                    corrupted_count += 1\n                    if corrupted_count <= 3:\n                        print(f\"  Invalid DICOM file: {os.path.basename(f)}\")\n                    continue\n                except Exception as e:\n                    corrupted_count += 1\n                    if corrupted_count <= 3:\n                        print(f\"  Error reading {os.path.basename(f)}: {e}\")\n                    continue\n                    \n            if not dicoms:\n                series_id = os.path.basename(series_path)\n                print(f\"‚ùå No valid DICOMs in series ({corrupted_count} corrupted files), using mean volume fallback\")\n                self.stats['empty_volumes'] += 1\n                # Log corrupted series for inspection\n                log_message = f\"{series_id}: No valid DICOMs ({corrupted_count} corrupted files)\\n\"\n                try:\n                    with open('corrupted_series.txt', 'a') as f:\n                        f.write(log_message)\n                        f.flush()  # Ensure immediate write\n                    print(f\"üìù Logged corrupted series: {series_id}\")\n                except Exception as e:\n                    print(f\"‚ö†Ô∏è  Failed to log corrupted series: {e}\")\n                return self._get_fallback_volume(), {}\n                \n            if corrupted_count > 0:\n                print(f\"‚ö†Ô∏è  Series loaded with {corrupted_count} corrupted files (kept {len(dicoms)} valid)\")\n                \n            # Extract metadata from first DICOM\n            first_ds = dicoms[0]\n            metadata = {\n                'modality': getattr(first_ds, 'Modality', 'UNKNOWN'),\n                'spacing': getattr(first_ds, 'PixelSpacing', [1.0, 1.0]),\n                'slice_thickness': getattr(first_ds, 'SliceThickness', 1.0),\n                'rescale_slope': getattr(first_ds, 'RescaleSlope', 1.0),\n                'rescale_intercept': getattr(first_ds, 'RescaleIntercept', 0.0),\n            }\n            \n            # Sort by instance number \n            dicoms.sort(key=lambda x: int(getattr(x, 'InstanceNumber', 0)))\n            \n            # Process pixel arrays (minimal logging)\n            pixel_arrays = []\n            shapes = []\n            \n            for d in dicoms:\n                if hasattr(d, 'pixel_array'):\n                    try:\n                        arr = d.pixel_array\n                        if arr.ndim == 2 and arr.size > 0:  # Valid 2D slice\n                            pixel_arrays.append(arr)\n                            shapes.append(arr.shape)\n                    except:\n                        continue  # Skip corrupted slices\n            \n            if len(pixel_arrays) == 0:\n                print(f\"‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\")\n                self.stats['shape_errors'] += 1\n                return self._get_fallback_volume(), metadata\n            \n            # Handle shape consistency\n            unique_shapes = list(set(shapes))\n            if len(unique_shapes) == 1:\n                # All same shape - direct stacking\n                volume = np.stack(pixel_arrays, axis=0).astype(np.float32)\n            else:\n                # Multiple shapes - resize to most common\n                most_common_shape = Counter(shapes).most_common(1)[0][0]\n                resized_arrays = []\n                for arr in pixel_arrays:\n                    if arr.shape == most_common_shape:\n                        resized_arrays.append(arr.astype(np.float32))\n                    else:\n                        zoom_factors = (most_common_shape[0] / arr.shape[0], \n                                      most_common_shape[1] / arr.shape[1])\n                        resized_arr = ndimage.zoom(arr, zoom_factors, order=1, prefilter=False)\n                        resized_arrays.append(resized_arr.astype(np.float32))\n                volume = np.stack(resized_arrays, axis=0).astype(np.float32)\n            \n            # Log successful loads (first few only)\n            if self.stats['total_loaded'] <= 10:\n                print(f\"‚úÖ Loaded: {volume.shape} from {len(pixel_arrays)} slices\")\n            \n            # Apply rescale if available\n            if metadata['rescale_slope'] != 1.0 or metadata['rescale_intercept'] != 0.0:\n                volume = volume * metadata['rescale_slope'] + metadata['rescale_intercept']\n\n            self.stats['successful_loads'] += 1\n            return volume, metadata\n            \n        except Exception as e:\n            print(f\"Error loading {series_path}: {e}, using mean volume fallback\")\n            self.stats['shape_errors'] += 1\n            return self._get_fallback_volume(), {}\n\n    def _get_fallback_volume(self):\n        \"\"\"Get mean volume fallback or zeros if no dataset reference\"\"\"\n        if hasattr(self, 'dataset') and hasattr(self.dataset, 'mean_volume'):\n            return self.dataset.mean_volume.copy()\n        return np.zeros(self.target_size, dtype=np.float32)\n\n    def print_stats(self):\n        \"\"\"Print loading statistics\"\"\"\n        total = self.stats['total_loaded']\n        successful = self.stats['successful_loads']\n        empty = self.stats['empty_volumes']\n        shape_errors = self.stats['shape_errors']\n        \n        if total > 0:\n            success_rate = (successful / total) * 100\n            print(f\"\\nüìä === DICOM Loading Stats ===\")\n            print(f\"‚úÖ Successful loads: {successful}/{total} ({success_rate:.1f}%)\")\n            print(f\"‚ùå Corrupted/empty: {empty} ({empty/total*100:.1f}%)\")\n            print(f\"‚ö†Ô∏è  Shape errors: {shape_errors} ({shape_errors/total*100:.1f}%)\")\n            \n            if success_rate < 70:\n                print(f\"üö® SUCCESS RATE TOO LOW ({success_rate:.1f}%)!\")\n                print(f\"   Most volumes are corrupted - check dataset quality!\")\n            elif success_rate < 85:\n                print(f\"‚ö†Ô∏è  Moderate success rate ({success_rate:.1f}%) - some data quality issues\")\n            else:\n                print(f\"‚úÖ Good success rate ({success_rate:.1f}%)\")\n            print(f\"===============================\")\n\n    def preprocess_volume(self, volume: np.ndarray, metadata: Dict) -> np.ndarray:\n        \"\"\"Enhanced preprocessing with modality-specific handling\"\"\"\n        if volume.ndim != 3 or volume.size == 0:\n            print(f\"Warning: Received a non-3D volume. Returning empty target volume.\")\n            return np.zeros(self.target_size, dtype=np.float32)\n        \n        # Default windowing\n        p1, p99 = np.percentile(volume, [5, 95])\n        volume = np.clip(volume, p1, p99)\n        \n        # Normalization\n        vol_min, vol_max = volume.min(), volume.max()\n        if vol_max > vol_min:\n            volume = (volume - vol_min) / (vol_max - vol_min)\n        \n        # Resize to target size\n        if volume.shape != self.target_size:\n            zoom_factors = [self.target_size[i] / volume.shape[i] for i in range(3)]\n            volume = ndimage.zoom(volume, zoom_factors, order=1, prefilter=False)\n        \n        return volume.astype(np.float32)\n\n    def load_localization_mask(self, series_id: str, localizer_df: pd.DataFrame) -> np.ndarray:\n        return np.zeros(self.target_size, dtype=np.float32)\n\n\n# ====================================================\n# ENHANCED DATASET\n# ====================================================\n\nclass EnhancedAneurysmDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, localizer_df: pd.DataFrame, \n                 series_dir: str, processor: AdvancedDICOMProcessor, \n                 mode: str = 'train', fold: int = None, shared_mean_volume: np.ndarray = None):\n        self.df = df\n        self.localizer_df = localizer_df\n        self.series_dir = series_dir\n        self.processor = processor\n        self.mode = mode\n        self.fold = fold\n        \n        # Data augmentation for training\n        if mode == 'train':\n            self.transform = Compose([\n                # Lightweight augmentations to improve generalization\n                RandRotate90d(keys=['volume'], prob=0.2, spatial_axes=(0, 1)),  # 90-degree rotations only\n                RandFlipd(keys=['volume'], prob=0.5, spatial_axis=0),           # Axial flip\n                RandFlipd(keys=['volume'], prob=0.5, spatial_axis=1),           # Sagittal flip\n                RandGaussianNoised(keys=['volume'], prob=0.2, std=0.03),        # Very low noise level\n                ToTensord(keys=['volume'])\n            ])\n        else:\n            self.transform = Compose([ToTensord(keys=['volume'])])\n\n        # Set dataset reference in processor for mean volume access\n        self.processor.dataset = self\n\n         # Use shared mean volume or compute new one\n        if shared_mean_volume is not None:\n            self.mean_volume = shared_mean_volume.copy()\n            print(f\"üîÑ Using shared mean volume fallback (shape: {self.mean_volume.shape})\")\n        else:\n            # Precompute mean volume for fallback on corrupted DICOM files\n            print(\"üîÑ Computing mean volume fallback from valid series...\")\n            valid_volumes = []\n            sample_size = min(10, len(df))  # Sample 10 series for speed\n            \n            for i, series_id in enumerate(df[Config.ID_COL][:sample_size]):\n                series_path = os.path.join(series_dir, series_id)\n                try:\n                    volume, _ = processor.load_dicom_series(series_path)\n                    if not np.all(volume == 0) and volume.size > 0:\n                        volume = processor.preprocess_volume(volume, {})\n                        valid_volumes.append(volume)\n                        print(f\"  ‚úÖ Valid volume {i+1}/{sample_size}: {volume.shape}\")\n                except Exception as e:\n                    print(f\"  ‚ùå Skipped corrupted volume {i+1}/{sample_size}: {e}\")\n                    continue\n            \n            if valid_volumes:\n                self.mean_volume = np.mean(valid_volumes, axis=0).astype(np.float32)\n                print(f\"üìä Mean volume computed from {len(valid_volumes)} valid series: {self.mean_volume.shape}\")\n            else:\n                self.mean_volume = np.zeros(processor.target_size, dtype=np.float32)\n                print(\"‚ö†Ô∏è  No valid volumes found, using zero fallback\")\n            \n            print(f\"üéØ Mean volume fallback ready (shape: {self.mean_volume.shape})\")\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        load_start = time.time()\n        row = self.df.iloc[idx]\n        series_id = row[Config.ID_COL]\n        series_path = os.path.join(self.series_dir, series_id)\n        \n        # Load and process volume\n        volume, metadata = self.processor.load_dicom_series(series_path)\n        dicom_time = time.time() - load_start\n        preprocess_start = time.time()\n        volume = self.processor.preprocess_volume(volume, metadata)\n        \n        # Create localization mask (for auxiliary loss)\n        loc_mask = self.processor.load_localization_mask(series_id, self.localizer_df)\n        \n        # Get labels\n        labels = row[Config.LABEL_COLS].values.astype(np.float32)\n        \n        # Apply transforms\n        data_dict = {'volume': volume}\n        if self.transform:\n            data_dict = self.transform(data_dict)\n        \n        volume_tensor = data_dict['volume'].unsqueeze(0)  # Add channel dimension\n        loc_mask_tensor = torch.from_numpy(loc_mask).unsqueeze(0)\n        labels_tensor = torch.from_numpy(labels)\n        \n        # Add metadata features\n        modality_encoding = self._encode_modality(metadata.get('modality', 'UNKNOWN'))\n        metadata_tensor = torch.tensor(modality_encoding, dtype=torch.float32)\n        \n        preprocess_time = time.time() - preprocess_start\n        # Print timing for first few samples to debug\n        if idx < 5:\n            print(f\"Sample {idx}: DICOM load: {dicom_time:.2f}s, Preprocess: {preprocess_time:.2f}s\")\n        \n        return {\n            'volume': volume_tensor,\n            'localization_mask': loc_mask_tensor,\n            'labels': labels_tensor,\n            'metadata': metadata_tensor,\n            'series_id': series_id\n        }\n    \n    def _encode_modality(self, modality: str) -> List[float]:\n        \"\"\"One-hot encode modality\"\"\"\n        modalities = ['CTA', 'MRA', 'MRI', 'MR', 'UNKNOWN']\n        encoding = [0.0] * len(modalities)\n        if modality in modalities:\n            encoding[modalities.index(modality)] = 1.0\n        else:\n            encoding[-1] = 1.0  # UNKNOWN\n        return encoding\n\n\n# ====================================================\n# ADVANCED MODEL ARCHITECTURE\n# ====================================================\n\n#class MultiModalAneurysmNet(nn.Module):\nclass SimplifiedAneurysmNet(nn.Module):\n    def __init__(self, num_classes: int = len(Config.LABEL_COLS), \n                 spatial_dims: int = 3, in_channels: int = 1, \n        #          features: Tuple = (32, 64, 128, 256, 512, 1024)):\n        # super(MultiModalAneurysmNet, self).__init__()\n                 features: Tuple = (16, 32, 64, 128, 256, 51)):\n        super(SimplifiedAneurysmNet, self).__init__()\n        \n        # Main 3D U-Net backbone\n        self.backbone = BasicUNet(\n            spatial_dims=spatial_dims,\n            in_channels=in_channels,\n            out_channels=features[0],\n            features=features,\n            dropout=0.1 #Reduced dropout \n        )\n        \n        # Global average pooling\n        self.global_pool = nn.AdaptiveAvgPool3d(1)\n        \n        # # Metadata processing\n        # self.metadata_mlp = nn.Sequential(\n        #     nn.Linear(5, 32),  # 5 modality categories\n        #     nn.ReLU(),\n        #     nn.Dropout(0.3),\n        #     nn.Linear(32, 64),\n        #     nn.ReLU()\n        # )\n        \n        # Classification head\n        #feature_size = features[0] + 64  # backbone features + metadata features\n        self.classifier = nn.Sequential(\n            # nn.Linear(feature_size, 512),\n            # nn.ReLU(),\n            # nn.Dropout(0.5),\n            # nn.Linear(512, 256),\n            nn.Linear(features[0], 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, volume, metadata=None):\n        # Extract features from 3D volume\n        features = self.backbone(volume)\n        # Global features for classification\n        global_features = self.global_pool(features).flatten(1)\n        classification_logits = self.classifier(global_features)\n        return classification_logits, None\n\n# ====================================================\n# WEIGHTED LOSS FUNCTION\n# ====================================================\n\nclass WeightedMultiLabelLoss(nn.Module):\n    def __init__(self, pos_weights=None, aneurysm_weight=13.0):\n        super().__init__()\n        self.pos_weights = pos_weights\n        self.aneurysm_weight = aneurysm_weight\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n        \n    def forward(self, logits, targets):\n        bce_loss = self.bce(logits, targets)\n        \n        # Apply position weights if provided\n        if self.pos_weights is not None:\n            bce_loss = bce_loss * self.pos_weights.to(logits.device)\n        \n        # Weight the \"Aneurysm Present\" class higher (last column)\n        weights = torch.ones_like(bce_loss)\n        weights[:, -1] = self.aneurysm_weight\n        \n        weighted_loss = bce_loss * weights\n        return weighted_loss.mean()\n\n\n# ====================================================\n# TRAINING FUNCTIONS\n# ====================================================\n\ndef compute_weighted_auc(y_true, y_pred):\n    \"\"\"Compute weighted AUC matching competition metric\"\"\"\n    aucs = []\n    weights = []\n    \n    for i in range(len(Config.LABEL_COLS)):\n        try:\n            auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n            aucs.append(auc)\n            # Weight \"Aneurysm Present\" (last column) higher\n            weights.append(13.0 if i == len(Config.LABEL_COLS) - 1 else 1.0)\n        except ValueError:\n            aucs.append(0.5)  # Default for no positive cases\n            weights.append(13.0 if i == len(Config.LABEL_COLS) - 1 else 1.0)\n    \n    weighted_auc = sum(a * w for a, w in zip(aucs, weights)) / sum(weights)\n    return weighted_auc, aucs\n\ndef train_epoch(model, train_loader, optimizer, criterion, scaler, device):\n    model.train()\n    total_loss = 0\n    num_batches = 0\n    skipped_batches = 0\n    \n    # Device verification for debugging\n    print(f\"üîß Training on device: {device}\")\n    if hasattr(model, 'module'):  # DataParallel wrapped\n        print(f\"üîß Model device (DataParallel): {next(model.module.parameters()).device}\")\n    else:\n        print(f\"üîß Model device: {next(model.parameters()).device}\")\n    print(f\"üîß Criterion on GPU: {hasattr(criterion, 'pos_weights') and criterion.pos_weights.device if hasattr(criterion, 'pos_weights') else 'N/A'}\")\n    \n    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch\")):\n        start_time = time.time()\n        # Transfer data to GPU with non-blocking for better performance\n        volume = batch['volume'].to(device, non_blocking=True)\n        metadata = batch['metadata'].to(device, non_blocking=True)\n        labels = batch['labels'].to(device, non_blocking=True)\n        loc_mask = batch['localization_mask'].to(device, non_blocking=True)\n        \n        # Add GPU monitoring for first few batches\n        if batch_idx < 3:\n            gpu_mem_before = torch.cuda.memory_allocated(0) / 1e9\n            print(f\"  üîç Batch {batch_idx}: GPU memory before forward: {gpu_mem_before:.2f}GB\")\n\n        # CRITICAL FIX: Skip batches with zero-filled volumes\n        if torch.all(volume == 0) or torch.var(volume) < 1e-6:\n            skipped_batches += 1\n            if batch_idx < 5:  # Log first few skips\n                print(f\"‚ö†Ô∏è  Skipping batch {batch_idx}: zero-filled or low-variance volume\")\n            continue\n            \n        # Forward pass timing\n        forward_start = time.time()\n        with autocast(device_type=device.type, enabled=Config.MIXED_PRECISION):\n            class_logits, _ = model(volume, metadata)\n            total_loss_batch = criterion(class_logits, labels)\n        forward_time = time.time() - forward_start\n        \n        # Add detailed timing for first few batches\n        if batch_idx < 3:\n            gpu_mem_after = torch.cuda.memory_allocated(0) / 1e9\n            print(f\"  ‚ö° Batch {batch_idx}: Forward pass: {forward_time:.3f}s, GPU memory after: {gpu_mem_after:.2f}GB\")\n            \n        # DEBUG: Check for extreme loss values\n        if total_loss_batch.item() > 1e7:\n            print(f\"üö® Warning: Extreme loss in training batch {batch_idx}: {total_loss_batch.item():.2e}\")\n            print(f\"   Labels: {labels[0].cpu().numpy()}\")  # Print first sample's labels \n        \n        # Gradient accumulation and backward pass timing\n        backward_start = time.time()\n        scaled_loss = total_loss_batch / Config.GRADIENT_ACCUMULATION\n        scaler.scale(scaled_loss).backward()\n        backward_time = time.time() - backward_start\n        \n        if batch_idx < 3:\n            print(f\"  üîÑ Batch {batch_idx}: Backward pass: {backward_time:.3f}s\")\n        \n        if (batch_idx + 1) % Config.GRADIENT_ACCUMULATION == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n        \n        total_loss += total_loss_batch.item()\n        num_batches += 1\n\n        # Print timing for first few batches to identify bottlenecks\n        if batch_idx < 5:\n            batch_time = time.time() - start_time\n            print(f\"Batch {batch_idx}: {batch_time:.2f}s\")\n    \n    if skipped_batches > 0:\n        print(f\"‚ö†Ô∏è  Skipped {skipped_batches} batches with corrupted/zero volumes\")\n    \n    return total_loss / max(num_batches, 1) if num_batches > 0 else float('inf')\n\ndef validate_epoch(model, val_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    num_batches = 0\n    skipped_batches = 0\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Validating\")):\n            volume = batch['volume'].to(device)\n            metadata = batch['metadata'].to(device)\n            labels = batch['labels'].to(device)\n            \n            # CRITICAL FIX: Skip batches with zero-filled volumes\n            if torch.all(volume == 0) or torch.var(volume) < 1e-6:\n                skipped_batches += 1\n                if batch_idx < 3:  # Log first few skips\n                    print(f\"‚ö†Ô∏è  Skipping validation batch {batch_idx}: zero-filled or low-variance volume\")\n                continue\n            \n            with autocast(device_type=device.type, enabled=Config.MIXED_PRECISION):\n                class_logits, _ = model(volume, metadata)\n                loss = criterion(class_logits, labels)\n\n                # DEBUG: Check for extreme loss values\n                if loss.item() > 1e7:\n                    print(f\"üö® Warning: Extreme loss in validation batch {batch_idx}: {loss.item():.2e}\")\n                    print(f\"   Labels: {labels[0].cpu().numpy()}\")  # Print first sample's labels\n            \n            total_loss += loss.item()\n            num_batches += 1\n            \n            # Collect predictions for AUC calculation\n            probs = torch.sigmoid(class_logits).cpu().numpy()\n            all_preds.append(probs)\n            all_labels.append(labels.cpu().numpy())\n\n    if skipped_batches > 0:\n        print(f\"‚ö†Ô∏è  Skipped {skipped_batches} validation batches with corrupted/zero volumes\")\n    \n    if len(all_preds) == 0:\n        print(\"üö® WARNING: No valid validation batches - all were corrupted!\")\n        return float('inf'), 0.5, [0.5] * len(Config.LABEL_COLS)\n    \n    all_preds = np.vstack(all_preds)\n    all_labels = np.vstack(all_labels)\n    \n    weighted_auc, individual_aucs = compute_weighted_auc(all_labels, all_preds)\n    \n    return total_loss / max(num_batches, 1), weighted_auc, individual_aucs\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T09:20:34.888052Z","iopub.execute_input":"2025-08-05T09:20:34.889086Z","iopub.status.idle":"2025-08-05T09:20:34.93694Z","shell.execute_reply.started":"2025-08-05T09:20:34.889054Z","shell.execute_reply":"2025-08-05T09:20:34.936307Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ====================================================\n# MAIN TRAINING EXECUTION\n# ====================================================\n\ndef main():\n    print(f\"Using device: {Config.DEVICE}\")\n    print(f\"Mixed precision: {Config.MIXED_PRECISION}\")\n    if torch.cuda.is_available():\n        print(f\"GPU devices available: {torch.cuda.device_count()}\")\n        for i in range(torch.cuda.device_count()):\n            gpu_mem = torch.cuda.get_device_properties(i).total_memory / 1e9\n            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)} ({gpu_mem:.1f}GB)\")\n    print(f\"Optimized settings - Batch size: {Config.BATCH_SIZE}, Workers: 6/4 (Conservative)\")\n    \n    # Load data\n    train_df = pd.read_csv(Config.TRAIN_CSV_PATH)\n    localizer_df = pd.read_csv(Config.LOCALIZER_CSV_PATH)\n\n    print(\"---!!! RUNNING IN DEBUG MODE ON A SMALL SUBSET !!!---\")\n    #print(f\"Training samples: {len(train_df)}\")\n    train_df = train_df.head(100)  # Limit to 100 samples for speed testing\n    print(f\"Training samples: {len(train_df)} (limited for speed testing)\")\n    print(f\"Positive aneurysm cases: {train_df['Aneurysm Present'].sum()}\")\n\n    # Calculate class weights for imbalanced data\n    pos_counts = train_df[Config.LABEL_COLS].sum()\n    neg_counts = len(train_df) - pos_counts\n    pos_weights = neg_counts / (pos_counts + 1e-8)  # Add small epsilon\n    pos_weights = np.minimum(pos_weights, 100.0)  # Cap weights at 100\n    pos_weights = torch.tensor(pos_weights, dtype=torch.float32)  # Convert to tensor\n    print(\"Class weights (capped at 100):\", pos_weights)\n\n    # DATASET INTEGRITY CHECK\n    print(\"\\nüîç Checking dataset integrity...\")\n    valid_series = 0\n    invalid_series = []\n    \n    for series_id in train_df[Config.ID_COL]:\n        series_path = os.path.join(Config.SERIES_DIR, series_id)\n        if os.path.exists(series_path):\n            dicom_files = [f for f in os.listdir(series_path) if f.endswith('.dcm')]\n            if dicom_files:\n                valid_series += 1\n            else:\n                invalid_series.append(f\"No DICOMs: {series_id}\")\n        else:\n            invalid_series.append(f\"Missing path: {series_id}\")\n    \n    success_rate = valid_series / len(train_df)\n    print(f\"üìä Dataset check: {valid_series}/{len(train_df)} series valid ({success_rate:.1%})\")\n    \n    if success_rate < 0.7:\n        print(f\"üö® WARNING: Only {success_rate:.1%} of series are accessible!\")\n        print(\"First few issues:\")\n        for issue in invalid_series[:5]:\n            print(f\"  - {issue}\")\n        print(f\"‚ö†Ô∏è  Training will proceed, but expect many corrupted DICOM errors\")\n    else:\n        print(f\"‚úÖ Good dataset integrity ({success_rate:.1%} valid)\")\n        \n    print()\n\n    # Initialize corrupted series log\n    with open('corrupted_series.txt', 'w') as f:\n        f.write(\"# Corrupted series log - check this file to identify problematic DICOM series\\n\")\n    print(\"üìù Initialized 'corrupted_series.txt' for logging corrupted series\")\n    \n    # Create stratified group k-fold split\n    # Use patient-level grouping to prevent data leakage\n    train_df['patient_group'] = train_df['PatientID'] if 'PatientID' in train_df.columns else range(len(train_df))\n    \n    skf = StratifiedGroupKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=42)\n    train_df['fold'] = -1\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(\n        train_df, train_df['Aneurysm Present'], groups=train_df['patient_group']\n    )):\n        train_df.loc[val_idx, 'fold'] = fold\n    \n    # Initialize processor\n    processor = AdvancedDICOMProcessor()\n    \n    # Train models for each fold\n    fold_scores = []\n    \n    for fold in range(Config.N_FOLDS):\n        print(f\"\\n{'='*50}\")\n        print(f\"FOLD {fold + 1}/{Config.N_FOLDS}\")\n        print(f\"{'='*50}\")\n        \n        # Split data\n        train_fold_df = train_df[train_df['fold'] != fold].reset_index(drop=True)\n        val_fold_df = train_df[train_df['fold'] == fold].reset_index(drop=True)\n        \n        print(f\"Train: {len(train_fold_df)}, Validation: {len(val_fold_df)}\")\n        \n        # Create datasets\n        train_dataset = EnhancedAneurysmDataset(\n            train_fold_df, localizer_df, Config.SERIES_DIR, processor, mode='train', fold=fold\n        )\n        val_dataset = EnhancedAneurysmDataset(\n            val_fold_df, localizer_df, Config.SERIES_DIR, processor, mode='val', fold=fold,\n            shared_mean_volume=train_dataset.mean_volume\n        )\n        \n        # Create data loaders with conservative settings for stability\n        train_loader = DataLoader(\n            train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, \n            num_workers=6, pin_memory=True, drop_last=True, \n            prefetch_factor=2, persistent_workers=True\n        )\n        val_loader = DataLoader(\n            val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, \n            num_workers=4, pin_memory=True, prefetch_factor=2, persistent_workers=True\n        )\n        \n        # Initialize model\n        model = SimplifiedAneurysmNet().to(Config.DEVICE)\n        # Enable multi-GPU training if available\n        if torch.cuda.device_count() > 1:\n            print(f\"üöÄ Using {torch.cuda.device_count()} GPUs for training\")\n            model = nn.DataParallel(model)\n        else:\n            print(f\"üì± Using single GPU: {Config.DEVICE}\")\n        criterion = WeightedMultiLabelLoss(pos_weights=pos_weights).to(Config.DEVICE)\n        \n        # Optimizer with different learning rates for different parts\n        # optimizer = optim.AdamW([\n        #     {'params': model.backbone.parameters(), 'lr': Config.LEARNING_RATE},\n        #     {'params': model.classifier.parameters(), 'lr': Config.LEARNING_RATE * 2},\n        #     {'params': model.metadata_mlp.parameters(), 'lr': Config.LEARNING_RATE * 2}\n        # ], weight_decay=Config.WEIGHT_DECAY)\n        optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)\n        \n        # Learning rate scheduler\n        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            optimizer, T_0=10, T_mult=2, eta_min=1e-6\n        )\n        \n        scaler = GradScaler(enabled=Config.MIXED_PRECISION)\n        \n        # Training loop\n        best_auc = 0\n        patience = 10\n        patience_counter = 0\n        \n        for epoch in range(Config.EPOCHS):\n            # Train\n            train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, Config.DEVICE)\n            \n            # Validate\n            val_loss, val_auc, individual_aucs = validate_epoch(model, val_loader, criterion, Config.DEVICE)\n            \n            # Step scheduler\n            scheduler.step()\n            \n            print(f\"Epoch {epoch+1:3d} | \"\n                  f\"Train Loss: {train_loss:.4f} | \"\n                  f\"Val Loss: {val_loss:.4f} | \"\n                  f\"Val AUC: {val_auc:.4f}\")\n\n            processor.print_stats() # Print DICOM loading stats after each epoch\n\n            # Check corrupted series log\n            try:\n                with open('corrupted_series.txt', 'r') as f:\n                    lines = f.readlines()\n                    corrupted_count = len([l for l in lines if not l.startswith('#')])\n                    if corrupted_count > 0:\n                        print(f\"üìÑ Corrupted series logged: {corrupted_count} entries in 'corrupted_series.txt'\")\n                    else:\n                        print(f\"‚úÖ No corrupted series logged this epoch\")\n            except FileNotFoundError:\n                print(f\"üìÑ No corrupted series log file found\")\n\n            # SANITY CHECK: Stop if data loading is fundamentally broken\n            if processor.stats['total_loaded'] > 20:  # Only check after some attempts\n                success_rate = processor.stats['successful_loads'] / processor.stats['total_loaded']\n                if success_rate < 0.5:  # Less than 50% success rate\n                    print(f\"\\nüö® STOPPING TRAINING: Data loading success rate is {success_rate:.1%}\")\n                    print(\"Fix the DICOM loading issues before continuing training!\")\n                    print(\"Most volumes are returning empty - this is a waste of time!\")\n                    break\n            \n            # Save best model\n            if val_auc > best_auc:\n                best_auc = val_auc\n                patience_counter = 0\n                torch.save({\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                    'val_auc': val_auc,\n                    'epoch': epoch,\n                    'fold': fold,\n                    'individual_aucs': individual_aucs\n                }, f'best_model_fold_{fold}.pth')\n            else:\n                patience_counter += 1\n                \n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n                \n            # Memory cleanup\n            if epoch % 5 == 0:\n                gc.collect()\n                torch.cuda.empty_cache()\n        \n        fold_scores.append(best_auc)\n        print(f\"Fold {fold + 1} best AUC: {best_auc:.4f}\")\n    \n    # Final results\n    mean_cv_score = np.mean(fold_scores)\n    std_cv_score = np.std(fold_scores)\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"CROSS-VALIDATION RESULTS\")\n    print(f\"{'='*50}\")\n    print(f\"Mean CV AUC: {mean_cv_score:.4f} ¬± {std_cv_score:.4f}\")\n    print(f\"Individual fold scores: {fold_scores}\")\n    \n    # Save training summary\n    results = {\n        'cv_scores': fold_scores,\n        'mean_cv_score': mean_cv_score,\n        'std_cv_score': std_cv_score,\n        'config': vars(Config())\n    }\n    \n    with open('training_results.json', 'w') as f:\n        json.dump(results, f, indent=2, default=str)\n    \n    print(\"Training complete! Models saved as 'best_model_fold_X.pth'\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T09:20:41.7878Z","iopub.execute_input":"2025-08-05T09:20:41.7881Z","execution_failed":"2025-08-05T09:26:58.695Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nMixed precision: True\nGPU devices available: 2\n  GPU 0: Tesla T4 (15.8GB)\n  GPU 1: Tesla T4 (15.8GB)\nOptimized settings - Batch size: 24, Workers: 12/8\n---!!! RUNNING IN DEBUG MODE ON A SMALL SUBSET !!!---\nTraining samples: 100 (limited for speed testing)\nPositive aneurysm cases: 48\nClass weights (capped at 100): tensor([ 32.3333,  49.0000,  13.2857,   9.0000,  49.0000,  11.5000,  10.1111,\n         99.0000,  49.0000,  49.0000, 100.0000,  32.3333,  49.0000,   1.0833])\n\nüîç Checking dataset integrity...\nüìä Dataset check: 100/100 series valid (100.0%)\n‚úÖ Good dataset integrity (100.0% valid)\n\nüìù Initialized 'corrupted_series.txt' for logging corrupted series\n\n==================================================\nFOLD 1/3\n==================================================\nTrain: 66, Validation: 34\nüîÑ Computing mean volume fallback from valid series...\n‚úÖ Loaded: (147, 512, 512) from 147 slices\n  ‚úÖ Valid volume 1/10: (32, 64, 64)\n‚úÖ Loaded: (276, 512, 512) from 276 slices\n  ‚úÖ Valid volume 2/10: (32, 64, 64)\n‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\n‚úÖ Loaded: (856, 512, 512) from 856 slices\n  ‚úÖ Valid volume 4/10: (32, 64, 64)\n‚úÖ Loaded: (178, 768, 696) from 178 slices\n  ‚úÖ Valid volume 5/10: (32, 64, 64)\n‚úÖ Loaded: (671, 512, 512) from 671 slices\n  ‚úÖ Valid volume 6/10: (32, 64, 64)\n‚úÖ Loaded: (205, 512, 512) from 205 slices\n  ‚úÖ Valid volume 7/10: (32, 64, 64)\n‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\n‚úÖ Loaded: (83, 512, 512) from 83 slices\n  ‚úÖ Valid volume 9/10: (32, 64, 64)\n‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\nüìä Mean volume computed from 7 valid series: (32, 64, 64)\nüéØ Mean volume fallback ready (shape: (32, 64, 64))\nüîÑ Using shared mean volume fallback (shape: (32, 64, 64))\nBasicUNet features: (16, 32, 64, 128, 256, 51).\nüöÄ Using 2 GPUs for training\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:   0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\nSample 2: DICOM load: 1.78s, Preprocess: 0.03s\nSample 0: DICOM load: 1.47s, Preprocess: 0.62s\n‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\n‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\nSample 4: DICOM load: 5.14s, Preprocess: 1.87s\nSample 3: DICOM load: 10.34s, Preprocess: 3.96s\nSample 1: DICOM load: 3.87s, Preprocess: 1.17s\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [01:39<01:39, 99.47s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 0: 1.93s\n‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\n‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [02:00<00:00, 60.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Batch 1: 0.38s\n","output_type":"stream"},{"name":"stderr","text":"Validating:   0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\nSample 0: DICOM load: 3.91s, Preprocess: 0.75s\nSample 1: DICOM load: 8.19s, Preprocess: 1.91s\nSample 2: DICOM load: 5.12s, Preprocess: 1.21s\nSample 3: DICOM load: 10.28s, Preprocess: 2.17s\nSample 4: DICOM load: 4.37s, Preprocess: 1.10s\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [02:34<00:00, 77.21s/it] \n","output_type":"stream"},{"name":"stdout","text":"Epoch   1 | Train Loss: 28.1122 | Val Loss: 28.1112 | Val AUC: 0.6270\n\nüìä === DICOM Loading Stats ===\n‚úÖ Successful loads: 7/10 (70.0%)\n‚ùå Corrupted/empty: 0 (0.0%)\n‚ö†Ô∏è  Shape errors: 3 (30.0%)\n‚ö†Ô∏è  Moderate success rate (70.0%) - some data quality issues\n===============================\n‚úÖ No corrupted series logged this epoch\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:   0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\n‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\n‚ùå No valid pixel arrays in series (corrupted DICOM), using mean volume fallback\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}