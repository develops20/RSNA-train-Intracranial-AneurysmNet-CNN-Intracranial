{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":99552,"databundleVersionId":13190393,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/02-aneurysmnet-cnn-intracranial-training-nb153?scriptVersionId=256412157\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# ====================================================\n# RSNA INTRACRANIAL ANEURYSM DETECTION - TRAINING PIPELINE\n# ====================================================\n\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport pydicom\nimport nibabel as nib\nimport cv2\nfrom scipy import ndimage\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Seeding for reproducibility\ndef set_global_seed(seed: int):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    try:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    except Exception:\n        pass\n\nset_global_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T18:40:04.54676Z","iopub.execute_input":"2025-08-15T18:40:04.547435Z","iopub.status.idle":"2025-08-15T18:40:10.111665Z","shell.execute_reply.started":"2025-08-15T18:40:04.547409Z","shell.execute_reply":"2025-08-15T18:40:10.110916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================\n# CELL 2: CONFIGURATION\n# ====================================================\n\nclass Config:\n    # Paths\n    TRAIN_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv'\n    LOCALIZER_CSV_PATH = '/kaggle/input/rsna-intracranial-aneurysm-detection/train_localizers.csv'\n    SERIES_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/'\n    SEGMENTATION_DIR = '/kaggle/input/rsna-intracranial-aneurysm-detection/segmentations/'\n    \n    # Stage 1: 3D Segmentation\n    STAGE1_TARGET_SIZE = (48, 112, 112) \n    STAGE1_BATCH_SIZE = 36\n    STAGE1_EPOCHS = 1\n    STAGE1_LR = 1e-4\n    \n    # General\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MIXED_PRECISION = True\n    CUDNN_BENCHMARK = True\n    N_FOLDS = 3\n    \n    # Competition constants\n    ID_COL = 'SeriesInstanceUID'\n    LABEL_COLS = [\n        'Left Infraclinoid Internal Carotid Artery', 'Right Infraclinoid Internal Carotid Artery',\n        'Left Supraclinoid Internal Carotid Artery', 'Right Supraclinoid Internal Carotid Artery',\n        'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery', 'Anterior Communicating Artery',\n        'Left Anterior Cerebral Artery', 'Right Anterior Cerebral Artery',\n        'Left Posterior Communicating Artery', 'Right Posterior Communicating Artery',\n        'Basilar Tip', 'Other Posterior Circulation', 'Aneurysm Present',\n    ]\n    TARGET_COL = 'Aneurysm Present'\n    \n    # Debug settings\n    DEBUG_MODE = False\n    DEBUG_SAMPLES = 200  # Use small subset for testing\n\n    # Optional: require GT masks for training\n    # If True, will filter to only series with non-empty .nii masks in SEGMENTATION_DIR.\n    # Leave False on Kaggle since GT masks are typically unavailable.\n    FILTER_TO_MASKED_SERIES = False\n\n    # Synthetic mask behavior\n    SYN_MASK_RANDOM_CENTER = True\n    SYN_MASK_SIZE_FRAC = (0.1, 0.3)  # edge as fraction of min(D,H,W)\n\n    # Augmentations\n    USE_ELASTIC_DEFORMATION = False  # disable heavy CPU elastic deformation for speed\n    \n    # Loss weighting\n    SEG_LOSS_WEIGHT = 0.1  # down-weight segmentation loss when masks may be synthetic\n    \n    # Reproducibility\n    SEED = 42\n\n    # Validation control (legacy; using Config.VAL_SUBSAMPLE_FRACTION below)\n    VAL_SUBSAMPLE_FRACTION = 1.0\n    VAL_FULL_EVERY_N_EPOCHS = 1\n\n    # DICOM loading performance\n    MAX_SLICES_PER_SERIES = 64\n    USE_DICOM_THREADPOOL = False\n    DICOM_WORKERS = 4\n\n    # Caching\n    USE_DISK_CACHE = True\n    CACHE_DIR = '/kaggle/working/stage1_cache'\n    MASK_CACHE_DIR = '/kaggle/working/mask_cache'\n    PRE_BUILD_CACHE = False\n    MASK_CACHE_DTYPE = np.uint8\n    PREPROC_VERSION = 'v2'  # bump when preprocessing changes to avoid stale cache\n    \n    # CT preprocessing\n    CTA_WINDOW_CENTER = 300.0\n    CTA_WINDOW_WIDTH = 700.0\n    TARGET_SPACING_MM = 1.0\n\n    # Training\n    GRAD_ACCUM_STEPS = 1\n\n    # GPU augmentations (lightweight; applied during training only)\n    AUG_ENABLE_GPU = False\n    AUG_BRIGHTNESS = 0.2    # multiplicative ±range\n    AUG_CONTRAST = 0.2      # contrast around mean ±range\n    AUG_NOISE_STD = 0.03    # max Gaussian noise std\n    AUG_GAMMA_MINMAX = (0.9, 1.1)\n\n    # UNet feature width scaling\n    UNET_FEATURES = (24, 48, 96, 192, 384, 24)\n    UNET_OUT_CHANNELS = 24\n\n    # Validation\n    VAL_SUBSAMPLE_FRACTION = 1.0\n    EARLY_STOPPING_PATIENCE = 3\n    \n    # Loss composition\n    FOCAL_LOSS_WEIGHT = 0.0\n\nprint(f\"✅ Configuration loaded - Device: {Config.DEVICE}\")\n\n# ====================================================\n# CELL 2.5: CUSTOM 3D UNET (REPLACES MONAI BASICUNET)\n# ====================================================\n\nclass Custom3DUNet(nn.Module):\n    \"\"\"Pure PyTorch 3D UNet implementation to replace MONAI BasicUNet\"\"\"\n    \n    def __init__(self, spatial_dims=3, in_channels=1, out_channels=None, \n                 features=None, dropout=0.1):\n        super().__init__()\n        \n        # Use configurable features\n        features = features or Config.UNET_FEATURES\n        out_channels = out_channels or Config.UNET_OUT_CHANNELS\n        self.features = features\n        self.dropout = dropout\n        \n        # Encoder (downsampling path)\n        self.encoder_blocks = nn.ModuleList()\n        prev_channels = in_channels\n        \n        for i, feature_count in enumerate(features[:-1]):  # Exclude last feature (decoder output)\n            # Each encoder block: Conv3D -> BatchNorm -> ReLU -> Conv3D -> BatchNorm -> ReLU\n            block = nn.Sequential(\n                nn.Conv3d(prev_channels, feature_count, kernel_size=3, padding=1),\n                nn.BatchNorm3d(feature_count),\n                nn.ReLU(inplace=True),\n                nn.Conv3d(feature_count, feature_count, kernel_size=3, padding=1),\n                nn.BatchNorm3d(feature_count),\n                nn.ReLU(inplace=True),\n                nn.Dropout3d(dropout) if dropout > 0 else nn.Identity()\n            )\n            self.encoder_blocks.append(block)\n            prev_channels = feature_count\n        \n        # Downsampling layers (MaxPool)\n        self.downsample_layers = nn.ModuleList([\n            nn.MaxPool3d(kernel_size=2, stride=2) \n            for _ in range(len(features) - 2)  # No downsampling after last encoder block\n        ])\n        \n        # Decoder (upsampling path)\n        self.decoder_blocks = nn.ModuleList()\n        self.upsample_layers = nn.ModuleList()\n        \n        # Reverse the features for decoder (skip the input feature count)\n        decoder_features = list(reversed(features[:-1]))  # [512, 256, 128, 64, 32]\n        \n        for i in range(len(decoder_features) - 1):\n            current_features = decoder_features[i]\n            next_features = decoder_features[i + 1]\n            \n            # Upsampling layer\n            upsample = nn.ConvTranspose3d(\n                current_features, next_features, \n                kernel_size=2, stride=2\n            )\n            self.upsample_layers.append(upsample)\n            \n            # Decoder block (concatenation + convolutions)\n            # Input: upsampled features + skip connection = next_features * 2\n            decoder_block = nn.Sequential(\n                nn.Conv3d(next_features * 2, next_features, kernel_size=3, padding=1),\n                nn.BatchNorm3d(next_features),\n                nn.ReLU(inplace=True),\n                nn.Conv3d(next_features, next_features, kernel_size=3, padding=1),\n                nn.BatchNorm3d(next_features),\n                nn.ReLU(inplace=True),\n                nn.Dropout3d(dropout) if dropout > 0 else nn.Identity()\n            )\n            self.decoder_blocks.append(decoder_block)\n        \n        # Final output convolution\n        self.final_conv = nn.Conv3d(features[0], out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        # Store skip connections\n        skip_connections = []\n        \n        # Encoder path\n        for i, encoder_block in enumerate(self.encoder_blocks):\n            x = encoder_block(x)\n            skip_connections.append(x)\n            \n            # Downsample (except for the last encoder block)\n            if i < len(self.downsample_layers):\n                x = self.downsample_layers[i](x)\n        \n        # Decoder path\n        skip_connections = skip_connections[:-1]  # Remove the deepest layer (no skip for bottleneck)\n        skip_connections.reverse()  # Reverse to match decoder order\n        \n        for i, (upsample_layer, decoder_block) in enumerate(zip(self.upsample_layers, self.decoder_blocks)):\n            # Upsample\n            x = upsample_layer(x)\n            \n            # Get corresponding skip connection\n            skip = skip_connections[i]\n            \n            # Ensure spatial dimensions match (handle odd-sized inputs)\n            if x.shape[2:] != skip.shape[2:]:\n                x = nn.functional.interpolate(x, size=skip.shape[2:], mode='trilinear', align_corners=False)\n            \n            # Concatenate skip connection\n            x = torch.cat([x, skip], dim=1)\n            \n            # Apply decoder block\n            x = decoder_block(x)\n        \n        # Final output\n        x = self.final_conv(x)\n        \n        return x\n\nclass Enhanced3DAugmentation:\n    \"\"\"Intensive 3D augmentations for medical imaging using scipy/numpy\"\"\"\n    \n    def __init__(self, mode='train'):\n        self.mode = mode\n        self.apply_augmentation = (mode == 'train')\n        \n    def random_rotation_3d(self, volume, max_angle=15):\n        \"\"\"Random 3D rotation\"\"\"\n        if not self.apply_augmentation or np.random.random() > 0.5:\n            return volume\n            \n        angle = np.random.uniform(-max_angle, max_angle)\n        # Rotate around z-axis (axial plane)\n        rotated = ndimage.rotate(volume, angle, axes=(1, 2), reshape=False, order=1)\n        return rotated\n    \n    def random_elastic_deformation(self, volume, sigma=4, points=3):\n        \"\"\"Heavy CPU op disabled by default for performance\"\"\"\n        if not Config.USE_ELASTIC_DEFORMATION:\n            return volume\n        if not self.apply_augmentation or np.random.random() > 0.3:\n            return volume\n        shape = volume.shape\n        dx = ndimage.gaussian_filter((np.random.random(shape) - 0.5), sigma) * points\n        dy = ndimage.gaussian_filter((np.random.random(shape) - 0.5), sigma) * points\n        dz = ndimage.gaussian_filter((np.random.random(shape) - 0.5), sigma) * points\n        x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]), indexing='ij')\n        indices = np.reshape(x + dx, (-1, 1)), np.reshape(y + dy, (-1, 1)), np.reshape(z + dz, (-1, 1))\n        deformed = ndimage.map_coordinates(volume, indices, order=1, mode='reflect')\n        return deformed.reshape(shape)\n    \n    def random_brightness_contrast(self, volume, brightness=0.2, contrast=0.2):\n        \"\"\"Random brightness and contrast for aneurysm visibility\"\"\"\n        if not self.apply_augmentation or np.random.random() > 0.7:\n            return volume\n            \n        # Brightness adjustment\n        brightness_factor = 1 + np.random.uniform(-brightness, brightness)\n        volume = volume * brightness_factor\n        \n        # Contrast adjustment\n        contrast_factor = 1 + np.random.uniform(-contrast, contrast)\n        mean = volume.mean()\n        volume = (volume - mean) * contrast_factor + mean\n        \n        return np.clip(volume, 0, 1)\n    \n    def random_gaussian_noise(self, volume, std_range=(0, 0.05)):\n        \"\"\"Add Gaussian noise to improve robustness\"\"\"\n        if not self.apply_augmentation or np.random.random() > 0.4:\n            return volume\n            \n        std = np.random.uniform(std_range[0], std_range[1])\n        noise = np.random.normal(0, std, volume.shape)\n        return np.clip(volume + noise, 0, 1)\n    \n    def random_gamma_correction(self, volume, gamma_range=(0.8, 1.2)):\n        \"\"\"Gamma correction for intensity variations\"\"\"\n        if not self.apply_augmentation or np.random.random() > 0.5:\n            return volume\n            \n        gamma = np.random.uniform(gamma_range[0], gamma_range[1])\n        return np.power(volume, gamma)\n    \n    def __call__(self, data_dict):\n        \"\"\"Apply all augmentations\"\"\"\n        result = {}\n        \n        for key in data_dict:\n            if key == 'volume' and isinstance(data_dict[key], np.ndarray):\n                volume = data_dict[key].copy()\n                \n                # Apply augmentations sequentially\n                # volume = self.random_rotation_3d(volume)\n                # volume = self.random_elastic_deformation(volume)\n                volume = self.random_brightness_contrast(volume)\n                volume = self.random_gaussian_noise(volume)\n                volume = self.random_gamma_correction(volume)\n                \n                # Convert to tensor\n                result[key] = torch.from_numpy(volume).float()\n            elif isinstance(data_dict[key], np.ndarray):\n                result[key] = torch.from_numpy(data_dict[key]).float()\n            else:\n                result[key] = data_dict[key]\n        \n        return result\n\nclass CustomTransforms:\n    \"\"\"Simple transforms for validation (no augmentation)\"\"\"\n    \n    def __init__(self, keys=['volume']):\n        self.keys = keys\n        \n    def __call__(self, data_dict):\n        \"\"\"Apply transforms to data dictionary\"\"\"\n        result = {}\n        \n        for key in data_dict:\n            if key in self.keys:\n                # Convert numpy array to tensor if needed\n                if isinstance(data_dict[key], np.ndarray):\n                    result[key] = torch.from_numpy(data_dict[key]).float()\n                else:\n                    result[key] = data_dict[key]\n            else:\n                result[key] = data_dict[key]\n        \n        return result\n\nprint(\"✅ Enhanced 3D UNet with medical augmentations loaded (MONAI-free!)\")\n\n# ====================================================\n# CELL 3: SIMPLE DICOM PROCESSOR\n# ====================================================\n\nclass SimpleDICOMProcessor:\n    def __init__(self, target_size=None):\n        self.target_size = target_size or Config.STAGE1_TARGET_SIZE\n        if Config.USE_DISK_CACHE:\n            os.makedirs(Config.CACHE_DIR, exist_ok=True)\n            os.makedirs(Config.MASK_CACHE_DIR, exist_ok=True)\n        \n    def load_dicom_series(self, series_path):\n        \"\"\"DICOM loading with CT HU windowing, orientation/spacing, isotropic resample, then target resize.\"\"\"\n        try:\n            series_id = os.path.basename(series_path.rstrip('/'))\n            cache_key = f\"{series_id}_D{self.target_size[0]}H{self.target_size[1]}W{self.target_size[2]}_S{Config.MAX_SLICES_PER_SERIES}_{Config.PREPROC_VERSION}.npy\"\n            cache_path = os.path.join(Config.CACHE_DIR, cache_key)\n            if Config.USE_DISK_CACHE and os.path.exists(cache_path):\n                try:\n                    vol = np.load(cache_path, allow_pickle=False)\n                    # Ensure cached volume matches target size\n                    if vol.shape != self.target_size:\n                        vol = self._resize_volume_to_target(vol)\n                        try:\n                            tmp_path = cache_path + '.tmp'\n                            np.save(tmp_path, vol.astype(np.float16), allow_pickle=False)\n                            os.replace(tmp_path, cache_path)\n                        except Exception:\n                            pass\n                    return vol.astype(np.float32)\n                except Exception:\n                    pass\n\n            # Read all dicoms\n            dicoms = []\n            for root, _, files in os.walk(series_path):\n                for f in files:\n                    if f.endswith('.dcm'):\n                        try:\n                            ds = pydicom.dcmread(os.path.join(root, f), force=True)\n                            if hasattr(ds, 'PixelData'):\n                                dicoms.append(ds)\n                        except Exception:\n                            continue\n            if not dicoms:\n                return np.zeros(self.target_size, dtype=np.float32)\n            \n            # Sort slices by orientation/IPPatient; fallback InstanceNumber\n            try:\n                orient = np.array(dicoms[0].ImageOrientationPatient, dtype=np.float32)\n                row = orient[:3]; col = orient[3:]\n                normal = np.cross(row, col)\n                def sort_key(ds):\n                    ipp = np.array(getattr(ds, 'ImagePositionPatient', [0,0,0]), dtype=np.float32)\n                    return float(np.dot(ipp, normal))\n                dicoms = sorted(dicoms, key=sort_key)\n            except Exception:\n                dicoms = sorted(dicoms, key=lambda ds: getattr(ds, 'InstanceNumber', 0))\n\n            # Get spacing (dz,dy,dx)\n            try:\n                dy, dx = map(float, dicoms[0].PixelSpacing)\n            except Exception:\n                ps = getattr(dicoms[0], 'PixelSpacing', [1.0,1.0])\n                dy, dx = float(ps[0]), float(ps[1])\n            # slice spacing from IPP median\n            zs = []\n            for i in range(1, len(dicoms)):\n                p0 = np.array(getattr(dicoms[i-1], 'ImagePositionPatient', [0,0,0]), dtype=np.float32)\n                p1 = np.array(getattr(dicoms[i], 'ImagePositionPatient', [0,0,0]), dtype=np.float32)\n                d = np.linalg.norm(p1 - p0)\n                if d > 0:\n                    zs.append(d)\n            dz = float(np.median(zs)) if zs else float(getattr(dicoms[0], 'SliceThickness', 1.0))\n            dz = dz if (dz > 0 and np.isfinite(dz)) else 1.0\n            dy = dy if (dy > 0 and np.isfinite(dy)) else 1.0\n            dx = dx if (dx > 0 and np.isfinite(dx)) else 1.0\n\n            # Build volume with HU and windowing\n            base_h = int(getattr(dicoms[0], 'Rows', 256))\n            base_w = int(getattr(dicoms[0], 'Columns', 256))\n            vol_slices = []\n            modality = (getattr(dicoms[0], 'Modality', '') or '').upper()\n            c = Config.CTA_WINDOW_CENTER; w = Config.CTA_WINDOW_WIDTH\n            lo, hi = c - w/2.0, c + w/2.0\n            for ds in dicoms:\n                try:\n                    arr = ds.pixel_array\n                except Exception:\n                    continue\n                if arr.ndim >= 3:\n                    h, w2 = arr.shape[-2], arr.shape[-1]\n                    n = int(np.prod(arr.shape[:-2])); frames = arr.reshape(n, h, w2)\n                else:\n                    frames = arr[np.newaxis, ...]\n                for sl in frames:\n                    sl = sl.astype(np.float32)\n                    if getattr(ds, 'PhotometricInterpretation', 'MONOCHROME2') == 'MONOCHROME1':\n                        sl = sl.max() - sl\n                    slope = float(getattr(ds, 'RescaleSlope', 1.0)); intercept = float(getattr(ds, 'RescaleIntercept', 0.0))\n                    sl = sl * slope + intercept\n                    # resize slice to base\n                    if sl.shape != (base_h, base_w):\n                        sl = cv2.resize(sl, (base_w, base_h))\n                    # modality normalization\n                    if modality == 'CT':\n                        s = np.clip(sl, lo, hi)\n                        s = (s - lo) / (hi - lo + 1e-6)\n                    else:\n                        mean = float(sl.mean()); std = float(sl.std() + 1e-6)\n                        s = (sl - mean) / std; zc = 3.0\n                        s = np.clip(s, -zc, zc); s = (s + zc) / (2.0*zc)\n                    vol_slices.append(s.astype(np.float32))\n                    \n            if not vol_slices:\n                return np.zeros(self.target_size, dtype=np.float32)       \n            volume = np.stack(vol_slices, axis=0).astype(np.float32)\n\n            # Isotropic resample to TARGET_SPACING_MM then resize to target grid\n            z, y, x = volume.shape\n            if Config.TARGET_SPACING_MM is not None:\n                newD = max(1, int(round(z * dz / Config.TARGET_SPACING_MM)))\n                newH = max(1, int(round(y * dy / Config.TARGET_SPACING_MM)))\n                newW = max(1, int(round(x * dx / Config.TARGET_SPACING_MM)))\n                volume = ndimage.zoom(volume, (newD / z, newH / y, newW / x), order=1)\n            volume = self._resize_volume_to_target(volume)  \n            \n            # Save preprocessed volume (target size) to cache (float16)\n            if Config.USE_DISK_CACHE:\n                try:\n                    tmp_path = cache_path + '.tmp'\n                    np.save(tmp_path, volume.astype(np.float16), allow_pickle=False)\n                    os.replace(tmp_path, cache_path)\n                except Exception:\n                    pass\n            return volume\n\n        except Exception as e:\n            print(f\"Failed to load {series_path}: {e}\")\n            return np.zeros(self.target_size, dtype=np.float32)\n            \n    def get_mask_cache_path(self, series_id: str) -> str:\n        return os.path.join(Config.MASK_CACHE_DIR, f\"{series_id}_mask.npy\")\n\n    def _resize_mask_fast(self, mask: np.ndarray) -> np.ndarray:\n        target_d, target_h, target_w = self.target_size\n        D, H, W = mask.shape\n        # Depth resample by index selection\n        if D != target_d:\n            idx = np.linspace(0, max(D - 1, 0), num=target_d).astype(int) if D > 0 else np.zeros(target_d, dtype=int)\n            mask = mask[idx]\n        # Per-slice nearest resize\n        if (H, W) != (target_h, target_w):\n            resized = np.empty((target_d, target_h, target_w), dtype=np.float32)\n            for i in range(target_d):\n                resized[i] = cv2.resize(mask[i].astype(np.float32), (target_w, target_h), interpolation=cv2.INTER_NEAREST)\n            mask = resized\n        return mask\n\n    def load_and_cache_mask(self, series_id: str, has_aneurysm: bool) -> tuple:\n        cache_path = self.get_mask_cache_path(series_id)\n        # Load from cache if present and correct shape\n        if os.path.exists(cache_path):\n            try:\n                m = np.load(cache_path, allow_pickle=False)\n                if m.shape == self.target_size:\n                    return m.astype(np.float32), False  # synthetic flag unknown; treat as non-synthetic cached\n            except Exception:\n                pass  # fall through to rebuild\n\n        seg_path = os.path.join(Config.SEGMENTATION_DIR, f\"{series_id}.nii\")\n        try:\n            is_synthetic = False\n            if os.path.exists(seg_path):\n                nii_img = nib.load(seg_path)\n                mask = nii_img.get_fdata().astype(np.float32)\n                mask = self._resize_mask_fast(mask)\n                mask = (mask > 0).astype(Config.MASK_CACHE_DTYPE)\n                # quick validation\n                if mask.max() == 0 and has_aneurysm:\n                    mask = self._create_synthetic_mask(series_id)\n                    is_synthetic = True\n            else:\n                mask = self._create_synthetic_mask(series_id) if has_aneurysm else np.zeros(self.target_size, dtype=Config.MASK_CACHE_DTYPE)\n                is_synthetic = bool(has_aneurysm)\n        except Exception as e:\n            print(f\"Error processing mask for {series_id}: {e}\")\n            mask = np.zeros(self.target_size, dtype=Config.MASK_CACHE_DTYPE)\n            is_synthetic = True\n\n        # Save to cache atomically\n        try:\n            tmp = cache_path + '.tmp'\n            np.save(tmp, mask, allow_pickle=False)\n            os.replace(tmp, cache_path)\n        except Exception:\n            pass\n        return mask.astype(np.float32), is_synthetic\n\n    def _create_synthetic_mask(self, series_id: str) -> np.ndarray:\n        np.random.seed(hash(series_id) % (2**32))\n        D, H, W = self.target_size\n        min_dim = min(D, H, W)\n        edge = int(np.random.uniform(0.1, 0.3) * min_dim)\n        edge = max(4, min(edge, min_dim))\n        cz = np.random.randint(edge//2, D - edge//2) if D > edge else D // 2\n        cy = np.random.randint(edge//2, H - edge//2) if H > edge else H // 2\n        cx = np.random.randint(edge//2, W - edge//2) if W > edge else W // 2\n        z1, z2 = max(0, cz - edge//2), min(D, cz + edge//2)\n        y1, y2 = max(0, cy - edge//2), min(H, cy + edge//2)\n        x1, x2 = max(0, cx - edge//2), min(W, cx + edge//2)\n        m = np.zeros(self.target_size, dtype=Config.MASK_CACHE_DTYPE)\n        m[z1:z2, y1:y2, x1:x2] = 1\n        return m\n\n    def _resize_volume_to_target(self, volume: np.ndarray) -> np.ndarray:\n        \"\"\"Resize a 3D volume to self.target_size (D,H,W).\"\"\"\n        target_d, target_h, target_w = self.target_size\n        D, H, W = volume.shape\n        # Depth resample by index selection\n        if D != target_d:\n            idx = np.linspace(0, max(D - 1, 0), num=target_d).astype(int) if D > 0 else np.zeros(target_d, dtype=int)\n            volume = volume[idx]\n        # Per-slice resize to (target_h, target_w)\n        if (H, W) != (target_h, target_w):\n            resized = np.empty((target_d, target_h, target_w), dtype=np.float32)\n            for i in range(target_d):\n                resized[i] = cv2.resize(volume[i].astype(np.float32), (target_w, target_h))\n            volume = resized\n        return volume.astype(np.float32)\n    \n    def preprocess_volume(self, volume):\n        \"\"\"Deprecated: kept for compatibility; not used in fast path.\"\"\"\n        # Fallback CPU preprocessing if needed\n        p1, p99 = np.percentile(volume, [1, 99])\n        volume = np.clip(volume, p1, p99)\n        volume = (volume - p1) / (p99 - p1 + 1e-8)\n        if volume.shape != self.target_size:\n            zoom_factors = [self.target_size[i] / volume.shape[i] for i in range(3)]\n            volume = ndimage.zoom(volume, zoom_factors, order=1)\n        return volume.astype(np.float32)\n\nprint(\"✅ DICOM Processor loaded\")\n\n# ====================================================\n# CELL 4: DATASET CLASS\n# ====================================================\n\nclass SimpleSegmentationDataset(Dataset):\n    def __init__(self, df, series_dir, processor, mode='train'):\n        self.df = df\n        self.series_dir = series_dir\n        self.processor = processor\n        self.mode = mode\n        \n        # Enhanced augmentation for training, simple transforms for validation\n        if mode == 'train':\n            self.transform = Enhanced3DAugmentation(mode='train')\n        else:\n            self.transform = CustomTransforms(keys=['volume'])\n        \n    def __len__(self):\n        return len(self.df)\n\n    def validate_segmentation_mask(self, series_id, mask):\n        \"\"\"Validate segmentation mask quality\"\"\"\n        # Check if mask is empty\n        if mask.max() == 0:\n            return False\n            \n        # Check mask connectivity and size\n        mask_binary = (mask > 0.5).astype(np.uint8)\n        labeled_mask, num_components = ndimage.label(mask_binary)\n        \n        if num_components == 0:\n            return False\n            \n        # Check component sizes (aneurysms should be small but not tiny)\n        component_sizes = []\n        for i in range(1, num_components + 1):\n            component_size = np.sum(labeled_mask == i)\n            component_sizes.append(component_size)\n        \n        # Valid if has reasonably sized components\n        valid_components = [size for size in component_sizes if 10 < size < 10000]\n        return len(valid_components) > 0\n    \n    \n    def load_segmentation_mask(self, series_id, volume_shape):\n        \"\"\"Load real segmentation mask from competition data with validation\"\"\"\n        seg_path = os.path.join(Config.SEGMENTATION_DIR, f\"{series_id}.nii\")\n        \n        try:\n            if os.path.exists(seg_path):\n                # Load NIfTI segmentation mask\n                import nibabel as nib\n                nii_img = nib.load(seg_path)\n                mask = nii_img.get_fdata().astype(np.float32)\n                \n                # Resize mask to match volume shape\n                if mask.shape != volume_shape:\n                    zoom_factors = [volume_shape[i] / mask.shape[i] for i in range(3)]\n                    mask = ndimage.zoom(mask, zoom_factors, order=0)  # Nearest neighbor for masks\n                \n                # Normalize mask values to 0-1\n                mask = (mask > 0).astype(np.float32)\n                \n                # Validate mask quality\n                if self.validate_segmentation_mask(series_id, mask):\n                    return mask\n                else:\n                    # Mask failed validation - use fallback for aneurysm cases\n                    has_aneurysm = int(self.df[self.df[Config.ID_COL] == series_id][Config.TARGET_COL].iloc[0])\n                    if has_aneurysm:\n                        # Create enhanced central region mask for aneurysm cases\n                        mask = np.zeros(volume_shape, dtype=np.float32)\n                        h, w, d = volume_shape\n                        # Multiple small regions to simulate potential aneurysm locations\n                        mask[h//3:2*h//3, w//3:2*w//3, d//3:2*d//3] = 0.7\n                        mask[h//4:3*h//4, w//4:3*w//4, d//2:d//2+d//8] = 1.0  # Central strong region\n                        return mask\n                    else:\n                        return np.zeros(volume_shape, dtype=np.float32)\n            else:\n                # No segmentation available - fallback based on aneurysm label\n                has_aneurysm = int(self.df[self.df[Config.ID_COL] == series_id][Config.TARGET_COL].iloc[0])\n                if has_aneurysm:\n                    # Randomized synthetic cube\n                    D, H, W = volume_shape\n                    min_dim = min(D, H, W)\n                    frac_low, frac_high = Config.SYN_MASK_SIZE_FRAC\n                    edge = int(np.random.uniform(frac_low, frac_high) * min_dim)\n                    edge = max(4, min(edge, min_dim))\n                    if Config.SYN_MASK_RANDOM_CENTER:\n                        cz = np.random.randint(edge//2, D - edge//2) if D > edge else D // 2\n                        cy = np.random.randint(edge//2, H - edge//2) if H > edge else H // 2\n                        cx = np.random.randint(edge//2, W - edge//2) if W > edge else W // 2\n                    else:\n                        cz, cy, cx = D//2, H//2, W//2\n                    z1, z2 = max(0, cz - edge//2), min(D, cz + edge//2)\n                    y1, y2 = max(0, cy - edge//2), min(H, cy + edge//2)\n                    x1, x2 = max(0, cx - edge//2), min(W, cx + edge//2)\n                    mask = np.zeros(volume_shape, dtype=np.float32)\n                    mask[z1:z2, y1:y2, x1:x2] = 1.0\n                    return mask\n                else:\n                    return np.zeros(volume_shape, dtype=np.float32)\n                \n        except Exception as e:\n            print(f\"Error loading segmentation for {series_id} (corrupt nii?): {e}\")\n            # For corrupt files, avoid giving synthetic positives blindly; use zeros\n            return np.zeros(volume_shape, dtype=np.float32)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        series_id = row[Config.ID_COL]\n        series_path = os.path.join(self.series_dir, series_id)\n        \n        # Load volume\n        volume = self.processor.load_dicom_series(series_path)\n\n        # Get aneurysm presence label\n        has_aneurysm = int(row[Config.TARGET_COL])\n        \n        # Load segmentation mask from cache (builds once if missing)\n        mask, is_synth = self.processor.load_and_cache_mask(series_id, bool(has_aneurysm))\n        \n        # Transform (CPU augmentations kept lightweight)\n        data_dict = {'volume': volume}\n        if self.transform:\n            data_dict = self.transform(data_dict)\n        vol_np = data_dict['volume'].numpy() if isinstance(data_dict['volume'], torch.Tensor) else data_dict['volume']\n        # Ensure numpy array, contiguous, and writable (decouple from read-only storage)\n        if isinstance(vol_np, torch.Tensor):\n            vol_np = vol_np.cpu().numpy()\n        vol_np = np.ascontiguousarray(vol_np).copy()\n        volume_tensor = torch.tensor(vol_np, dtype=torch.float32).unsqueeze(0).contiguous()  # [1, D, H, W]\n        # mask already at target size/dtype via cache\n        mask_np = np.ascontiguousarray(mask).copy()\n        mask_tensor = torch.tensor(mask_np, dtype=torch.float32).unsqueeze(0).contiguous()\n        \n        return {\n            'volume': volume_tensor,\n            'mask': mask_tensor,\n            'has_aneurysm': torch.tensor(has_aneurysm, dtype=torch.float32),\n            'is_synthetic_mask': torch.tensor(1.0 if is_synth else 0.0, dtype=torch.float32),\n            'series_id': series_id\n        }\n\nprint(\"✅ Dataset class loaded\")\n\n# ====================================================\n# CELL 5: 3D U-NET MODEL\n# ====================================================\n\nclass Simple3DSegmentationNet(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__()\n        \n        # Use our Custom3DUNet - pure PyTorch implementation!\n        self.backbone = Custom3DUNet(spatial_dims=3, in_channels=in_channels, dropout=0.1)\n        \n        # Segmentation head\n        self.seg_head = nn.Conv3d(Config.UNET_OUT_CHANNELS, out_channels, kernel_size=1)\n\n        # Classification head (aneurysm presence)\n        self.global_pool = nn.AdaptiveAvgPool3d(1)\n        self.classifier = nn.Sequential(\n            nn.Linear(Config.UNET_OUT_CHANNELS, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 1)\n        )\n\n        \n    def forward(self, x):\n        # Extract features\n        features = self.backbone(x)\n        \n        # Segmentation output\n        seg_logits = self.seg_head(features)\n        \n        # Classification output\n        pooled_features = self.global_pool(features).flatten(1)\n        cls_logits = self.classifier(pooled_features)\n        \n        return seg_logits, cls_logits\n\nprint(\"✅ Model architecture loaded\")\n\n# ====================================================\n# CELL 6: ENHANCED LOSS FUNCTIONS\n# ====================================================\n\nclass DiceLoss(nn.Module):\n    \"\"\"Dice Loss for better segmentation of small objects\"\"\"\n    def __init__(self, smooth=1e-6):\n        super().__init__()\n        self.smooth = smooth\n        \n    def forward(self, predictions, targets):\n        # Apply sigmoid to logits and clamp to avoid log(0) / NaN in grads\n        predictions = torch.sigmoid(predictions).clamp(min=1e-6, max=1-1e-6)\n        \n        # Flatten tensors\n        predictions = predictions.view(-1)\n        targets = targets.view(-1)\n        \n        # Calculate intersection and union\n        intersection = (predictions * targets).sum()\n        dice = (2. * intersection + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n        \n        return 1 - dice\n\nclass FocalLoss(nn.Module):\n    \"\"\"Focal Loss for handling class imbalance\"\"\"\n    def __init__(self, alpha=0.25, gamma=2.0, smooth=1e-6):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.smooth = smooth\n        \n    def forward(self, predictions, targets):\n        # Apply sigmoid to get probabilities\n        probs = torch.sigmoid(predictions)\n        \n        # Calculate focal loss components with clamped probs for stability\n        probs = probs.clamp(min=1e-6, max=1-1e-6)\n        pt = torch.where(targets == 1, probs, 1 - probs)\n        ce_loss = nn.functional.binary_cross_entropy_with_logits(predictions, targets, reduction='none')\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        \n        return focal_loss.mean()\n        \nclass EnhancedCombinedLoss(nn.Module):\n    \"\"\"Enhanced loss combining Dice + BCE + Focal (seg) + BCE (cls) with weights\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.dice_loss = DiceLoss()\n        self.bce_loss = nn.BCEWithLogitsLoss()\n        self.focal_loss = FocalLoss(alpha=0.25, gamma=2)\n        \n    def forward(self, seg_logits, cls_logits, seg_targets, cls_targets, is_synthetic_mask: bool):\n        # Multi-component segmentation loss\n        dice_loss = self.dice_loss(seg_logits, seg_targets)\n        bce_seg_loss = self.bce_loss(seg_logits, seg_targets)\n        focal_seg_loss = self.focal_loss(seg_logits, seg_targets)\n        seg_loss = 0.5 * dice_loss + 0.3 * bce_seg_loss + Config.FOCAL_LOSS_WEIGHT * focal_seg_loss\n        \n        # Down-weight seg loss if targets are synthetic\n        seg_weight = Config.SEG_LOSS_WEIGHT if is_synthetic_mask else 1.0\n        seg_loss = seg_weight * seg_loss\n        \n        # Classification loss (preserve batch dim)\n        # Clamp logits for stability\n        cls_loss = self.bce_loss(cls_logits.view(-1), cls_targets)\n        \n        # Total loss: prioritize classification guidance\n        total_loss = seg_loss + 1.0 * cls_loss\n        return total_loss, seg_loss, cls_loss\n\nprint(\"✅ Enhanced loss functions loaded (Dice + BCE + Focal)\")\n\n# ====================================================\n# CELL 7: TRAINING FUNCTIONS\n# ====================================================\n\ndef train_epoch(model, loader, optimizer, criterion, device, scaler=None):\n    model.train()\n    total_loss = 0\n    total_seg_loss = 0\n    total_cls_loss = 0\n    num_batches = 0\n    \n    # Initialize grads for accumulation\n    optimizer.zero_grad(set_to_none=True)\n    for step, batch in enumerate(tqdm(loader, desc=\"Training\")):\n        volume = batch['volume'].to(device, non_blocking=True)\n        mask = batch['mask'].to(device, non_blocking=True)\n        has_aneurysm = batch['has_aneurysm'].to(device, non_blocking=True)\n        is_synthetic_mask = batch.get('is_synthetic_mask', torch.zeros_like(has_aneurysm)).to(device, non_blocking=True)\n        \n        # Forward pass\n        with torch.cuda.amp.autocast(enabled=Config.MIXED_PRECISION, dtype=torch.float16):\n            # Volumes are pre-normalized and resized in cache; skip per-step norm/resize\n            vol = volume\n            if mask.shape[2:] != Config.STAGE1_TARGET_SIZE:\n                with torch.cuda.amp.autocast(enabled=False):\n                    mask = torch.nn.functional.interpolate(\n                        mask.float(), size=Config.STAGE1_TARGET_SIZE, mode='nearest'\n                    )\n\n            # Lightweight GPU augmentations (training only)\n            if Config.AUG_ENABLE_GPU:\n                # Brightness (clamped)\n                if Config.AUG_BRIGHTNESS > 0:\n                    b = (1.0 + (torch.rand(vol.size(0), 1, 1, 1, 1, device=vol.device) * 2 - 1) * Config.AUG_BRIGHTNESS)\n                    vol = torch.clamp(vol * b, 0, 1)\n                # Contrast\n                if Config.AUG_CONTRAST > 0:\n                    mean = vol.mean(dim=[2,3,4], keepdim=True)\n                    c = (1.0 + (torch.rand(vol.size(0), 1, 1, 1, 1, device=vol.device) * 2 - 1) * Config.AUG_CONTRAST)\n                    vol = torch.clamp((vol - mean) * c + mean, 0, 1)\n                # Gamma\n                gmin, gmax = Config.AUG_GAMMA_MINMAX\n                if gmin != 1.0 or gmax != 1.0:\n                    gamma = torch.rand(vol.size(0), 1, 1, 1, 1, device=vol.device) * (gmax - gmin) + gmin\n                    vol = torch.clamp(vol, 0, 1) ** gamma\n                # Noise\n                if Config.AUG_NOISE_STD > 0:\n                    std = torch.rand(vol.size(0), 1, 1, 1, 1, device=vol.device) * Config.AUG_NOISE_STD\n                    noise = torch.randn_like(vol) * std\n                    vol = torch.clamp(vol + noise, 0, 1)\n\n            seg_logits, cls_logits = model(vol)\n            # Calculate loss\n            # Prefer dataset-provided synthetic flag if present; fallback to zero-mask check\n            is_synthetic = bool((is_synthetic_mask > 0.5).item()) if is_synthetic_mask.numel() == has_aneurysm.numel() else bool((mask.max() <= 0).item())\n            # Ensure target dtype matches logits dtype to avoid mixed-type kernels\n            mask = mask.to(dtype=seg_logits.dtype)\n            loss, seg_loss, cls_loss = criterion(seg_logits, cls_logits, mask, has_aneurysm, is_synthetic)\n        \n        # Backward pass with gradient clipping (AMP-aware)\n        if scaler is not None and Config.MIXED_PRECISION:\n            scaler.scale(loss / Config.GRAD_ACCUM_STEPS).backward()\n            if (step + 1) % Config.GRAD_ACCUM_STEPS == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad(set_to_none=True)\n        else:\n            (loss / Config.GRAD_ACCUM_STEPS).backward()\n            if (step + 1) % Config.GRAD_ACCUM_STEPS == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n            \n        total_loss += loss.item()\n        total_seg_loss += seg_loss.item()\n        total_cls_loss += cls_loss.item()\n        num_batches += 1\n\n        # Release per-batch tensors to help GPU memory\n        del volume, mask, has_aneurysm, is_synthetic_mask, seg_logits, cls_logits, loss, seg_loss, cls_loss\n    \n    return (total_loss / num_batches, \n            total_seg_loss / num_batches,\n            total_cls_loss / num_batches)\n            \n\ndef validate_epoch(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    total_seg_loss = 0\n    total_cls_loss = 0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Validating\"):\n            volume = batch['volume'].to(device, non_blocking=True)\n            mask = batch['mask'].to(device, non_blocking=True)\n            has_aneurysm = batch['has_aneurysm'].to(device, non_blocking=True)\n            \n            # Forward pass\n            with torch.cuda.amp.autocast(enabled=Config.MIXED_PRECISION, dtype=torch.float16):\n                vol = volume\n                if mask.shape[2:] != Config.STAGE1_TARGET_SIZE:\n                    with torch.cuda.amp.autocast(enabled=False):\n                        mask = torch.nn.functional.interpolate(\n                            mask.float(), size=Config.STAGE1_TARGET_SIZE, mode='nearest'\n                        )\n\n                # No GPU augs in validation\n                seg_logits, cls_logits = model(vol)\n                # Calculate loss\n                is_synthetic = bool((mask.max() <= 0).item())\n                loss, seg_loss, cls_loss = criterion(seg_logits, cls_logits, mask, has_aneurysm, is_synthetic)\n            \n            total_loss += loss.item()\n            total_seg_loss += seg_loss.item()\n            total_cls_loss += cls_loss.item()\n            num_batches += 1\n\n            # Release per-batch tensors to help GPU memory\n            del volume, mask, has_aneurysm, seg_logits, cls_logits, loss, seg_loss, cls_loss\n    \n    return (total_loss / num_batches, \n            total_seg_loss / num_batches,\n            total_cls_loss / num_batches)\n\nprint(\"✅ Training functions loaded\")\n\n# ====================================================\n# CELL 8: MAIN TRAINING LOOP\n# ====================================================\n\ndef main():\n    print(f\"🚀 STAGE 1: 3D SEGMENTATION FOR REGION LOCALIZATION\")\n    print(f\"Using device: {Config.DEVICE}\")\n    print(f\"Target size: {Config.STAGE1_TARGET_SIZE}\")\n    \n    # Load data\n    train_df = pd.read_csv(Config.TRAIN_CSV_PATH)\n    \n    # Load localizer data (for future use)\n    try:\n        localizer_df = pd.read_csv(Config.LOCALIZER_CSV_PATH)\n        print(f\"Loaded localizer data: {len(localizer_df)} entries\")\n    except:\n        localizer_df = None\n        print(\"No localizer data found - continuing without it\")\n    \n    # Debug mode - small subset\n    if Config.DEBUG_MODE:\n        train_df = train_df.head(Config.DEBUG_SAMPLES)\n    print(f\"Training samples (pre-filter): {len(train_df)}\")\n    print(f\"Aneurysm cases (pre-filter): {train_df[Config.TARGET_COL].sum()}\")\n\n    # Optionally filter to series with real, non-empty masks\n    if Config.FILTER_TO_MASKED_SERIES:\n        def has_valid_mask(series_id: str) -> bool:\n            seg_path = os.path.join(Config.SEGMENTATION_DIR, f\"{series_id}.nii\")\n            if not os.path.exists(seg_path):\n                return False\n            try:\n                nii = nib.load(seg_path)\n                mask = nii.get_fdata()\n                return np.any(mask > 0)\n            except Exception:\n                return False\n\n        filtered_df = train_df[train_df[Config.ID_COL].apply(has_valid_mask)].reset_index(drop=True)\n        if len(filtered_df) == 0:\n            print(\"⚠️ No series with ground-truth masks found. Proceeding without filtering.\")\n        else:\n            train_df = filtered_df\n        print(f\"Training samples (post-filter): {len(train_df)}\")\n        print(f\"Aneurysm cases (post-filter): {train_df[Config.TARGET_COL].sum()}\")\n    \n    # Simple train/val split\n    val_size = len(train_df) // 5\n    val_df = train_df[:val_size].reset_index(drop=True)\n    train_df = train_df[val_size:].reset_index(drop=True)\n    \n    print(f\"Train: {len(train_df)}, Val: {len(val_df)}\")\n    \n    # Create datasets\n    processor = SimpleDICOMProcessor()\n    train_dataset = SimpleSegmentationDataset(train_df, Config.SERIES_DIR, processor, 'train')\n    val_dataset = SimpleSegmentationDataset(val_df, Config.SERIES_DIR, processor, 'val')\n\n    # Optional pre-build caches to eliminate stalls\n    if Config.PRE_BUILD_CACHE:\n        print(\"\\n🚀 Pre-building caches (volumes + masks) ...\")\n        combined_df = pd.concat([train_df, val_df], ignore_index=True)\n        for _, row in tqdm(combined_df.iterrows(), total=len(combined_df), desc=\"Building caches\"):\n            sid = row[Config.ID_COL]\n            spath = os.path.join(Config.SERIES_DIR, sid)\n            if not os.path.exists(spath):\n                continue\n            # Build volume cache\n            _ = processor.load_dicom_series(spath)\n            # Build mask cache\n            has_an = bool(int(row[Config.TARGET_COL]))\n            _ = processor.load_and_cache_mask(sid, has_an)\n        gc.collect()\n        print(\"✅ Cache building complete. Training will have minimal stalls.\")\n    \n    # Create loaders (tuned for throughput)\n    cpu_count = os.cpu_count() or 2\n    # num_workers = min(8, max(2, cpu_count - 1))\n    num_workers = 8\n    loader_kwargs = dict(\n        batch_size=Config.STAGE1_BATCH_SIZE,\n        pin_memory=True,\n        num_workers=num_workers,\n        persistent_workers=True,\n        prefetch_factor=8,\n    )\n\n    # Custom collate to avoid non-resizable storage issues during default_collate\n    def collate_segmentation_batch(batch_list):\n        volumes = torch.stack([sample['volume'].contiguous() for sample in batch_list], dim=0)\n        masks = torch.stack([sample['mask'].contiguous() for sample in batch_list], dim=0)\n        has = torch.stack([\n            sample['has_aneurysm'] if isinstance(sample['has_aneurysm'], torch.Tensor)\n            else torch.tensor(sample['has_aneurysm'], dtype=torch.float32)\n            for sample in batch_list\n        ], dim=0)\n        synth = torch.stack([\n            sample.get('is_synthetic_mask', torch.tensor(0.0, dtype=torch.float32))\n            for sample in batch_list\n        ], dim=0)\n        series_ids = [sample['series_id'] for sample in batch_list]\n        return {\n            'volume': volumes,\n            'mask': masks,\n            'has_aneurysm': has,\n            'is_synthetic_mask': synth,\n            'series_id': series_ids,\n        }\n\n    train_loader = DataLoader(train_dataset, shuffle=True, drop_last=False, collate_fn=collate_segmentation_batch, **loader_kwargs)\n\n    # Full validation for final run\n    val_loader = DataLoader(val_dataset, shuffle=False, drop_last=False, collate_fn=collate_segmentation_batch, **loader_kwargs)\n    \n    # Create model\n    model = Simple3DSegmentationNet().to(Config.DEVICE)\n\n    # Enable TF32 for speed on Ampere (safe for training)\n    try:\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n    except Exception:\n        pass\n    \n    # Multi-GPU if available\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs\")\n        model = nn.DataParallel(model)\n    \n    # Enhanced optimizer and loss - proven optimization from winning solutions\n    optimizer = optim.AdamW(model.parameters(), lr=Config.STAGE1_LR, weight_decay=1e-4)\n    criterion = EnhancedCombinedLoss()  # Use enhanced loss function\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.STAGE1_EPOCHS, eta_min=1e-6)\n    scaler = torch.cuda.amp.GradScaler(enabled=Config.MIXED_PRECISION)\n    \n    # Training loop\n    best_loss = float('inf')\n    no_improve_epochs = 0\n    \n    for epoch in range(Config.STAGE1_EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{Config.STAGE1_EPOCHS}\")\n        \n        # Train\n        train_loss, train_seg_loss, train_cls_loss = train_epoch(\n            model, train_loader, optimizer, criterion, Config.DEVICE, scaler\n        )\n        \n        # Validate\n        val_loss, val_seg_loss, val_cls_loss = validate_epoch(\n            model, val_loader, criterion, Config.DEVICE\n        )\n        \n        # Step scheduler\n        scheduler.step()\n        \n        print(f\"Train - Total: {train_loss:.4f}, Seg: {train_seg_loss:.4f}, Cls: {train_cls_loss:.4f}\")\n        print(f\"Val   - Total: {val_loss:.4f}, Seg: {val_seg_loss:.4f}, Cls: {val_cls_loss:.4f}\")\n        \n        # Save best model & early stopping\n        if val_loss < best_loss:\n            best_loss = val_loss\n            no_improve_epochs = 0\n            torch.save({\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'epoch': epoch,\n                'val_loss': val_loss\n            }, 'stage1_segmentation_best.pth')\n            print(f\"💾 Saved best model (val_loss: {val_loss:.4f})\")\n        else:\n            no_improve_epochs += 1\n            if no_improve_epochs >= Config.EARLY_STOPPING_PATIENCE:\n                print(f\"⏹️ Early stopping at epoch {epoch+1} (no improvement for {Config.EARLY_STOPPING_PATIENCE} epochs)\")\n                break\n\n        # Proactive memory cleanup after each epoch\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n                torch.cuda.empty_cache()\n        except Exception:\n            pass\n        gc.collect()\n    \n    print(f\"\\n✅ Stage 1 complete! Best val loss: {best_loss:.4f}\")\n    print(\"📁 Model saved as 'stage1_segmentation_best.pth'\")\n    \n    return model\n\n# ====================================================\n# CELL 9: ROI EXTRACTOR FOR STAGE 2 (FUTURE USE)\n# ====================================================\n\nclass ROIExtractor:\n    def __init__(self, roi_size=(224, 224), confidence_threshold=0.5):\n        self.roi_size = roi_size\n        self.confidence_threshold = confidence_threshold\n    \n    def extract_rois(self, volume, segmentation_mask):\n        \"\"\"Extract 2D ROI slices from 3D volume using segmentation mask\"\"\"\n        rois = []\n        \n        # Find slices with high confidence regions\n        for slice_idx in range(volume.shape[0]):\n            slice_volume = volume[slice_idx]\n            slice_mask = segmentation_mask[slice_idx]\n            \n            # Check if this slice has potential aneurysm regions\n            if np.max(slice_mask) > self.confidence_threshold:\n                # Find connected components\n                binary_mask = (slice_mask > self.confidence_threshold).astype(np.uint8)\n                \n                # Find contours\n                contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n                \n                for contour in contours:\n                    # Get bounding box\n                    x, y, w, h = cv2.boundingRect(contour)\n                    \n                    # Expand bounding box\n                    margin = max(w, h) // 4\n                    x = max(0, x - margin)\n                    y = max(0, y - margin)\n                    w = min(slice_volume.shape[1] - x, w + 2*margin)\n                    h = min(slice_volume.shape[0] - y, h + 2*margin)\n                    \n                    # Extract ROI\n                    roi = slice_volume[y:y+h, x:x+w]\n                    \n                    # Resize to standard size\n                    roi_resized = cv2.resize(roi, self.roi_size)\n                    \n                    rois.append({\n                        'roi': roi_resized,\n                        'slice_idx': slice_idx,\n                        'bbox': (x, y, w, h),\n                        'confidence': np.max(slice_mask[y:y+h, x:x+w])\n                    })\n        \n        return rois\n\nprint(\"✅ ROI Extractor loaded (for Stage 2)\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T19:02:53.436086Z","iopub.execute_input":"2025-08-15T19:02:53.436944Z","iopub.status.idle":"2025-08-15T19:02:53.53655Z","shell.execute_reply.started":"2025-08-15T19:02:53.436914Z","shell.execute_reply":"2025-08-15T19:02:53.535707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================\n# CELL 10: RUN TRAINING\n# ====================================================\n\n# Start Training\nmodel = main()\n\nprint(\"Expected training time: Approx 5 hours\")\nprint(\"Output: stage1_segmentation_best.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T19:03:26.771028Z","iopub.execute_input":"2025-08-15T19:03:26.771546Z","iopub.status.idle":"2025-08-16T00:36:42.637037Z","shell.execute_reply.started":"2025-08-15T19:03:26.771516Z","shell.execute_reply":"2025-08-16T00:36:42.63614Z"}},"outputs":[],"execution_count":null}]}